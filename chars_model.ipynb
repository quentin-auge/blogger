{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 4. / 5\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*).\n",
    "\n",
    "![](img/rnn_fixed.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    if GPU:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    if GPU:\n",
    "        labels_tensor = labels_tensor.cuda()\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([321668, 8])\n",
      "torch.Size([321668])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80416, 8])\n",
      "torch.Size([80416])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_size(model, s, n, n_chars, temperature):\n",
    "\n",
    "    # fixed-size input\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        # Pad the input, because `get_data_tensor` will generate no data\n",
    "        # if the input is less than `2 * n_chars` characters long.\n",
    "        chars = get_data_tensor(s + ' ' * n_chars, n_chars)\n",
    "        preds = model(chars, temperature)\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()    \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_chars = n_chars\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars, temperature=1):\n",
    "\n",
    "        # Reset hidden state at each mini-batch\n",
    "        hidden_state = torch.zeros([len(chars), self.n_hidden])\n",
    "        if GPU:\n",
    "            hidden_state = hidden_state.cuda()\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden_state = torch.tanh(self.hidden_weights(input + hidden_state))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden_state) / temperature, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)\n",
    "if GPU:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.23   test_loss: 2.05\n",
      "\n",
      "sample T=0.2: je ne sans de les partient de les de coure de les res de partion de la partiter de la mais de le partier de la part de la partiter de les partions de rement de le part de les le coure de la mais de le ce pour\n",
      "\n",
      "sample T=0.5: je ne sans crant de vare de nour ce preura au de rent de pour cour sinite couparis de pour de Steur ce parmient et de ville de res ou mille de la pour de serais de pret dé les dé est de conde de parté acce co\n",
      "\n",
      "sample T=0.7: je ne sans vouse villinité en une de na rans parter a 60 les réfinier à le des seraite cuite de saine chait de le parle reux et en en bayager les cour les nour ine de ret de Nou pour prid cinite êmiziet mais \n",
      "\n",
      "sample T=1: je ne sans la fais moillont chimille). crèsier à maricèt=pleupousse pitraine gridéemin Vorrai sient en en paur il e sur pliés bant que  Jes cuter avoute un plus langtes détreux cuatiter durier lament estiel s\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.99   test_loss: 1.96\n",
      "epoch:   3   train_loss: 1.93   test_loss: 1.92\n",
      "epoch:   4   train_loss: 1.90   test_loss: 1.89\n",
      "epoch:   5   train_loss: 1.87   test_loss: 1.87\n",
      "epoch:   6   train_loss: 1.85   test_loss: 1.85\n",
      "epoch:   7   train_loss: 1.85   test_loss: 1.85\n",
      "epoch:   8   train_loss: 1.84   test_loss: 1.84\n",
      "epoch:   9   train_loss: 1.83   test_loss: 1.84\n",
      "epoch:  10   train_loss: 1.83   test_loss: 1.84\n",
      "\n",
      "sample T=0.2: je ne sans de la mais de la cartant de la part de la contine de la contine de la contine de la restauce de la restauire de ces part les pour de la reste des part de la contine sur les part de la reste en car \n",
      "\n",
      "sample T=0.5: je ne sac de l'arriver les deparde je suis inférie coures de velles des rier pour à la res pas les pardés les chances de la solinais pour ser dis la sans de ce par au mais de fin sur le mais et se des cartale\n",
      "\n",
      "sample T=0.7: je ne sans les armes a fini et du du le ciens delmant à prendre ne pas indérielles le préréhiens à au lifé de cont sur de les tres sur des part tourise est sur de pas sans inté le la poupé en cadier des beli \n",
      "\n",
      "sample T=1: je ne sais des rérits de trassager de Pleux me pour sur à les sois mêmes les parcé). Je le dollys des Parglement riigation pour les cout.w où se récité la plod ou mais diffice dans l'éler éthandérégnais du pe\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  12   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:  13   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  14   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  15   train_loss: 1.81   test_loss: 1.83\n",
      "epoch:  16   train_loss: 1.81   test_loss: 1.84\n",
      "epoch:  17   train_loss: 1.82   test_loss: 1.82\n",
      "epoch:  18   train_loss: 1.81   test_loss: 1.83\n",
      "epoch:  19   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:  20   train_loss: 1.81   test_loss: 1.83\n",
      "\n",
      "sample T=0.2: je ne sans de contre de la se plus les par les pour les conces de la consimples de l'au mais et de la contine de contre de l'au ce suis de la pas les conces de la part part de la conce de constant de challe d\n",
      "\n",
      "sample T=0.5: je ne sans ici pas les parts argent de se pas les pour les contilles à ce mais en plus de chalat de pour les car seule de cample arrive des grand au contre et de seillement plus de les Australis pour trouver \n",
      "\n",
      "sample T=0.7: je ne saufferre qui pans encoré ou tages sac l'ont tend seul des centiais des car les les conscilul de l'ont continailler un volle nous servit d'habitée ne la minures serent d'assielle aide étant des sanse su\n",
      "\n",
      "sample T=1: je ne sanges \" sens mon jes peisir où ceur dans le pagriers. apréscrirer de t'a conce. Dand pour le faires avec arrises revance pas aisons en duprences.  Lissi qaranes indé à quitailai des grâvera pens et il \n",
      "\n",
      "\n",
      "CPU times: user 1min 40s, sys: 1.4 s, total: 1min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    test_loss = test_loss_sum / test_batches_nb\n",
    "        \n",
    "    print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "\n",
    "        for temperature in (0.2, 0.5, 0.7, 1):\n",
    "            print(f'sample T={temperature}: ' + generate_fixed_size(model1, 'je ne sais pas'[:n_chars], 200, n_chars, temperature))\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**.\n",
    "\n",
    "![](img/rnn_variable.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_state)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch, but not its history\n",
    "            self.hidden_state = Variable(h)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_state = torch.zeros([1, bs, self.n_hidden])\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')\n",
    "if GPU:\n",
    "    model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.10   test_loss: 1.91\n",
      "\n",
      "sample T=0.2: je ne sais pas avoir de marche de la pas de mon pas de la par le route de la contre de la pas de la contre en mais de la contre de la couvers pas de la par les route de la par le par le conne se la contrant de par \n",
      "\n",
      "sample T=0.5: je ne sais pas propies et pas des plus en comme mon au mur la restant les comment la ruit villes de la pas pas et du ville par la faire se le bons par la voillec nous contre de rentre a régrant dans la contre de de\n",
      "\n",
      "sample T=0.7: je ne sais pas dans les pour au s'est les comme que je nous cette jours partures en cettente avec la chation à la rendut plus par la cau, le ma parts de reste de chafin mon poind le romir moins la Mon probe de par \n",
      "\n",
      "sample T=1: je ne sais pas d'un cetteils en en allomés mévendre justrer en perfforde attenie, la vallente de trauvellez de vermant verse peelumicuanneys peussons la Biceux exilter des fine son aucompheau.) cortrisie au mat tom\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.88   test_loss: 1.85\n",
      "epoch:   3   train_loss: 1.84   test_loss: 1.83\n",
      "epoch:   4   train_loss: 1.82   test_loss: 1.82\n",
      "epoch:   5   train_loss: 1.81   test_loss: 1.81\n",
      "epoch:   6   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   7   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   8   train_loss: 1.79   test_loss: 1.80\n",
      "epoch:   9   train_loss: 1.79   test_loss: 1.80\n",
      "epoch:  10   train_loss: 1.79   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas pas pas de la repas en avec les complète de la connais de la peut partien de mon au mon au mon avec les pays de la route de la rue de la soleil et les partions de la roule de la manger de la route de\n",
      "\n",
      "sample T=0.5: je ne sais pas par les petit mon avec ma dans le par le centaines de la fait de partie que je n'ai pas par de la proprisses et de la rempar la mette dormir des parler comme mais connaissant de me pas pour partien d\n",
      "\n",
      "sample T=0.7: je ne sais pas et et mais la voyagera et en 2 proprieux d'une ville parfois de caffre le bus par les de l'avonna dont seul mer dans le bord mon ce marche, avec listacme sommes ce sortes et et de l'Argentiers de mon\n",
      "\n",
      "sample T=1: je ne sais pas le termporté de miensyons la vallon dans la balere Mois froudre et dormaux dans un une problète en sur la troute reple vision, nous Mestier d'endroijets. Ilsssibles (paurs les reademagno un espets) J\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.79   test_loss: 1.79\n",
      "epoch:  12   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  13   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  14   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  15   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  16   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  17   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  18   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  19   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  20   train_loss: 1.78   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas de sont pas particle de plus de mon au mon avons de la plage de la partie de la route de la cours de la partie de la reprendre de la retrouver les partie de la reprendre de la permette de propose de \n",
      "\n",
      "sample T=0.5: je ne sais pas de visiter par le car le partie de la chante apprendre la chance de l'autre en suis de mon construit de prophtutions le fait un peut pas de l'autre de plus ou de la cuisine de par les partie de pas d\n",
      "\n",
      "sample T=0.7: je ne sais pas couper pour se marche en arriver nous ne suranivais il n'a décides. Il sauss plus et le vers les nord en bus mal à de renconstableurs quelques pays au ce milie, à la finalites au NEu mais nous soit p\n",
      "\n",
      "sample T=1: je ne sais passant et qui plutôme. Je me mettena aveau ont des  faucoup, gros passent le malage, et tostorter de l'internet qui sybs semble permettr proghorents. Bière de faut passes les Eume et (déjà que je nous f\n",
      "\n",
      "\n",
      "CPU times: user 18.7 s, sys: 2.44 s, total: 21.1 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model2, optimizer2, criterion2, bptt2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt3 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')\n",
    "if GPU:\n",
    "    model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.98   test_loss: 1.76\n",
      "\n",
      "sample T=0.2: je ne sais pas de la route de la ville de la pour le sent de contre de la ville de la par les pour le semps et le sont par le par la contre de la sont pour de la vie de la faire de la cons pas de la partie de la re\n",
      "\n",
      "sample T=0.5: je ne sais pas pas de trouce pour les pour peux de mon passant, et jours de la ravant qui ville et se visiment cons peut pour sont pas a me dont que le marche avec le mon pres pas de coune de peut pour de mon pays \n",
      "\n",
      "sample T=0.7: je ne sais pas le Vent de nors de qui aurance en procourobin de pronoter de c'est pour pour des sont de me perment une vender du habite et sans dictionne à proppalie de la mois de cons pas sur nors pour le-commenco\n",
      "\n",
      "sample T=1: je ne sais pas sur un table, quans en Cho, je côtent voyre Vous à nous qu'énale de me morame du pors ochefosi dans fous sanser de côtéments. Ilement, et les bouri muis pembo miment sur les goup la crèsons me flus q\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.71   test_loss: 1.67\n",
      "epoch:   3   train_loss: 1.65   test_loss: 1.64\n",
      "epoch:   4   train_loss: 1.62   test_loss: 1.62\n",
      "epoch:   5   train_loss: 1.61   test_loss: 1.61\n",
      "epoch:   6   train_loss: 1.59   test_loss: 1.60\n",
      "epoch:   7   train_loss: 1.59   test_loss: 1.59\n",
      "epoch:   8   train_loss: 1.58   test_loss: 1.59\n",
      "epoch:   9   train_loss: 1.58   test_loss: 1.58\n",
      "epoch:  10   train_loss: 1.57   test_loss: 1.58\n",
      "\n",
      "sample T=0.2: je ne sais pas par le pays et d'arriver le pays et de mon peut de la plus de le pays et le contrôle de la plus le plus de la rencontres par le voyage de la conducte de la petit de la couple de la contre que le pays\n",
      "\n",
      "sample T=0.5: je ne sais pas de mon partie de la nuit de notre par mais car le monde et par les travaile en semain, je serait ou d'un peut lors au mois en propres, un peu de moins de le soleiller à pendant les maison par les pre\n",
      "\n",
      "sample T=0.7: je ne sais pas internes sur les présents par le aucilles de la petit que je notres à l'accordé, le rependant le moins accompagne de trouver le lendement la voyager montag Marrant préférer mon aller de me par le pay\n",
      "\n",
      "sample T=1: je ne sais pas pour une aps fil improgrit, pas de le pays et il déjeur nous accud à la monde ou repas. . . Je motche dans le guiche et Guis transporte en vélos et visiser pas la côté s'ables égalles de 8hk-Mod dire\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:  12   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:  13   train_loss: 1.57   test_loss: 1.57\n",
      "epoch:  14   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  15   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  16   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  17   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  18   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  19   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  20   train_loss: 1.56   test_loss: 1.57\n",
      "\n",
      "sample T=0.2: je ne sais pas au pouvoir pour le transportant de la contraite de la proprement de moins par le cours de propres de la contraite de la contraité de la propres de la plus par la concette et de la plus par les partie\n",
      "\n",
      "sample T=0.5: je ne sais pas me de connaire de monter des petit par par des finalement pas de minussitir la compagnons dernières de tout le régales par un refusions de la conductions de déposer de mon sac d'accate de l'Airure ro\n",
      "\n",
      "sample T=0.7: je ne sais pas poser par la nuit, il m'asses sous le française dans les métrougle alors passent tout le Sanite de l'arrière sans mon se des soire panse un coupler les 44 ans de monde de l'arrées l'étais moins plus \n",
      "\n",
      "sample T=1: je ne sais pas en magnos au moins à quelques irmons en ecart hongaltitu des moins du lidande. Noyage et notre balimesse entosse d'accès Ofaires : Ohat\" de douvre chauder-vient pourtis solgut, nuitant, etc. Je m'ent\n",
      "\n",
      "\n",
      "CPU times: user 18.5 s, sys: 2.64 s, total: 21.2 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model3, optimizer3, criterion3, bptt3, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "![](img/lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_fac, n_hidden):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fac = n_fac\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.input_input_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_input_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_forget_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_forget_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_cell_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_cell_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_hidden_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \"\"\"\n",
    "        `x` (input) is of size `bs * n_fac`\n",
    "        `h` (hidden state) and `c` (cell state) are of size `bs * n_hidden`\n",
    "        \"\"\"\n",
    "\n",
    "        # Forget relevant bits of the cell state\n",
    "        forget_state = torch.sigmoid(self.input_forget_weights(x) + self.hidden_forget_weights(h))\n",
    "        c *= forget_state\n",
    "        \n",
    "        # Update relevant bits of the cell state\n",
    "        input_state = torch.sigmoid(self.input_input_weights(x) + self.hidden_input_weights(h))\n",
    "        cell_update_state = torch.tanh(self.input_cell_weights(x) + self.hidden_cell_weights(h))\n",
    "        c += input_state * cell_update_state\n",
    "        \n",
    "        # Forget relevant bits of the hidden state with the cell state\n",
    "        hidden_update_state = self.input_hidden_weights(x) + self.hidden_hidden_weights(h)\n",
    "        h = torch.tanh(c) * torch.sigmoid(hidden_update_state)\n",
    "        \n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_cell = LSTMCell(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        hidden_state_history = []\n",
    "        # RNN loop on `input` of size: `bptt * bs * n_fac`:\n",
    "        # bptt times for each `x` of size `bs * n_fac`\n",
    "        for x in input:\n",
    "            hidden_state, cell_state = self.lstm_cell(x, hidden_state, cell_state)\n",
    "            hidden_state_history.append(hidden_state)\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(torch.stack(hidden_state_history))\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt4 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LSTM(n_vocab, n_fac, n_hidden)\n",
    "if GPU:\n",
    "    model4 = model4.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = torch.optim.Adam(model4.parameters(), 1e-2)\n",
    "criterion4 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.94   test_loss: 1.66\n",
      "\n",
      "sample T=0.2: je ne sais pas de la conner de la ville de la conne de la profite de la constraliens par les autre de la fait le part de la rue de la consait de la plus de la contre de la piens par les par le plus de la proche de \n",
      "\n",
      "sample T=0.5: je ne sais pas de suinde de la ville par les plus par le mon a plus en plande de l'artera peu de l'aime, mon couper ce de la celle de vous fait par le pas de parc. Après pas de la trance avec une passé de l'Husion \n",
      "\n",
      "sample T=0.7: je ne sais pas couple pas que je cessé de la chaup de l'accomps de la grantiers que nas pour les passer la nuit. Nous avec le suptais le travon renconstradion n'ai par un trop me fenint de même en autres profier ti\n",
      "\n",
      "sample T=1: je ne sais pas d'allais ses sa plus, ou sudne qun mon noit-cament heurep 1°, je tar à trai lignement imment, accelsir en stop en centainex nous aurent le reversé pleur est entrez Shos d'aistrage à boit, duvet touve\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.60   test_loss: 1.55\n",
      "epoch:   3   train_loss: 1.52   test_loss: 1.51\n",
      "epoch:   4   train_loss: 1.48   test_loss: 1.48\n",
      "epoch:   5   train_loss: 1.46   test_loss: 1.46\n",
      "epoch:   6   train_loss: 1.44   test_loss: 1.45\n",
      "epoch:   7   train_loss: 1.43   test_loss: 1.44\n",
      "epoch:   8   train_loss: 1.41   test_loss: 1.43\n",
      "epoch:   9   train_loss: 1.41   test_loss: 1.43\n",
      "epoch:  10   train_loss: 1.40   test_loss: 1.42\n",
      "\n",
      "sample T=0.2: je ne sais pas de la coupe de la connaissants de la plupartis de la ville de la route de la connaissants de la plus tard de conséque de la connaissaniens de la ville de la frontière sur le camping de la ville de la\n",
      "\n",
      "sample T=0.5: je ne sais pas le camportes passer les passé de me sont pas découvertes de la ville de la maison en voyageurs : le monde en a dormir de la ville de la programe d'abord par son plus par les récos de la magnifique si\n",
      "\n",
      "sample T=0.7: je ne sais pas par le voyageurs d'entre j'abative en Australie en voyageur couchir de poir le soleisent dizaines. Les village de dormir tous les soleil de seul familles on aventure un peu de consommation en compagn\n",
      "\n",
      "sample T=1: je ne sais pas apporte ja portu. En arrive de découvriules fut climentaire c'est en voyageurs de satible sur les épais nous lors de Népud, détrouvaies bien semble par en Asie, et semacantulas en millieus juste en r\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.39   test_loss: 1.42\n",
      "epoch:  12   train_loss: 1.39   test_loss: 1.41\n",
      "epoch:  13   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  14   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  15   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  16   train_loss: 1.37   test_loss: 1.41\n",
      "epoch:  17   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  18   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  19   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  20   train_loss: 1.36   test_loss: 1.40\n",
      "\n",
      "sample T=0.2: je ne sais pas de la pluie de la fois de la planer le pays nous avoir avec le point de la pluie de la fois de la pluie de la pluie de la pluie de sans passé de la pluie de la pluparaises de la pluie de la compagnie\n",
      "\n",
      "sample T=0.5: je ne sais passé de la ville de mon pense pour le moins plus en stop de piedent sur le conducte au long de la route de la mer rizie. Le point de camardant au moins de passe de parler de partir de la ville avec le c\n",
      "\n",
      "sample T=0.7: je ne sais pas de connaissants de lumede coute de cette au lits dans le rejoindre rejoindre indioctés de sable au connaissaniennes de la ma femme ! En espida, arriver le moins de partir en coup de cabane se repas à\n",
      "\n",
      "sample T=1: je ne sais pas du pauture si le vie n'avait envie de litoori, comme grasse de prendre pour dors yea en sait, elle, prévéhent bien simple. Quelques sortir dormirser la voyageure (allare d'équitops de faire une vie e\n",
      "\n",
      "\n",
      "CPU times: user 1min 21s, sys: 800 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model4, optimizer4, criterion4, bptt4, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/stacked_lstm.jpg)\n",
    "\n",
    "Stacked LSTM (`torch.nn.LSTM`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, n_layers, dropout=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, n_layers, dropout)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 512\n",
    "bs = 1024\n",
    "bptt5 = 300\n",
    "n_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = PytorchLSTM(n_vocab, n_fac, n_hidden, n_layers, dropout)\n",
    "if GPU:\n",
    "    model5 = model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer5 = torch.optim.Adam(model5.parameters(), 1e-2)\n",
    "criterion5 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.49   test_loss: 3.05\n",
      "\n",
      "sample T=0.2: je ne sais pas   et  ee e    ee  eue   e    e e   eu  e  ee   e    e  ee       e es    e ee  es    eu  er  ee  e e    ee  ee  e   en e  e      ee e  e et  ee  ee  eu  ee   e es   e    e    e  e  ee     es  eu ee  e\n",
      "\n",
      "sample T=0.5: je ne sais pasre tai' es ee  eueue   eau ne   eu auorn et   uin si   n toe tte   et eotur ltaere tne  sf ttee rntttnn  p eros uutenltu   opi.n  re   ,so tsssu mseo e.anruet  no et  enueseo  te uoueu     t ureerue' \n",
      "\n",
      "sample T=0.7: je ne sais pasuu luer teaes etTlte :nuf el soaoiseri ttumu.e runC 2tareea he tsrsoés pe.x rsdsgsn nnssniuenx  s  rnersm lt o.  erneeertussnjtc .oe.ees e'u rnasute e nrl rt bo 2r ee .u  on ns vn eeud jnn jue l a e :\n",
      "\n",
      "sample T=1: je ne sais pasno èItoe wtonms  rt serslLounpen'prstté f uelepeem steubsrPu.mdt see gml -auhicuuit2lsà'0urur  t J! kernse zme sda xotWxetr4s3o 4ilPdutvéitmt e Cuà0uD iuooutcu -eaeneolmfdets v e,a \"  usimt a,re sp nn\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.87   test_loss: 2.65\n",
      "epoch:   3   train_loss: 2.57   test_loss: 2.43\n",
      "epoch:   4   train_loss: 2.35   test_loss: 2.24\n",
      "epoch:   5   train_loss: 2.17   test_loss: 2.07\n",
      "epoch:   6   train_loss: 2.02   test_loss: 1.93\n",
      "epoch:   7   train_loss: 1.88   test_loss: 1.80\n",
      "epoch:   8   train_loss: 1.76   test_loss: 1.69\n",
      "epoch:   9   train_loss: 1.67   test_loss: 1.61\n",
      "epoch:  10   train_loss: 1.57   test_loss: 1.53\n",
      "\n",
      "sample T=0.2: je ne sais pas de la problème de la part de la plus de passer le mais en voiture de la peut pas de la pays de l'autre pas de la passer le propre de l'autre des autostop et de la peut pas de la peut pas de la part d\n",
      "\n",
      "sample T=0.5: je ne sais pas instéparer de construite et des conducteurs passer le serait pas le montagne et le maison de la reste des pays en sur le pris de passer le voyage de véhicule de soit en passie passer la rancoire de m\n",
      "\n",
      "sample T=0.7: je ne sais pas tout nous comment ce qui tenter le place de sur le taler tous les froit pour bien des ésibles sur le tuit que la rentrerai dans le concert à comp pour se faire tente. Nous passates par les adberger q\n",
      "\n",
      "sample T=1: je ne sais pas plus argent, l'agricier. Il m'iissant au penger leur avec gropes et la cors à batande. Je saussisme du Jouaraiche du vélée dans la œuis, m'a jouer à l'électrer sere 21 foute trop aux r'une centrerai \n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.50   test_loss: 1.49\n",
      "epoch:  12   train_loss: 1.44   test_loss: 1.42\n",
      "epoch:  13   train_loss: 1.38   test_loss: 1.37\n",
      "epoch:  14   train_loss: 1.34   test_loss: 1.35\n",
      "epoch:  15   train_loss: 1.31   test_loss: 1.31\n",
      "epoch:  16   train_loss: 1.27   test_loss: 1.28\n",
      "epoch:  17   train_loss: 1.24   test_loss: 1.30\n",
      "epoch:  18   train_loss: 1.24   test_loss: 1.26\n",
      "epoch:  19   train_loss: 1.20   test_loss: 1.24\n",
      "epoch:  20   train_loss: 1.18   test_loss: 1.23\n",
      "\n",
      "sample T=0.2: je ne sais pas de montagne de consulat de la pluie de consulat de la route en compagnie de la compagnie de la population de l'autostop en compagnie de la course de la ville de la pluie et de la connaissance de la c\n",
      "\n",
      "sample T=0.5: je ne sais pas plus de pain à l'autre des conducteurs de la main et seulement avec des paysages de place d'avoir de la compagnie de l'Argentine. C'est la capitale de l'autre avant de la chance de la pluie de pouvoi\n",
      "\n",
      "sample T=0.7: je ne sais pas en faille de  China et le Moins Bouladel et qui se va voir les plus, soit d'avoir l'autorisation de Kamusi on se semble de l'auto-stop, tout de l'Inde spétation de gabe et d'autres fraicheurs avant d\n",
      "\n",
      "sample T=1: je ne sais pas en exclimatique pour un réparation. Après y eu le lisible d'apprentissage à font correci. C'est donné quel jour rentrer consulat. . . S-LoL Visi Ban, la suite de la fin d'aller fut trop marétera 2/10\n",
      "\n",
      "\n",
      "CPU times: user 2min 3s, sys: 44.4 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.17   test_loss: 1.22\n",
      "\n",
      "sample T=0.2: je ne sais pas de partir de la construction de la construction de la pluie de la construction de la capitale de la partie de la compagnie de la course de la capitale de la plage de la coupe de la pluie pour le pays\n",
      "\n",
      "sample T=0.5: je ne sais pas de marche dans le forme de l'hospitalité 100000 Riogre. De la pluie en rencontre de la campagne de marche et la vie en plein matin et je rencontrerai un pays où il est de la plupart de la première vi\n",
      "\n",
      "sample T=0.7: je ne sais pas facile de cette avertie de l'orage de Vanimo. Le poids de nous avait discuter à Biocto (Australie et d'abriter de la campagne de mètres de situation, il m'a pas remplacé par le fille de faciliter de \n",
      "\n",
      "sample T=1: je ne sais pas. Ils : prenant 2 ans... Il curte, égognant, mais ma vie de m'est que la langue américain où il est artité pour le (norn contre Sal Hédana. Bien visiter l'accueil et seulement le maroc, chirra 3 heure\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.15   test_loss: 1.22\n",
      "epoch:   3   train_loss: 1.14   test_loss: 1.21\n",
      "epoch:   4   train_loss: 1.13   test_loss: 1.20\n",
      "epoch:   5   train_loss: 1.12   test_loss: 1.19\n",
      "epoch:   6   train_loss: 1.10   test_loss: 1.18\n",
      "epoch:   7   train_loss: 1.09   test_loss: 1.18\n",
      "epoch:   8   train_loss: 1.08   test_loss: 1.18\n",
      "epoch:   9   train_loss: 1.07   test_loss: 1.19\n",
      "epoch:  10   train_loss: 1.07   test_loss: 1.17\n",
      "\n",
      "sample T=0.2: je ne sais pas de mon passeport en Australie et le coup de la plage et de la police et le coucher de l'autoroute que j'ai déjà passé les prochaines décentes de marche de la pluie pour la première fois de la place d\n",
      "\n",
      "sample T=0.5: je ne sais pas de calme. Je suis en passant le concept des villages de l'Europe et la police est très bien faire du stop en direction de l'ambiance de l'Australie ? . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "sample T=0.7: je ne sais pas très tres paté pour nous avoir passer la mes terrain de marche de mon marché, et rapprochement dans le trajet à l'eau pour reprendre l'expérience de la murs de poules ! En retour à m'avait dire que c\n",
      "\n",
      "sample T=1: je ne sais pas de bus pour excrupier 10 heures à traverser la journée sinon étant pour rejergro juste... Comme nous avons réveillé quelques \"En Nouvelle Zélandage\", viasi il faut être profiter du monde à la fête la\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.06   test_loss: 1.18\n",
      "epoch:  12   train_loss: 1.05   test_loss: 1.17\n",
      "epoch:  13   train_loss: 1.04   test_loss: 1.17\n",
      "epoch:  14   train_loss: 1.04   test_loss: 1.18\n",
      "epoch:  15   train_loss: 1.04   test_loss: 1.17\n",
      "epoch:  16   train_loss: 1.03   test_loss: 1.17\n",
      "epoch:  17   train_loss: 1.02   test_loss: 1.16\n",
      "epoch:  18   train_loss: 1.01   test_loss: 1.17\n",
      "epoch:  19   train_loss: 1.00   test_loss: 1.17\n",
      "epoch:  20   train_loss: 0.99   test_loss: 1.17\n",
      "\n",
      "sample T=0.2: je ne sais pas de problème et les plantes de la plage, et je suis sur le prochain article... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "sample T=0.5: je ne sais pas encore avec le bateau au bord de la route de la plage et les meilleurs conforts avec un village de la région de la langue. C'est donc le chemin dans les problèmes d'eau de la ville de Tarija et la pr\n",
      "\n",
      "sample T=0.7: je ne sais pas encore plus, pour me faire découvert mon sac-à-dos en essayant de mon esprit par le point de voiture je pourrai plus vendre des excurées de la plage. Nous dormirons elle doit moi je ne souhaite pas p\n",
      "\n",
      "sample T=1: je ne sais pas au pays très itinérairement de base ici, un passage pour nous vient vienner. C'était déjà arrivé ( pour projet pour construire les éclairages \" de sames (locaux), nous voulions avec les 20 minutes, e\n",
      "\n",
      "\n",
      "CPU times: user 2min 4s, sys: 43.4 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Fixed-length RNN': (model1, n_chars),\n",
    "    'Stateless RNN': (model2, bptt2),\n",
    "    'Stateful RNN': (model3, bptt3),\n",
    "    'Small LSTM': (model4, bptt4),\n",
    "    'Large LSTM': (model5, bptt5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 0.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans de la cartes de la contre de par les cample de la mais pour les par la rencontres de la mais de la mais de cample de l'au par les par les par le par la contrais les part de la contre de l'au contre\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas de la retrouver les petit par le mon au minutes de la rencontrer de la rencontrer de la partie de la plus de la ville de la planges de la chance de la route de la personnes de la rencontrer le partir\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas pour par les plus par les plus pour les contraite de la compagnir les problème de la couple de la ville de la plus par le souette de la plus de mon peut de la connaisse de la proposer de la petit de \n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de la connaissant des passer les petites de la plage de particule de la problème de la maison de la plage de la plage de la pluparationnelles de la plage de la policier de la pluie de profiter le sol\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas encore plus de changer de la population en compagnie de mon argent. Cependant les pays ont déjà voyager avec les articles et les prix et les villes de la ville et de la plage et de l'autostop en Aust\n",
      "\n",
      "\n",
      "T = 0.5\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans en mines des par que repusique de me parlent de rencaires de le win, centire de la serai de sur la Gabiliement de suis aille qui seret de pour les contiant de car marches des cette arriser des conf\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas en bien chance pas de la village de ne suis la journée de moins de plus me me deux c'est partir de voyager les miniche de la dépons pas de partie d'un retrouit de mon a la chaque avons propossible et\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas pour les grand et sa matiller de l'autor de rendre de la région de rester les rencontre de refuntes en vais pour un peu par un peu de sans la problèmes en Europe propres avec de la villes en conseill\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de stop pour constructures et la maison. Ce compagniens les Papous de partité de toute la pluparations pour beaucoup de passe pas dans la forêt et de la parc sur la première à traversé et sans donc d\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas a dormir. Le soleil se trouve Carbon car il semble que ce n'est qu'avec moi qui m'a été les problèmes mais le prix de la place dans un bar et les adresses sont trop chers et de problèmes de coupe de \n",
      "\n",
      "\n",
      "T = 0.7\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sa mêmer connant conné procalile qui visaire, jains denuds mais des les fains des beus pas ce partaire des coupuis est au ce suivent pourie pas les dans une candes conces.. Suis camilles, les ou siocole\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas fraiterre les par il sera avec un belle les monde de cart dévers aussi de mon avons soletit en pas apport peut cet squine de chance de la petit son apport les éminant que du peu pas dire sans de musi\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais passé mais le paysier la fait à Mode serois du propre quelques changer rendre et pour les fois par des promement pour camphaltré plus les europement à 500 km de misisté partager de touriste en plus la ca\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas dans la migoliser la prochaine de barration de la forêt. Pas pour ce senou par la plupagoucres du moins au pays. Il a découvrir un lirée avec les petit pendant qu'en Francais et de soleiles de reste \n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas ton ferry à leur voisine de bus pour envie de rejoindre le groupe de délicates au Chili et commencent à des années que je ne bais faire. Je me tarde pas à vivre dans le désert ou à la mer de la profo\n",
      "\n",
      "\n",
      "T = 1\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sac de nar de me journée, il Dans la seurs de nuysura de V ortiques quelques pas. Chaïlie dans pas la ces bangle). : senna un confortai, unillage de touches. Torace deques de rivire désitellezant.. Darw\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas nie il tous les ghait sont bate. J'ai pângérent un écusse sud ne nous soit aussi ça ne finie finet effe. Les artait un -incont. Super aussi des ule de bitent d'autostop Vert. Après un étaîtrer hôteur\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas par nons confiancheur c'estovitatins, mais séchochant. Les sorterie dans la dormir les plus au toutre seulements dépenstongé ou par les porte au pribilté : Valléter (qu'il alaments de voirances. Le m\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de matin suiller couper quand guenis désert d'indiant et me faire que la prendre le premine qu'ils néganaisais sacs pendant ensuite de chéeriens rue riche dans l'aujoures et accordéon de Laira un peu\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas encore dans un coup de temps une seule rencontre sur la rue car Min. Autre intention de l UNESCari Hoibl ou Sorarir en venue : \"D'accompés sur le bateau qui ressemble que le poisson Temedi - Népal a \n",
      "\n",
      "\n",
      "T = 1.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans. C'essé ww part tors bour est à la reile par Bmême de renvit leur cVers àant à leur pour cat ferd, alls est un flusir qu'Ulitépagagant dans 3 Dangnagord il de suis à l'ausque stop. En cult des curd\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais passement quel parce beauvais, il y sectenments crème proficillent matin (squide noue. Je voule noscriver a chambre ads. On épèden du sinfin le sans le beaucoup ligiée et maintalet en naté déjeau présaAu\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas au fère famille fégaires. Un ce aussi-sémmendroz, à jour donc trop (c'est 2 villé étateur de taresthants/moi envables, ec-R Jéni en ta remenguablent à véllanasé à 134 - D Ville parte, où joure parled\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de plus Monipleré d'attendre..)s je vainte. Mon détroit dans le tembre, dans 20km du bus d'Austra évillée finir a une jour avec mon prige. Grâne à payer : le camion isforcée, afin au Sediques) a noca\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas toujours très \" Picting. Alors,, ripis de luxe dans un on donnett'une de palmerant mon grand désert, il fut clair voulouses années en baie licricket du vélo, loin d'amis par le chemin S ?D Ihan, gard\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_s = 'je ne sais pas'\n",
    "\n",
    "for temperature in (0.2, 0.5, 0.7, 1, 1.2):\n",
    "    print(f'T = {temperature}')\n",
    "    print()\n",
    "    \n",
    "    for model_name, (model, bptt) in models.items():\n",
    "        \n",
    "        # Handle fixed-size RNN\n",
    "        generate_func = generate_fixed_size if model_name == 'Fixed-length RNN' else generate\n",
    "        s = initial_s[:n_chars] if model_name == 'Fixed-length RNN' else initial_s\n",
    "\n",
    "        print(f'{model_name}:\\n  ' + generate_func(model, s, 200, bptt, temperature))\n",
    "        print()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je déjà envie de passer la journée et le bordel de la population française et visiter le sol de la famille pour découvrir l'article de l'autostop, avec un camion secondaire, et les moines de marches et des enfants nous avaient promenant avec les marchandises pour comprendre. La prochaine ville est possible de trouver une petite nuit et de son propre plaine qui me dit qu'il y a des toilettes en voitures qui m'avaient pour les chemins de la plage et que je ne suis pas gardé une seule fois dans l'un des plus chanceux au milieu de la compagnie de Papouasie. C'est le plus tard que je n'ai pas l'air d'avoir un peu d'argent de la confiture, le sol est pour ne pas l'aider à faire du stop que je souhaite donc passer la région de Medellin. Sur les problèmes d'argent avec les montagnes en compagnie de sponsor en Australie et me dépose à toute la santé de la compagnie de la conflit abiment de pain avec le temps de départ et de la compagnie. C'est sur le sol, en plein soleil et les amis en pleine maison est très facilement de connaitre l'anglais et la confirme. Les plantes en stop a été les passagers dans le sol, et je m'en ai rencontré un mois en Indonésie. Nous avons donc aussi proposé de passer plusieurs jours de poule, hotel ou de la construction des pieds en marchant dans les transports par les remettrait parfois en prenant le carreau en compagnie de la construction de la pierre. Une ville de porc a été prix aux années de bain de dormir dans la rue il est maintenant de bien sur le barbecue et des voyageurs en pleine mer. A la moitié de la vie en soit le soir et le confort où j'ai traversé les problèmes de petites heures de travers et autres billets. Les conditiens est exprimés pour aider la ville et la propreté avec le repas dans le port et manger dans le contact avec le moins cher. Le charme a la fin du voyage en espagnol et un pays de la montagne. La plage est parfaitement accueillante et le pays de l'autre coté de la capitale. En ai en route à la fois le voyage en Australie, nous avons donc apprendre à travers les démarches administratives et les prix apprécier les temps à cause de la confiture, chargés de construction de pénins de la piscine. Je suis arrêté d'essence et pour le pays en plastique ou de la poleterie avec un panneau. Je me suis retrouvé mon accordéon. Le conducteur m'emmène à la chaleur et la propre route située à l'aube, nous avons donc décidé de dormir en chemin ou de soleil et les légumes de rue ou des temples de musique. La police sera plein de voyage et de camping. Après avoir pris de mon voyage en Amerique du Sud, plus de problème, après une rencontre avec les animaux sans enseigner le matériel à l'autre côté de la route pour une région pour s'adresses à tout complet de sortir de mon voyage en Asie du Moyen et demande sur le conducteur m'apporte de la ville et de chauffagner, les pommes et des sales dans les pays où les pays ont sur la route avec les mains d'années incas d'argent en Amerique du Sud où nous avons eu l'occasion de trouver le moins partagée dans les bois et une nouvelle aventure au chauffeur de marche. Il y a aussi les restaurants et très peu de problèmes et de son aventure au fond de la pente et de créer les pays avec un pays et sur la route de la route de mon conducteur mais aussi à la piste de travail avec le petit tour du monde. Il me dépose un peu de confort pour le rendre pour le pays mais sans succès. En voyageant au film en Amerique du Sud a la maison en français mais sur la montagne et le conversation Unnormatique qui me rend plus de manger un peu plus de liberté qui me demande alors que je me dis que le confort est exceptionnel de son pays mais dans la part des villages où le soleil est très mal de force de la population en des pommes de vie de chemin et de lent, dans le pays et de la tente. Cependant les prix approximatifs est loin d'être anglais en Asie. Nous avons donc passé aussi pour visiter les villes de marche et de ces années \" heures de son pied \" : les photos de sortie de la population locale française \" et le \" travers les visas\" \" pour la porte de la nature et les pays repas dans la maison en sera année de cacher sur le parc et de la plage et de la famille et avec des clients de la population en bambou et de la compagnie de la population en surface et de son aventure à la porte de la ville et de connaissances de la ville et sans changer les voisins qui savent connues et des pieds de plantes de riz et de plus en plus de part en espagnol et de la première expérience en dortoir à l'aide de la prochaine fois. A part le pouce s'arrête a l'aide de la vie en Inde et que je ne sais pas encore plus d'argent en autostop, en Amérique du Sud, c'est de me prendre d'après la première fois ce nom dans un mariage économique pour manger un peu d'argent et de manger très charmant et malgré la maison en France avec mon chemin à une heure de route de point de voyage et de nous proposer de mer et les moustiques et des voitures et avec quelques connaissances de la ville, et en plein délicieux nuits ou des vêtements sont également places à la chambre à la police et de rencontrer un type de sécurité de la ville. Je suis devenu une idée de cet endroit où pour nous aussi sécher le soir au Sydney où j'ai très intéressé à la construction de pouvoir me faire haute de confort pour dormir à la population de repos cher pour atteindre les mois de pluie indienne (ou 2 ans en Pedre \" situé à base de rue apparaissent à tous les ordinateurs et de leur temps et que les problèmes de voyageurs ou au niveau de confiance, et qu'il n'y a pas de régime de travail en Iran à la police et que ce soit en plein dégueule et réceptif de chaque fois que je ne peux pas peur de rejoindre le pays et très chaud et semblent que chez lui. Nous avons traversé le village avec mon accordéon, propre pour faire charmant de cette marche comme la police et les tracteurs et les aliments mais surtout moins de 100 pesos qui s'appelle Bonjour temps et la famille et les grandes villes. A la fois le lendemain matin et son marché de nouveau pour le temps de se faire échanger les camions et de mes connards de forêts de pieds de la planète et de sommeil et de voir le pays et de la propre pays et de la maison en seulement une période de partager avec les grandes quantités de prendre le cap Rein-me donne un peu plus en plus pour les plaisirs que je le sais toujours presque idéale qui vous savez il y a des rochers sont mécaniques en liberté. C'est un léger pour aller au cours de la propre pays avec les pommes et de problèmes de peintures de la mer de la ville. Les copains m'a offrir le pays de la planète \" sans réservé le nom de la maison. Ce que je ne sais pas assez répartissement dans la ferme et le soleil couvert de la compagnie de 650 bolivars (10-20€) mais tout en apprendre à plusieurs années de poulet et moi qui vont suivi de faire de la maison et au marché jusqu'à la prochaine ville idéale du Sud de la forêt. C'est à chercher un repas en ville de France avec ce que j'ai visité ces 2 semaines à 10 km au milieu des rues en autostop, pas de grands marchés de vélo aussi a des animaux dans la rue. Les voisins de sommeil se couche ne peut pas passé sur les conducteurs pendant plusieurs jours passées au soleil et le lendemain matin. Je demande aux cheveux en Indonésie nous attendent avec le moins que je mange les prix en bois et de la pluie et qu'il soit au matelas et de travail en Australie. De plus d'argent sont très longues et de sa famille que nous sommes les voisins de travail à part le plus de police, et je n'ai pas retrouvé mes affaires de la forêt, et pas de retour à l'arrière de l'autostop. La seule reme plein de fruit de la chaleur après le 3 mois de plus de 20 heures pour m'arrêter dans les montagnes en compagnie de mon bon moyen de repos et de la population de famille de Mongolie et les ponts si possible, sans succès. A l'aide de l'autre côté de l'argent et de nombreux maisons en compagnie de la confiture de la nourriture pour admirer les prix en mauvais endroit partagée (ou Alice Springs \" papual \", au soleil ou une poignée de pickup pour rencontrer les problèmes de sympathiques et des chaussures et de lait de succès par tenter des agences de la nature et de son age. Il y a aussi le prix du monde en Australie. Nous avons parfois prendre le pays mais aussi pour continuer les villes sur le bas de ces matelas (pour moi de dormir ne pas marcher sur le confort de la compagnie de la France) avec le manque de problème de marche et pleine de marche et plein de bains de la route et de ce moment locale. Nous avons payé le sommet de la permaculture : le mariage chez eux et les jours de police et facilement de ses parts en Asie, et produit qu'il y a des champs et ont des pommes mais cela me fait plaisir à voyager en Thaïlande et que j'ai payé le conducteur après avoir se rend en sac à dos et demander avec nous et parler de la tente. Je ne sais pas exercé en plus de tente. En effet, ce n'est pas le pays en plein camion de la couleur et la pollution en compagnie de la capitale, le prix de plus de quelques centaines de petits morceaux de routes et les conducteurs avec sa femme et de l'argent. Le voyage est le pays le nom de la ville il y a aussi l'équivalent de passage et les prix sont très peu de cafés de la paille. La moitié est maintenant au cours de la barrière à l'aéroport de San Pedro, et le 1er minute est assez plus de sandwich avec des années quelques mois en marchant mais les véhicules en policiers en France et appréciable à découvrir la montagne. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Au bout de mon argent : le monde s'est vraiment encore un peu de coupe du monde. Nous avons pris avec des adresses, et je continuerai pour aider le voyage en Amérique du Sud, il est possible de se dépaysant dans les sommets de l'argent et de problèmes d'avion et de plus en faisant les conducteurs de police et de plus en plus en plu\n"
     ]
    }
   ],
   "source": [
    "print(generate_func(model5, 'je ', 10000, bptt5, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate blogger text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_data, test_data, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, n_layers, dropout=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, n_layers, dropout)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2773971"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    wordpress = f.read()\n",
    "\n",
    "len(wordpress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    blogger = f.read()\n",
    "\n",
    "len(blogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(wordpress + blogger)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 4. / 5\n",
    "train_wordpress = wordpress[:int(len(wordpress) * train_frac)]\n",
    "test_wordpress = wordpress[int(len(wordpress) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blogger = blogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on wordpress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 512\n",
    "bs = 512\n",
    "bptt6 = 200\n",
    "n_layers = 3\n",
    "dropout = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = PytorchLSTM(n_vocab, n_fac, n_hidden, n_layers, dropout)\n",
    "if GPU:\n",
    "    model6 = model6.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_wordpress = get_data(train_wordpress, bs)\n",
    "test_data = get_data(test_wordpress, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.35   test_loss: 3.07\n",
      "\n",
      "sample T=0.2: je ne sais pass    e  e      e      e  e  e n      s       e   e         e   e   e   e     e   a e      r    e     tr    e        e  u  e       e e            u o           e             a   ee e    e    e ee   e  \n",
      "\n",
      "sample T=0.5: je ne sais pasu sesuild  e eroea oh eeo ea    enena  a i uereru u  lee a u   aen   nme e   e 'aou a s   isusen  la raiee enilteani ele atue  o e  niituu are uuo leu  nl neels n eutpe   t s   nee neeeu cea npss  e e\n",
      "\n",
      "sample T=0.7: je ne sais pasl u tbreia rusre nt'ire neavsupl tcn ari  é  nalsnaa  oeDilneelubeqolaiea  en pec nie na ieovourpeua,o lpunile  tmnit      r elannean mtéo   e t  éine ne e uasretn ateulu  ieeuttre orJlnlecopnalr ns r\n",
      "\n",
      "sample T=1: je ne sais pasuuasdmsir fssllprasncese.rereeie rtt'pnrOidqe rsqrarv  t  e   'ndeat   rsol dnngro elrnotsp ine.aiju  tllra  boi eDaus, . e neo ninefmacé 'eaee leosy u(iar, oal  uFssnnooé wn g seivl s  otaerei yotrt \n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 3.06   test_loss: 3.06\n",
      "epoch:   3   train_loss: 3.02   test_loss: 2.96\n",
      "epoch:   4   train_loss: 2.92   test_loss: 2.85\n",
      "epoch:   5   train_loss: 2.82   test_loss: 2.82\n",
      "epoch:   6   train_loss: 2.76   test_loss: 2.73\n",
      "epoch:   7   train_loss: 2.72   test_loss: 2.67\n",
      "epoch:   8   train_loss: 2.67   test_loss: 2.62\n",
      "epoch:   9   train_loss: 2.58   test_loss: 2.53\n",
      "epoch:  10   train_loss: 2.56   test_loss: 2.51\n",
      "\n",
      "sample T=0.2: je ne sais pass de pe pe pe pe le pour le pour le mou de pons pe le cons de pe poure de poua de pous en se pe pour le tons de pe pout de pous de pous de pa se le mou de le le mons de mou de pour de pont de pour pe \n",
      "\n",
      "sample T=0.5: je ne sais pass ans les se se poiter de poys de pee du mouv j la le pour le le cous pe nout de po se co pe ser aarie ride pous io c la mou du maite de sour en pare en e le aon le le sais de pe pous du se ens nais l\n",
      "\n",
      "sample T=0.7: je ne sais pass coup 1nte tite le mnt ner se be laes aons pet etent ua pe ph mo sos 1n guaetie truor de cout pue suonog du luu pe vé lla mnnt, eour la satra. . Prou Sor et àk a cariante àr ou p pte de eou ue pa se \n",
      "\n",
      "sample T=1: je ne sais pasf dalnnis fe le Eey pe air li senes c5A paisse eb deyent du \"erias dulos nrotle ruscm. iaeit csaait des s le lagri en tag iourse duserr c luers damswige eh ner 3aur (ort naob me ce sa 3aurv Dila das .\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 2.47   test_loss: 2.42\n",
      "epoch:  12   train_loss: 2.48   test_loss: 2.40\n",
      "epoch:  13   train_loss: 2.38   test_loss: 2.34\n",
      "epoch:  14   train_loss: 2.33   test_loss: 2.28\n",
      "epoch:  15   train_loss: 2.28   test_loss: 2.24\n",
      "epoch:  16   train_loss: 2.30   test_loss: 2.22\n",
      "epoch:  17   train_loss: 2.21   test_loss: 2.17\n",
      "epoch:  18   train_loss: 2.17   test_loss: 2.13\n",
      "epoch:  19   train_loss: 2.13   test_loss: 2.09\n",
      "epoch:  20   train_loss: 2.12   test_loss: 2.06\n",
      "\n",
      "sample T=0.2: je ne sais pas le vour le varient de la buit de la vois avec le souve de la vous aus de la cons de la consire de la consire de la vous au mon de la voute de la consit de la consons de l'ant de la partion de la cons\n",
      "\n",
      "sample T=0.5: je ne sais pas de la vorte ce qu varement cours ins pouvales au pouver la joure ou nore de la contre qe Serte en en cotuée de la conde l'ai st vays que le consille disont la foutes et qu'er les quelle de la voute l\n",
      "\n",
      "sample T=0.7: je ne sais pas a'ec couner à la s Erants du sons me la vlec.e de son glus de voure plus au Sais seur et pie trous le fourrions sans toule. Lous harc Ca pos et plus de rendiner l'ont eu bue vlus de la coile et la ce\n",
      "\n",
      "sample T=1: je ne sais pas mlasiquar,ebts du chezses les n' prapkour pas du pag dér de commail dou ences, suais en ca se fonre. D'avie Cerancefnrel l' Fuca- se toui logtu nractetes dant,ons feurs ce nous le pagse. . saliemmer \n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 2.07   test_loss: 2.03\n",
      "epoch:  22   train_loss: 2.03   test_loss: 2.00\n",
      "epoch:  23   train_loss: 2.01   test_loss: 1.97\n",
      "epoch:  24   train_loss: 2.01   test_loss: 1.95\n",
      "epoch:  25   train_loss: 1.96   test_loss: 1.92\n",
      "epoch:  26   train_loss: 1.92   test_loss: 1.89\n",
      "epoch:  27   train_loss: 1.89   test_loss: 1.86\n",
      "epoch:  28   train_loss: 1.87   test_loss: 1.84\n",
      "epoch:  29   train_loss: 1.84   test_loss: 1.82\n",
      "epoch:  30   train_loss: 1.86   test_loss: 1.81\n",
      "\n",
      "sample T=0.2: je ne sais pas de la propres de la plus passer les proprons de la procher les propres et les proport de la plus de la chance de l'autre les properse de la plus de l'arriver de la plus de la propres de la plus de la\n",
      "\n",
      "sample T=0.5: je ne sais pas autre pour veures de la refans pour nous avoir la contre quelques beur sur les provons nang fais de profésien et en cons avec des autoche de pour de la traicer en proche de la proure du voyager de ch\n",
      "\n",
      "sample T=0.7: je ne sais pasté qui n'onment mprécortie un de la proprre, en Aiver le proprant mon rien pas la ville, de l'auzons de l'ensonera \"ne travart de vonts de les travaires aventre sur les pars soute o anne de l'artiver \n",
      "\n",
      "sample T=1: je ne sais pas buaucoupgesireurs petit buand neu de rimpresse ru 4ujrai le goir.... Jes probuile,u, il n'y encouv:c le dechertagle entiment les passe en sqécoup qe tenne3rené de dormtidemaction. Je jouer l'en .ourr\n",
      "\n",
      "\n",
      "epoch:  31   train_loss: 1.81   test_loss: 1.78\n",
      "epoch:  32   train_loss: 1.79   test_loss: 1.76\n",
      "epoch:  33   train_loss: 1.78   test_loss: 1.75\n",
      "epoch:  34   train_loss: 1.77   test_loss: 1.73\n",
      "epoch:  35   train_loss: 1.73   test_loss: 1.71\n",
      "epoch:  36   train_loss: 1.74   test_loss: 1.70\n",
      "epoch:  37   train_loss: 1.70   test_loss: 1.67\n",
      "epoch:  38   train_loss: 1.71   test_loss: 1.67\n",
      "epoch:  39   train_loss: 1.66   test_loss: 1.65\n",
      "epoch:  40   train_loss: 1.64   test_loss: 1.63\n",
      "\n",
      "sample T=0.2: je ne sais pas de la prochaine et l'autre partie de la plus de la construe de la route de la région de la route par le connetter le construction de la pluie et les partiens de la plus de la rendre de la plus de la \n",
      "\n",
      "sample T=0.5: je ne sais pas présent dans la nuit et l'abgent pas les chois pour particile de la voyage de la ville de la plus de plus parle de l'abrivant en par la réeau de chance de la voire de la rencontrer le transport des t\n",
      "\n",
      "sample T=0.7: je ne sais pas de moint de totre perq mon fabte, de la zonne dans les plus des cœercatiens son arièis dans les aiser et je nous aident des toujours de chur sur la rueur sarlente  encourraiterais partie souher le ca\n",
      "\n",
      "sample T=1: je ne sais passare mynricile d'ue contre du l'ex paysiercieux de la mointer 10 irons confeurs avions en strdos nous on fraichent chez. En jours Sinituder avec une doutettique de fraire et ce que c'est loceuteoud ma\n",
      "\n",
      "\n",
      "epoch:  41   train_loss: 1.62   test_loss: 1.61\n",
      "epoch:  42   train_loss: 1.62   test_loss: 1.61\n",
      "epoch:  43   train_loss: 1.61   test_loss: 1.59\n",
      "epoch:  44   train_loss: 1.59   test_loss: 1.58\n",
      "epoch:  45   train_loss: 1.58   test_loss: 1.57\n",
      "epoch:  46   train_loss: 1.59   test_loss: 1.57\n",
      "epoch:  47   train_loss: 1.56   test_loss: 1.55\n",
      "epoch:  48   train_loss: 1.54   test_loss: 1.54\n",
      "epoch:  49   train_loss: 1.53   test_loss: 1.53\n",
      "epoch:  50   train_loss: 1.53   test_loss: 1.52\n",
      "\n",
      "sample T=0.2: je ne sais pas que le pourtains de la partie de la pluie et le plus de mon petite au milieu de la pluie et les paysions de la planète et le pays de la plus de la plus de la planèter le plus de mon pays de la plus d\n",
      "\n",
      "sample T=0.5: je ne sais pas que le petite maintenant la pluie de la route et de la route, les béue plus les fruits de la chance de la plus comme de plus de la plus par mes concontres de la concontre, il faut le même des autons \n",
      "\n",
      "sample T=0.7: je ne sais pas chest qien voyage de sarmire appartiar plus tard plus de la ville à vivra le butellement et de chemin de la vable mais les derailases de cipp dans les projenses de la facile de pied de temps de la ré\n",
      "\n",
      "sample T=1: je ne sais pas mon répue qu' les hallages (idon bustralits qui se retrourobles. Ji gm tépitions alors de cocrd jour à vy09k/gès ! Utant aussi aidés pensais  mais j'ai éeurempre le rizont inprendaux possé postemain,\n",
      "\n",
      "\n",
      "epoch:  51   train_loss: 1.52   test_loss: 1.52\n",
      "epoch:  52   train_loss: 1.50   test_loss: 1.51\n",
      "epoch:  53   train_loss: 1.50   test_loss: 1.50\n",
      "epoch:  54   train_loss: 1.65   test_loss: 1.55\n",
      "epoch:  55   train_loss: 1.52   test_loss: 1.51\n",
      "epoch:  56   train_loss: 1.49   test_loss: 1.49\n",
      "epoch:  57   train_loss: 1.47   test_loss: 1.48\n",
      "epoch:  58   train_loss: 1.47   test_loss: 1.47\n",
      "epoch:  59   train_loss: 1.45   test_loss: 1.46\n",
      "epoch:  60   train_loss: 1.45   test_loss: 1.46\n",
      "\n",
      "sample T=0.2: je ne sais pas si je ne serait de la police de la pluie de la pluie de nourriture de connaissance de la pluie de la pluie de la pluie de la rue de nombreux parties de la maison de la rencontrer de la pluie de chaud\n",
      "\n",
      "sample T=0.5: je ne sais pas pars le visiter de vent sur le transpoindre l'ambre de la retrouver des petites après avoir pas de propose de la rencontrer des nombreuses et en France et les montagnes des chauffeurs de la pouvait d\n",
      "\n",
      "sample T=0.7: je ne sais pas de l'ousté de l'accésion, les rencontres de bus de l'entroit de y arme sur la pluie en convitation de confique dans la route et suffille très facile de la poste passer d'attenter des pains. Pour le s\n",
      "\n",
      "sample T=1: je ne sais pasc Aora commencer. Leqtinan un éouche od Wossible de compaction de dessus en maintenant sup la cus,ucs, et j'aur à 20 km. Dans qouille et d'embalr. I pessintter en Austracke, où un faire de surpo.ermé \n",
      "\n",
      "\n",
      "epoch:  61   train_loss: 1.44   test_loss: 1.45\n",
      "epoch:  62   train_loss: 1.45   test_loss: 1.45\n",
      "epoch:  63   train_loss: 1.43   test_loss: 1.44\n",
      "epoch:  64   train_loss: 1.42   test_loss: 1.44\n",
      "epoch:  65   train_loss: 1.41   test_loss: 1.43\n",
      "epoch:  66   train_loss: 1.40   test_loss: 1.42\n",
      "epoch:  67   train_loss: 1.40   test_loss: 1.42\n",
      "epoch:  68   train_loss: 1.42   test_loss: 1.42\n",
      "epoch:  69   train_loss: 1.39   test_loss: 1.42\n",
      "epoch:  70   train_loss: 1.41   test_loss: 1.41\n",
      "\n",
      "sample T=0.2: je ne sais pas le point de l'autre les car il y a des construires de la plus de la plus de l'autostop et de la construit de la pluie de la construit de la route avec le conseille de confinue du pays au milieu de la\n",
      "\n",
      "sample T=0.5: je ne sais pas les locaux le camping pour aucune plus rejoindre le gros profiter de trouver le paysage du travaille de l'ile de la même partici, par les conficiers. Je le petit prochains mais aussi à travers le plu\n",
      "\n",
      "sample T=0.7: je ne sais pas de l'Australie. Mn l \" à 7enouve l'ouhant les locaux moins éakisses belles chinois un vertie depuis tout si me serai un ros son apporter un ouvrir de rive d'aller de produits de voidu pour comportent\n",
      "\n",
      "sample T=1: je ne sais pas nores, après un piste pas de'ans presque, je pourrui  autour comme un pays, et hon espère, la rigière de la pussie et de même une et juste ou buale parle marêj les but d'objoire cu b'arryvité et hudu\n",
      "\n",
      "\n",
      "epoch:  71   train_loss: 1.39   test_loss: 1.41\n",
      "epoch:  72   train_loss: 1.39   test_loss: 1.41\n",
      "epoch:  73   train_loss: 1.39   test_loss: 1.40\n",
      "epoch:  74   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  75   train_loss: 1.37   test_loss: 1.39\n",
      "epoch:  76   train_loss: 1.36   test_loss: 1.39\n",
      "epoch:  77   train_loss: 1.36   test_loss: 1.38\n",
      "epoch:  78   train_loss: 1.35   test_loss: 1.38\n",
      "epoch:  79   train_loss: 1.34   test_loss: 1.38\n",
      "epoch:  80   train_loss: 1.34   test_loss: 1.38\n",
      "\n",
      "sample T=0.2: je ne sais pas de la plupart de l'autostop en Australie et de la pluie de la première pour de la première fois de la maison en stop en Australie et de nombreux arrivées de plus internet de la pluie de la constructi\n",
      "\n",
      "sample T=0.5: je ne sais pas la couchsurfing et l'occasion de la police est pour le porte que je ne sait sur le point de la marche de me poindre les amis de confirues de la vie et de ne ne plus de nombreux aubritures à l'avais p\n",
      "\n",
      "sample T=0.7: je ne sais pas demaine des locaux avant au milieu dans le monde de mes astrassent de l'autre voiture en Amormement sur le thé pour compliquer la monter dormir de plus loin découvrir le cherche de l'expérienne et qu\n",
      "\n",
      "sample T=1: je ne sais pas pryfreuts, encore le kieur. Poit le retEon aucun voir les fruits dans. La petite très bien soit plusieurs fois sur le P€maz) à l'aib et n'aime rencontré le Guvgla,, ici sont rien dien nous d'aller po\n",
      "\n",
      "\n",
      "epoch:  81   train_loss: 1.34   test_loss: 1.38\n",
      "epoch:  82   train_loss: 1.34   test_loss: 1.37\n",
      "epoch:  83   train_loss: 1.34   test_loss: 1.37\n",
      "epoch:  84   train_loss: 1.34   test_loss: 1.37\n",
      "epoch:  85   train_loss: 1.33   test_loss: 1.37\n",
      "epoch:  86   train_loss: 1.33   test_loss: 1.37\n",
      "epoch:  87   train_loss: 1.33   test_loss: 1.36\n",
      "epoch:  88   train_loss: 1.32   test_loss: 1.36\n",
      "epoch:  89   train_loss: 1.33   test_loss: 1.36\n",
      "epoch:  90   train_loss: 1.33   test_loss: 1.36\n",
      "\n",
      "sample T=0.2: je ne sais pasur de la plupart de la plupart de la plupart de la plupart de la plupart de la plupart de la plupart de la plupart de la route de la route de cette fois plus de la soirée de travailler et les plus ser\n",
      "\n",
      "sample T=0.5: je ne sais pasue sur le plus seul est donné quelques manques nous aller dans un beaucoup plus plus seulement son chaussure en France de la frontière de mes plus seulement une sacré à la nuit par le soleil sont seul\n",
      "\n",
      "sample T=0.7: je ne sais pasu  terres que nous aider me discuter les voitures en stop  une important de visiter le plus qu'elle pays et d'un différence est fait la froitionnel au centre est bien déjà ne problement  un peu de plu\n",
      "\n",
      "sample T=1: je ne sais pasument ensuite pure de dormir cette résion cepte-ronésienne de températions afin de lennement entre donc en plus que nous arors.oigens, est Aooudé que nous vous sommes en plein du turant. . . Imrelle à\n",
      "\n",
      "\n",
      "epoch:  91   train_loss: 1.32   test_loss: 1.36\n",
      "epoch:  92   train_loss: 1.32   test_loss: 1.36\n",
      "epoch:  93   train_loss: 1.31   test_loss: 1.35\n",
      "epoch:  94   train_loss: 1.30   test_loss: 1.35\n",
      "epoch:  95   train_loss: 1.30   test_loss: 1.35\n",
      "epoch:  96   train_loss: 1.30   test_loss: 1.35\n",
      "epoch:  97   train_loss: 1.29   test_loss: 1.34\n",
      "epoch:  98   train_loss: 1.30   test_loss: 1.34\n",
      "epoch:  99   train_loss: 1.29   test_loss: 1.34\n",
      "epoch: 100   train_loss: 1.29   test_loss: 1.34\n",
      "\n",
      "sample T=0.2: je ne sais pas très beaucoup plus tardi pour les autres pour les autres points de la construit de la construit de la vie de passeport et les autres portes de mes conseilles et de la pluie et de mon pays au milieu d\n",
      "\n",
      "sample T=0.5: je ne sais pas se fait pas de leur dien de la plus aussi de contentres pour les autres fruits par les plus soleil et de la plupart de la ville de temps de 5abitants passé de la ville de nombreux plats est déjà pren\n",
      "\n",
      "sample T=0.7: je ne sais pas en Afrine ville aux escennes pour les photours de la route, le coucheradites. Ma proppère d'une voiture, je ne voilà au rétemment malade le 1a n'en derniere possible, et chez les villes ou des paysag\n",
      "\n",
      "sample T=1: je ne sais pasodité entrette est mangèn, les etbes tandicité. Leure le même situes mang on peut être applaités à la jungle, avec de mat, Madonesan heureuse foydge, à l'un doit l'éustralie avec le babrialle de plus \n",
      "\n",
      "\n",
      "CPU times: user 13min 24s, sys: 5min 8s, total: 18min 32s\n",
      "Wall time: 18min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.Adam(model6.parameters(), 1e-2)\n",
    "train(model6, optimizer, criterion, train_data_wordpress, test_data, bptt6, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.61   test_loss: 1.57\n",
      "\n",
      "sample T=0.2: je ne sais pas de la construit de la prochaine de la construit de la première de la route et de la moins de la montagne et de la construit de la propose de la comprendre le parle de la partie de la plus de la proch\n",
      "\n",
      "sample T=0.5: je ne sais pas partire et très seulement sur le prochaine de place et de marche de l'autre aux conducteurs, et je suis des maisons de la remonde en avec les consons d'autres sement proche de la fois que je nouveau \n",
      "\n",
      "sample T=0.7: je ne sais pas en le démarché. Mais vie a découvre dans décidé d'aller très inconsons et au milieu du passer de nous revenon, je comprissant de route qui signe dans la mendre quelques france de Guerait, même si j'a\n",
      "\n",
      "sample T=1: je ne sais pas famillement sur la toute au pouvier sur pour ses plusienre de vale, toutique bien qui ne tout prendre 3400 ans au leur arrière de fuse du vons besolieussun en rirée, au pos petit, même de râgété peni\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.55   test_loss: 1.53\n",
      "epoch:   3   train_loss: 1.52   test_loss: 1.51\n",
      "epoch:   4   train_loss: 1.49   test_loss: 1.49\n",
      "epoch:   5   train_loss: 1.47   test_loss: 1.47\n",
      "epoch:   6   train_loss: 1.45   test_loss: 1.45\n",
      "epoch:   7   train_loss: 1.42   test_loss: 1.43\n",
      "epoch:   8   train_loss: 1.40   test_loss: 1.41\n",
      "epoch:   9   train_loss: 1.39   test_loss: 1.40\n",
      "epoch:  10   train_loss: 1.37   test_loss: 1.39\n",
      "\n",
      "sample T=0.2: je ne sais pas de marche de la pluie de la construire des dizaines de la prochaine de la ville de la route de la maison de la main et le profiter le proposer les prochaines et de l'argent de la pluie de la pluie de\n",
      "\n",
      "sample T=0.5: je ne sais pas pour partir et de se profiter de l'argent de la reprendre au campération de pouvoir le dépasse de la frontière de rencontrer à l'habité. A l'accordéon de la profiter à la maison de la montre le temps\n",
      "\n",
      "sample T=0.7: je ne sais pas pour la roue : australien de la rivière et se travailler en plus de l'argent par le coupageant au véhicule de la randonnée d'un nombreux elle voiture en bouleuse, je suis dizaine sou malade que le st\n",
      "\n",
      "sample T=1: je ne sais pas en autostop départ moins, se déception de bus, dans l'héberger aveill \" passé de transpinse de la ville sans menuirer. avec le nat, bese de savais (la plus de la mon célédentes. Non prix 120m et un a\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.35   test_loss: 1.37\n",
      "epoch:  12   train_loss: 1.34   test_loss: 1.36\n",
      "epoch:  13   train_loss: 1.32   test_loss: 1.35\n",
      "epoch:  14   train_loss: 1.31   test_loss: 1.34\n",
      "epoch:  15   train_loss: 1.29   test_loss: 1.33\n",
      "epoch:  16   train_loss: 1.28   test_loss: 1.32\n",
      "epoch:  17   train_loss: 1.27   test_loss: 1.31\n",
      "epoch:  18   train_loss: 1.26   test_loss: 1.31\n",
      "epoch:  19   train_loss: 1.25   test_loss: 1.30\n",
      "epoch:  20   train_loss: 1.24   test_loss: 1.29\n",
      "\n",
      "sample T=0.2: je ne sais pas de passer la plupart de l'arrivée de la construction de la construction de la main et de l'argent pour les problèmes de la ville de la plage et de se propriétaire de la forêt de la route et de l'arge\n",
      "\n",
      "sample T=0.5: je ne sais pas trop par le temps de bien les prochains sont pas de me fait par les amis de l'Argentine. Cette fois de plus le bord de la plage et de l'arrivée de la construit de la route à la fois que j'ai pas de p\n",
      "\n",
      "sample T=0.7: je ne sais pas continuer de nous avons apprendr'est avant de plats sont des pays ou de l'écritant donc le temps de ce moment de l'argent et d'une zone du tout que faire le sac de 10 feu de vraiment la presqu'on tro\n",
      "\n",
      "sample T=1: je ne sais pas communique pour étrante, quelques faune, avec un petit en liste de luxe niveau en verre (en cher et cepérès a afin dans un peu tous devienne, à naturel, c'est plus dormir d'un quartier pour m'ont mén\n",
      "\n",
      "\n",
      "CPU times: user 2min 44s, sys: 1min 2s, total: 3min 46s\n",
      "Wall time: 3min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.Adam(model6.parameters(), 1e-3)\n",
    "train(model6, optimizer, criterion, train_data_wordpress, test_data, bptt6, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.28   test_loss: 1.29\n",
      "\n",
      "sample T=0.2: je ne sais pas de problème de prendre le consulat de la plage de la construction de la capitale de la plage de la construction de la plage de la plage et de la plage et de nous avons passé la construction de la pla\n",
      "\n",
      "sample T=0.5: je ne sais pas un peu de découvrir la coco de la ville ou de la rivière et de l'aventure de l'arbre en stop de voyager dans la ferme, mais comme le bon marché de nombreux aborigènes. Comme des gens de bois par l'or\n",
      "\n",
      "sample T=0.7: je ne sais pas de travail dans leur bleu que le souhaite le possible. Le soir par les bouddhistes par un taxi, il a un des offres permanés de bus car il n'est papou par les Papous et de chemin sur la culture bout d\n",
      "\n",
      "sample T=1: je ne sais pas trop où 50 heures (a chercher de la Wuche. Nom en Chine.... Vous resterons du vend notre plampise et accueillir au contrèd de la chance simple, tau dans la déciternement et un deux sont bien parler u\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.23   test_loss: 1.29\n",
      "epoch:   3   train_loss: 1.23   test_loss: 1.28\n",
      "epoch:   4   train_loss: 1.22   test_loss: 1.28\n",
      "epoch:   5   train_loss: 1.22   test_loss: 1.28\n",
      "epoch:   6   train_loss: 1.21   test_loss: 1.27\n",
      "epoch:   7   train_loss: 1.21   test_loss: 1.27\n",
      "epoch:   8   train_loss: 1.20   test_loss: 1.27\n",
      "epoch:   9   train_loss: 1.20   test_loss: 1.26\n",
      "epoch:  10   train_loss: 1.19   test_loss: 1.26\n",
      "\n",
      "sample T=0.2: je ne sais pas encore comme le sens de la ville et de la route de la plage et de la plage et de la ville et de la plage et de la prochaine chambre de la plage et de la plage de la plage et de la capitale de la plag\n",
      "\n",
      "sample T=0.5: je ne sais pas si les années de la maison est donc prendre le plus proposer ces petits conducteurs de la piste de la construit de la mauvaise parle de visiter les restaurants tout ils ne sont pas en plus au profite\n",
      "\n",
      "sample T=0.7: je ne sais pas de nous avons déjà entre ne la malgré la station est facile à la région de Chine et comme 10 km dans un bain, des jeunes de rivières, nous avons et de nourrir le problème de pouvoir notre aventure. L\n",
      "\n",
      "sample T=1: je ne sais pas s'riviciens, il a du camp, et quelques missient les pives de travailles, de consommation n'est pas dénivalant dans l'est l'école, du supermarckent avant de téléphone manière. . On me prendre l'empire\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.19   test_loss: 1.26\n",
      "epoch:  12   train_loss: 1.18   test_loss: 1.25\n",
      "epoch:  13   train_loss: 1.18   test_loss: 1.25\n",
      "epoch:  14   train_loss: 1.18   test_loss: 1.25\n",
      "epoch:  15   train_loss: 1.17   test_loss: 1.25\n",
      "epoch:  16   train_loss: 1.17   test_loss: 1.24\n",
      "epoch:  17   train_loss: 1.16   test_loss: 1.24\n",
      "epoch:  18   train_loss: 1.16   test_loss: 1.24\n",
      "epoch:  19   train_loss: 1.15   test_loss: 1.23\n",
      "epoch:  20   train_loss: 1.15   test_loss: 1.24\n",
      "\n",
      "sample T=0.2: je ne sais pas de problème de problème de repos et de la ville et de la construction de la moins cher et de la plage de la prochaine destination de la pluie et de la pluie et de nous avoir partir de la ville et de \n",
      "\n",
      "sample T=0.5: je ne sais pas à la même canalité. Nous avons décidé de rencontrer les trajets de la maison mais seulement de voyager en plus de plus de 30 dollars dans les installer les heures pour faire du stop pour me demander \n",
      "\n",
      "sample T=0.7: je ne sais pas envie de moment le peu sans très douce contre une petite petite ville et à la cuisine, que l'entrée du projet accompagné des champs de semblent plusieurs fois vous apprendra que les pickar (morceente\n",
      "\n",
      "sample T=1: je ne sais pas le terre avions mes membres pour attaquer acheter, des gens que la ville, dans les problèmes. En effectue et dans les pommes de bons rêves ne sont appédition et les bons formes françaises médiatisabl\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 1.14   test_loss: 1.23\n",
      "epoch:  22   train_loss: 1.14   test_loss: 1.23\n",
      "epoch:  23   train_loss: 1.13   test_loss: 1.22\n",
      "epoch:  24   train_loss: 1.13   test_loss: 1.22\n",
      "epoch:  25   train_loss: 1.13   test_loss: 1.22\n",
      "epoch:  26   train_loss: 1.12   test_loss: 1.22\n",
      "epoch:  27   train_loss: 1.12   test_loss: 1.22\n",
      "epoch:  28   train_loss: 1.11   test_loss: 1.22\n",
      "epoch:  29   train_loss: 1.11   test_loss: 1.21\n",
      "epoch:  30   train_loss: 1.11   test_loss: 1.21\n",
      "\n",
      "sample T=0.2: je ne sais pas le continent de la plage et de nombreux passagers et de police et de la culture et de se retrouver du stop et de nombreux mois de plus de plus en plus de plus pour les plus importants de la route et \n",
      "\n",
      "sample T=0.5: je ne sais pas d'apporter les petits plats de bananes et de police et de poussière conservée au milieu de la chance de la plage et de plus de prendre le matin et de nombreux magnifiques de la plage et des animaux a\n",
      "\n",
      "sample T=0.7: je ne sais pas de nouveau parlent pour le bus à quelques cales, mais fut la dernière expérience en autostop en Australie, et même en Adéréner et avoir le plaisir de sa passer la colonée et devoir dormir dehors en c\n",
      "\n",
      "sample T=1: je ne sais passé trouver un bon grand et qu'en a la seconde car il est passer par un équivalent il faut leur feutôt les Poignées de la culture surtout le minez quelques années plus de professeurs passeurs et de bié\n",
      "\n",
      "\n",
      "CPU times: user 4min 4s, sys: 1min 33s, total: 5min 37s\n",
      "Wall time: 5min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.Adam(model6.parameters(), 1e-3)\n",
    "train(model6, optimizer, criterion, train_data_wordpress, test_data, bptt6, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune on blogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_blogger = get_data(train_blogger, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.88   test_loss: 3.06\n",
      "\n",
      "sample T=0.2: je ne sais pas porrong poonr porrong poonr poonrong pro poonr porrong pr porrour pro poins pro poonrong poong porrong porrong poonronporrone porront pro porrong pro proro porrong poonronp poonronport pro porrour po\n",
      "\n",
      "sample T=0.5: je ne sais pas poont poonanp prorr re poonanit parpis pr pe po poins pivrosons porrond pirconors pornh poonr forr vora tir vi prrt poonrong s poonh poone poonps porrr poins poonrs pr pr re pr rorh voit porr vro por\n",
      "\n",
      "sample T=0.7: je ne sais pasndro psir pronoorrarh poong porrond parsarg poir peuna porros pe ti  ps r r ro pour norraronr toyr pre porpoongs arvs pive porr s nir pouve pr ro pr vorr p r ne p c t proons pr toragast tort je gcore \n",
      "\n",
      "sample T=1: je ne sais pas. pore v  orin p fr forno a s g vrsmémfdr  s corsomt p porne rsé fo unimonrsrd vorang perh ni fia prema nr corqgip gvonasenpurrirnes îcronipr pr pr onr ciéjo so poir qu pr vrofurezriporhont vûant  pe \n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.84   test_loss: 2.63\n",
      "epoch:   3   train_loss: 2.57   test_loss: 2.44\n",
      "epoch:   4   train_loss: 2.34   test_loss: 2.30\n",
      "epoch:   5   train_loss: 2.21   test_loss: 2.19\n",
      "epoch:   6   train_loss: 2.10   test_loss: 2.12\n",
      "epoch:   7   train_loss: 2.02   test_loss: 2.05\n",
      "epoch:   8   train_loss: 1.94   test_loss: 2.00\n",
      "epoch:   9   train_loss: 1.87   test_loss: 1.95\n",
      "epoch:  10   train_loss: 1.81   test_loss: 1.90\n",
      "\n",
      "sample T=0.2: je ne sais pas de la render de tout de le trouver de tracis de pas de mon en de contre de le partie de trouve de la pas de pas de le pour le trouve de le pour le pas de sais de contre de le pas de la pas de pas pas\n",
      "\n",
      "sample T=0.5: je ne sais pas pas de si d'aller de le traverse de l'est de les villes se la prande de pour n'ai crais, et de cettier pas se le toute de kcoure de mon de remaire pour en parce pas de mort de la fina de sons de la t\n",
      "\n",
      "sample T=0.7: je ne sais pas en ce fait à non cectures de toute mins de se les actit, et que plus costipos sur mortiper un fait. C'est de mu de prophone. Le dires à la constre audé le transatient qui la roinon entre toute de pou\n",
      "\n",
      "sample T=1: je ne sais pasmené permoins a tracié éitanteils kx, et ubdé pas peuvé en) Hua dans cause un guécamoolopos nivait sorte. Ce tu de quentionneai (pour nos la reniens. En si hesu galuer deh les se sérous les reste de p\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.76   test_loss: 1.86\n",
      "epoch:  12   train_loss: 1.71   test_loss: 1.83\n",
      "epoch:  13   train_loss: 1.67   test_loss: 1.80\n",
      "epoch:  14   train_loss: 1.63   test_loss: 1.77\n",
      "epoch:  15   train_loss: 1.59   test_loss: 1.75\n",
      "epoch:  16   train_loss: 1.56   test_loss: 1.75\n",
      "epoch:  17   train_loss: 1.54   test_loss: 1.73\n",
      "epoch:  18   train_loss: 1.51   test_loss: 1.72\n",
      "epoch:  19   train_loss: 1.48   test_loss: 1.70\n",
      "epoch:  20   train_loss: 1.46   test_loss: 1.68\n",
      "\n",
      "sample T=0.2: je ne sais pas de la plus par le propose en plus pas de la place de la route de la route de part je suis de la route de se plus de la plus de l'autostop de la plus pas de la propose de l'autostop de la route pour l\n",
      "\n",
      "sample T=0.5: je ne sais pas de la maison de passer dans le bus d'argent pas pas de la fait de plus de leur pour qu'ils les prendres ses conducteurs qui trop de dépendre de plate des fait que les montagnes expromes, et le prendr\n",
      "\n",
      "sample T=0.7: je ne sais pas la font, et que je dormille. C'est de plus de campages (pas à Petit, des montagles qui heures avec une fance Aux sur les plus plus grentes, les projets de base. Je vais feste : ces boutes, la tourist\n",
      "\n",
      "sample T=1: je ne sais pas l'impas, tous sance. Ciens pays, ontalemek dire intente qu'il espendant dans sa Prêter ses pétatus quani au Nolkbes de jours de me l'un posans officiaus fait quette du ceux, en Fratuat\"pas de part là\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 1.44   test_loss: 1.67\n",
      "epoch:  22   train_loss: 1.41   test_loss: 1.66\n",
      "epoch:  23   train_loss: 1.39   test_loss: 1.65\n",
      "epoch:  24   train_loss: 1.38   test_loss: 1.66\n",
      "epoch:  25   train_loss: 1.36   test_loss: 1.64\n",
      "epoch:  26   train_loss: 1.34   test_loss: 1.63\n",
      "epoch:  27   train_loss: 1.32   test_loss: 1.63\n",
      "epoch:  28   train_loss: 1.31   test_loss: 1.62\n",
      "epoch:  29   train_loss: 1.30   test_loss: 1.63\n",
      "epoch:  30   train_loss: 1.31   test_loss: 1.65\n",
      "\n",
      "sample T=0.2: je ne sais pas de la frontière de la route de la plus de la moition de la frontière de la pluie de la pluie de la ville de la frontière de la route de la pluie de la plus de la route de la pluie de la consontion de\n",
      "\n",
      "sample T=0.5: je ne sais pas pendant pas de l'est pas de temps d'une partie les choses nous atteirs d'accordent de la chance d'accroche partiille et en village de la suite de la ville au moins de chance, ce niveau de la place de\n",
      "\n",
      "sample T=0.7: je ne sais passent la tente. Je lui fais qu'une policioleur colombrer depuis les bases : Je peux qu'on le consertout à l'Est de ce continuer les bouples à l'inviter la route la tentes (endroits que je me suis des c\n",
      "\n",
      "sample T=1: je ne sais passé il froide, ou de forleures de paficiers de coinaliments quaide son prix parlis. Jourd, je rechaleur. Mes villages, et on elle je ne les séjourslasser lires gens 3 et le fait que ça me vite peutemen\n",
      "\n",
      "\n",
      "epoch:  31   train_loss: 1.30   test_loss: 1.63\n",
      "epoch:  32   train_loss: 1.27   test_loss: 1.63\n",
      "epoch:  33   train_loss: 1.26   test_loss: 1.64\n",
      "epoch:  34   train_loss: 1.25   test_loss: 1.61\n",
      "epoch:  35   train_loss: 1.24   test_loss: 1.62\n",
      "epoch:  36   train_loss: 1.22   test_loss: 1.61\n",
      "epoch:  37   train_loss: 1.21   test_loss: 1.60\n",
      "epoch:  38   train_loss: 1.20   test_loss: 1.61\n",
      "epoch:  39   train_loss: 1.19   test_loss: 1.61\n",
      "epoch:  40   train_loss: 1.18   test_loss: 1.61\n",
      "\n",
      "sample T=0.2: je ne sais pas de me suis pas de la construction de la route de me dit que je n'ai pas de me dit que le chemin de la chance de la sortie de l'autostop de l'autostop de la route de la carte avec le cas de la carte d\n",
      "\n",
      "sample T=0.5: je ne sais pas une nous en camion de m'en a des destinations particuliens d'instants à l'autre de l'occasion et le long du milieu de la petiteur de la goin de viange. Ils en coutume de l'entrée de l'Argentine à la \n",
      "\n",
      "sample T=0.7: je ne sais passer l'avant de passer avec des matinales. Cette fois-ci, où je vais en trouve un chiliens. \"Je déconduction de la route de vraipas d'argent avec les reste de croqué une heure et du capitaine de l'Aile\n",
      "\n",
      "sample T=1: je ne sais pas de dormir mon sac de 700 à marcher. Franité a> Sidistat vite que je me checturent à temps dans Pristiquais qu'il n'y a besoin de touristes. Loici. - on pémétée de cettes les maintene sur la second à \n",
      "\n",
      "\n",
      "epoch:  41   train_loss: 1.17   test_loss: 1.62\n",
      "epoch:  42   train_loss: 1.17   test_loss: 1.62\n",
      "epoch:  43   train_loss: 1.16   test_loss: 1.62\n",
      "epoch:  44   train_loss: 1.16   test_loss: 1.63\n",
      "epoch:  45   train_loss: 1.16   test_loss: 1.62\n",
      "epoch:  46   train_loss: 1.14   test_loss: 1.62\n",
      "epoch:  47   train_loss: 1.12   test_loss: 1.61\n",
      "epoch:  48   train_loss: 1.11   test_loss: 1.61\n",
      "epoch:  49   train_loss: 1.09   test_loss: 1.61\n",
      "epoch:  50   train_loss: 1.09   test_loss: 1.62\n",
      "\n",
      "sample T=0.2: je ne sais pas de conserve de contraste de marche, et le conducteur de trouver un peu de marche pour les villages de montagnes de conserve de passer pour les villages et des pipes de trafic des camions que je n'ai \n",
      "\n",
      "sample T=0.5: je ne sais pas en train de terre et des cacahuètes de passer les castages et m'a seulement les prochaines de chance de passer par un camion de se développes de trouver un lift de plus de cette fois. Ils avant de me\n",
      "\n",
      "sample T=0.7: je ne sais pas de plus paye se terre dans la plage, et par exemple grop. Il y a pierre, et peut, le chef de vacus, et je lue me langer et de 23 kilomètres les dernières vides idées dans ses montagnes de passage de \n",
      "\n",
      "sample T=1: je ne sais pas un autre navogue bassine et demie de vers 1,5f. Je boudre.\" Tental avec prévue, j'arrive (d'accord, trop de kilomètres, et vite perdure, j'ai décemment avec leur péage, et je savais, c'est pas pour u\n",
      "\n",
      "\n",
      "CPU times: user 2min 5s, sys: 42.6 s, total: 2min 48s\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.Adam(model6.parameters(), 1e-2)\n",
    "train(model6, optimizer, criterion, train_data_blogger, test_data, bptt6, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.08   test_loss: 1.61\n",
      "\n",
      "sample T=0.2: je ne sais pas de contraste en prendre le cas de confort des articles de cette fois de l'autostop de l'autostop de l'autostop de la carte avec le trafic des conducteurs sur le chemin de prendre le cas de contrôle d\n",
      "\n",
      "sample T=0.5: je ne sais pas de la ville en demande monte sur une réterdis environ de la coca, et je suis au moins ou souvent comme des trucs et des camions sur le prix de ce que je ne suis pas au reste de l'autostop sur un vill\n",
      "\n",
      "sample T=0.7: je ne sais pas partie de cette heure aller et le couple d'un moment de force de la première que des parties actuelles et le fait qu'il de lesdances sont pas en train de re montagneux valiser le lendemain de transpe\n",
      "\n",
      "sample T=1: je ne sais pas mieux que j'ai le piober, j'ai passage pas par un pervert de vraisont avec 1.400 mètres de marche, et installe la peine mouurde \"vendent le savon-bourrément d'aire devant ce fois. Et une nous encore \n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.06   test_loss: 1.60\n",
      "epoch:   3   train_loss: 1.05   test_loss: 1.60\n",
      "epoch:   4   train_loss: 1.05   test_loss: 1.60\n",
      "epoch:   5   train_loss: 1.04   test_loss: 1.60\n",
      "epoch:   6   train_loss: 1.04   test_loss: 1.60\n",
      "epoch:   7   train_loss: 1.04   test_loss: 1.60\n",
      "epoch:   8   train_loss: 1.03   test_loss: 1.60\n",
      "epoch:   9   train_loss: 1.03   test_loss: 1.61\n",
      "epoch:  10   train_loss: 1.03   test_loss: 1.61\n",
      "\n",
      "sample T=0.2: je ne sais pas de l'autostop de la coca de la route de travail de ce que je ne suis pas sur le chemin de traverser la profondeur de la route principale, et je me suis pas de la ville de la route pour le conducteur \n",
      "\n",
      "sample T=0.5: je ne sais pas entre le camping de plats sur les montagnes du Chili, car je me fais chercher le themière de la frontière. Il y a des restes. Je leur demande si il est la capitale de l'autostop d'un camion de toutes\n",
      "\n",
      "sample T=0.7: je ne sais pas dors un coup, et d'anglais, au porte à changer des champs importances dans leur parc veulent à la sachue) moins de 10 centimes de chez moi. En plus de me semble seconde pour les incromé. Le conducteu\n",
      "\n",
      "sample T=1: je ne sais pas de première monte de Sart. Satis, c'est pour la limite une argentin, pas d'argent (En a bien entendant le plus chère avec un pays par Mersi à jour ...  Pamez avant de thy, me grapombarle uns très le \n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.03   test_loss: 1.61\n",
      "epoch:  12   train_loss: 1.02   test_loss: 1.61\n",
      "epoch:  13   train_loss: 1.02   test_loss: 1.61\n",
      "epoch:  14   train_loss: 1.02   test_loss: 1.61\n",
      "epoch:  15   train_loss: 1.02   test_loss: 1.61\n",
      "epoch:  16   train_loss: 1.01   test_loss: 1.61\n",
      "epoch:  17   train_loss: 1.01   test_loss: 1.62\n",
      "epoch:  18   train_loss: 1.01   test_loss: 1.62\n",
      "epoch:  19   train_loss: 1.01   test_loss: 1.62\n",
      "epoch:  20   train_loss: 1.00   test_loss: 1.62\n",
      "\n",
      "sample T=0.2: je ne sais pas de la plupart des camions que je ne suis pas rare de la route est la destination de mon pays est construite de la route et de la coca, et le camping sauvage, et je me suis rencontré de l'autostop de \n",
      "\n",
      "sample T=0.5: je ne sais pas plus restaurants de rue, et que je n'ai pas le calier pour ma vaguer la fenêtre, mais c'est de faire du monde au bord de la ville de la cordillère des compagnies sont déjà que le trafic de l'argent. \n",
      "\n",
      "sample T=0.7: je ne sais pas d'apprécie un morceau de respect pour payer la route pour les conducteurs en pantalon pour le backponsant d'une mois et font payer à la ville de la rivière. Du coup de risque se faire qu'ils de discu\n",
      "\n",
      "sample T=1: je ne sais pas Aller blanche au Nord-annéelligation, autre manimatiquer la dec. Je prends pas que l'eilmié en échange\", bien grand y a l'époque, malgré lectautiliques qui arrive en Om volontaires ça n'est clairemen\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 1.00   test_loss: 1.62\n",
      "epoch:  22   train_loss: 1.00   test_loss: 1.62\n",
      "epoch:  23   train_loss: 1.00   test_loss: 1.63\n",
      "epoch:  24   train_loss: 0.99   test_loss: 1.63\n",
      "epoch:  25   train_loss: 0.99   test_loss: 1.63\n",
      "epoch:  26   train_loss: 0.99   test_loss: 1.63\n",
      "epoch:  27   train_loss: 0.98   test_loss: 1.63\n",
      "epoch:  28   train_loss: 0.98   test_loss: 1.64\n",
      "epoch:  29   train_loss: 0.98   test_loss: 1.64\n",
      "epoch:  30   train_loss: 0.98   test_loss: 1.64\n",
      "\n",
      "sample T=0.2: je ne sais pas de connard et le plus froid de la cocaïne, et le contraste au milieu de la coca, pour aller demander au milieu de la ville de la route et de la pluie de la cocaïne, et le plus partie de l'autostop de\n",
      "\n",
      "sample T=0.5: je ne sais pas de vivent sur la route pour l'exception de la mauvaise de grands avant de mon sac. Le lendemain, comme de l'autostop sur le dernier que le terminal de faire une fois en deux guides constituent des pa\n",
      "\n",
      "sample T=0.7: je ne sais pas très bien sur la main de la descente va passé la route. Son finale. Voilà de la frontière, ou parce que la ville de l'autostop en chemin : la fin d'avoir un pêcheur le poids d'années qui n'en a dégue\n",
      "\n",
      "sample T=1: je ne sais pas dans le réside par un rûstrasque sur kilomé. Elle est par là. L'objectiment direction face, en compte la plage. Prose de ma manière même le garde trouver des seules construitlits envirantes aller deu\n",
      "\n",
      "\n",
      "CPU times: user 1min 17s, sys: 25.1 s, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = torch.optim.Adam(model6.parameters(), 1e-3)\n",
    "train(model6, optimizer, criterion, train_data_blogger, test_data, bptt6, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample T=0.2:\n",
      "\n",
      "je ne sais pas de connard on a mis de communication de contrôle de la cocaïne, et le conducteur nous disent que le minerai de l'autostop en train de trouver un peu de trafic des camions sur les pays de la pluie de l'autostop en plus de la carte de la route est constitué de trouver un compagnie de la coca, pour le prix de chance, et je ne suis pas de confort en plus de la prochaine destination de mon sac de couchage de mon pays est le pays de la route est la capitale de la route est le mariguer un lift de la cocaïne, et je me suis pas de l'autostop de la pluie de l'autostop de la route est le chemin de contrôle de la coca, ou le couple de la route est le paysage de la suite de la pluie de la ville de la pluie de l'autostop de la route est la capitale de la route route pour la première fois qu'on a pas de travail de la route en plus de la chance de la ville de la ville de la route nous profite de la pluie de la cocaïne, et le camping sauvage, et je me fais construite de la ville de la coca, c'est le chemin, mais je ne sais pas de connard on avait pas par conséquent le pays de la coca de la chance de l'autostop de la coca, c'est le camping service de trouver un couple de mon sac de couchage de ce stop pas de mon sac de couchage de contrôle de la route et de la route nous dit qu'on ne sait de marcher pour le train de me déposent en compagnie de la coca, c'est le cas de connard on a décidé de me rendre au contraire de la route et de la route n°2 mois de marche par le conducteur de marche qui se passe en plus de la coca de la ville de la profondeur de la route et de la ville de la route est le camping sont les cours de l'autostop de la coca, pour nous prend la maison de planter la tente de la route principale de la ville de mon sac à l'air de la route est plus loin, et je peux dormir dans le conducteur avec le coup de l'autoroute de la route n°3 kilomètres de marche par les coups d'un coin de planter la tente en stop de la frontière au milieu de la coca de la route nous entrée de la coca, c'est le camping service de l'autoroute au milieu de la coca de la ville de la cocaïne, et le contrairement de la route est le trafic des pays de la pluie de la place principale de la route de l'autostop avec le conducteur de marche qui m'a plus de la coca, c'est le cas de couchage de contrôle de la montagne n°3. Je leur décide de la maison de plus de ce qu'on ne passe de l'argent pour le courage de mon argent, les transports en stop sur le chemin de campagnes de plats déjà pas de connard on a prendre le cas de confort au camping seul part de la route et de la montagne n'est pas de trafic est constitué de la cocaïne, et je me suis fait pas de la ville de la carte est la capitale de la pluie de l'autostop de l'autostop de la coca, c'est le chemin de transport de la route principale de la route est le pays de la coca, pour trouver un lift de la coca de la route, c'est le chemin de camping de la ville d'un camion de la route et de la route est le pays de la route est le monde au bord de la montagne n°1. a semble tranquille, parce que je ne suis pas par les coups de cette fois où on n'a pas une autre côté de la cocaïne, et le conducteur sur le chemin de campagnes que je ne suis pas de connard on a profite de la coca, et je me suis pas de connard on a peut pas le courage de trouver un couple de ma carte de la route et de la cocaïne, et le prochain contraire de la ville de la route nous emmener au milieu de la ville de la route de la route nous emmener à l'autoroute de la route et de la route et de la coca, c'est la destination de la ville de la route principale de la route est la carte de la route pour le prix du coup, on me rappelle que je me rends de marche pas de courage de montagnes de transport à l'aide de la montagne n°3 sont passés de la ville de la place principale de la route et de la route est le paysage de marche par les cours de la pluie de l'autostop de la nuit à l'auberge de la main de mon sac de couchage de part et de la route principale. C'est un peu de travail de la prochaine destination de la première fois que je reprends la route principale de la route est plus tard, je me rends au bord de la ville de la route pour le conducteur nous a un coup de ce stop pas de connard on avait pas par le chemin de transport de me faire de la montagne dans le conducteur de marche de marche à l'arrière de la route est la destination de la forêt de la suite de la route principale de la coca de la route pour le soleil de manière de la route nous aura pas de connard on a profite de la route est plus de la coca disparaître des camions de connard on a pas de la pluie de l'autostop de la route est le soleil de l'autre côté de la pluie de l'autostop de la cocaïne, et le prochain de la ville de la route est la capitale de la coca, et je continue dans le conducteur de marche par le prix de campagne de la route principale. Je ne suis pas de la première fois que je ne suis pas trop le chemin de voyage de connard on a l'air de la ville de la montagne est la \n",
      "\n",
      "\n",
      "sample T=0.5:\n",
      "\n",
      "je ne sais pas en autostop et me remercie pas de moi, il ne sont pas vraiment de l'argent ce que j'ai pouvoir tariper la route principale. Le conducteur de marche avec des pays simalatels en plus de poulet avec une pratique de la musique que la prochaine nationale. Je me retrouve en reste plus loin, et je ne sais pas pas pour le savoir d'expériences d'altitude de français, mais c'est à des champs et encore plus loin. J'ai passé le cas en contraire, un sac à l'aide de mon conducteur de marche en stop, j'ai pris la maison et de l'autoroute avec des petits auberges sont pas de conducteurs qui ne sont pas le parc, je lui dit qu'on se rend de fruits sur les ruines de confort à l'école plus loin. Le bateau sur la route façon. De campagne. J'achète une petite habituellement consident à la capitale de la destination du pays avec les coups de cet article par les collent et s'arrêtent à l'oreille de l'Argentine que celle de l'entrée de la tente par les bouteilles qui a décidé de rester le premier limiter des camions de vue le cas, je peux pas de musique des cartoud dans la rue de la montagne est un peu de la ville, le temps de marche de marche. En construction de l'autoroute : je prends un sympathie par là, il y a Au cours d'un collectivo pour le camping seulement la suite de l'autoroute et de la route visiter les bouteilles avec le temps d'un des pays distrières moins de 20 minutes, les chiens sauvages, qui s'arrêtent de ce stop pas de trafic de couchage, vous fonctionne dans la rue de l'argent, les pays sans avant de mon sac ? Alors je suis déjà détour par les différentes, des gens les montagnes de coinsiquement dangereux, et je reste un bus qui contraire, et puis qu'il fait plus de la rivière, et je n'ai pas une chambre de mes piques qui se passer quelques barres de la frontière pour aidé d'autoroute de l'autostop de Salento des cartes de terre du voyage. Le contraste avec les moins des choses de guides collectes. Ce soir planter la partie du milieu de la route ville tout autant le conducteur de la maison. On se retrouve un peu de venir ambiance au Chili, et mon point de traverser la route dont je ne n'ai pas le cas présent. En fait de l'évaluaumée de la coca, où les ruines de boucles pas encore plus loin, et que je n'ai pas de trafic de couchage de contrôle de la côte de ce que ça va de mon sac à ce façon, aussi le problème parce que je peux pas désormais de la ville du tout, je peux du résume pas plus fréquentée de l'autostop de doute sur la place plus loin. J'ai toujours avec un peu trop de marche pour le train de retourner à la ville de dormir deux fois plus tard, je prends le ranger son camion de concept en compagnie de bonne deux jours de travail me propose de ce moment, la route la partie privée de la coca de 200 de contrôle de la carrera pas de confort aux roches du lac, j'ai passé sur la tente est un coin d'intention de faire des camions qu'en Colombie au large du milieu de la route nous ont en profite pour accepter des vendeurs de première fois. Je vais voir le trek d'autostop sur la suite sortir de la cocaïne, et des plus de castiques proches de choses de conserve à l'Argentine. C'est le trajet du minerai et les petits entre les deux conditions qui me laisser en locale pour partir de la survilier et le volume principale. Je n'ai pas croisé le concept de la ville de la double (ils sont pas le prix à un peu pour le prix de toute façon ? Le poisson, et il faut parcourir le monde est le trafic des plus expériences de touristes à la nuit à chaque ferme de terre et de l'autoroute, du pays au conducteur nous demande pour moi. En fait. Je me fais pas en terre par conséquent par le l'apprendre le plus franchement de l'argent de mon sac à la main, j'aurais haut de la sortie de la route avec des prochaines de toute façon. En camion de backpacker de la débarquer les articles de touristes de touristes de constructions que dans la rue de l'argent pour un spot à me faire atteint de la station avec des cartes articles à cette raison de la construction de la ville d'un autre matinale. J'ai déjà plus d'aller demander en deux présents et des camions que je n'ai pas le chemin nombre de minutes de groupes pour toutes longues de facture, c'est la ville de l'autostop comment en avant ce que j'ai passer le condition de la forêt de la ville avec le grille par le premier et de minutes de changer de la route en échange de se variables du camion de mon sac de couchage prochaine de la ville, parce qu'ils ne sont pas de dormir deux ans, d'autres pays les coins de cets avants, mais on va passer la montagne rempli de dormir deux ans dans la misère, la capitale de la cocaïne, avec les cours, seulement 30 kilomètres de marche à pieds des camions que je n'ai pas d'argent de la route et de la route n°4 minutes de marche. Je vais déjà des camionneurs durestes, les coups d'espagnol, et je vais même le bus son problème pour moi, et je commence à dormir dans la montagne et sans doute pour moi, je les courbes pour le là que le paysage de contourner de la vi\n",
      "\n",
      "\n",
      "sample T=0.7:\n",
      "\n",
      "je ne sais pas la rivière qui m'a passé la route qu'il faut un peu plus de ses choses, car ça en plein pouvoir première tropicale au chargement pour l'autostop sur le mal de chiens sauvages, il est tout le voyage le place à l'interies. De toute façon de me déposent, j'en saisi. C'est bien qu'ils est parti de la frontière, hutte popier avec un couple qui me fait nous avant de parpe de la route sur le trajet tout s'arrête : ce qui vont car ce qui me relative aux autres sympas de trajet de la journée sur le tout de ma carte sans langue. Je recherche tout le sélication de les livres de marche avec les premières générales qui tournes d'expérience en altitude, il y a encore un coup, et c'est le camping est rencontré. C'est le lendemain. On sait à l'échelle pour la recroiser plus de l'aventure, parce que j'ai des payer de temps essence, à Vallegrande avec moi que je vais pour autostope s'est conseille dernière jour au champ, vous appelle avec le gardes compagnie comparative apéaisant de l'oreille de la nuit tombe. Je passe un peu plus tard dans la forêt importe. Je ne me rappelle le problème. J'ai l'intention de la frontière et débarquer en partie de la distance d'adresse de jardin, car j'ai délinée qui a l'air commence à faire lille. J'ai passer, c'est là, il en a du centre décentre au milieu de la route entre les seuls endroits du parc et il y en a pas de plastique et la retour à pieds. Le lendemain, à me faire sent antivalement incapable de l'opérativement avec plaisir de sa vie à un incruste, à nature de ses stations de toute façon, la déhiculaire et moyen, si que mon sac à l'arrière, on est réputé en Pérou pour encore plus bien que mon colombien d'autres exploits au milieu d'un des trucs qui fait l'impressionnos. Au grerrouvern.  la plage, en de la ville de cette contrôle des maisons plus ou plus tard, je redescendre dans la plupart d'en \"memmer \"pour le correspond le plus mal que je vais vas suite l'assure de comminutes de marche, on peut plus d'attente est plus tard. La foîte, mais depuis une cité mon fraîchement contre un coup de marche au moins sur le compte de minutes de connaissance sur la même station les populations partagent pris à l'arrière du monde, mais je me faur, et il est intéresse qu'elle autour de marche. D'ailleurs, et je vais voir la nuit toujours intéressant. En fait, je fais pas chogo, et bien cher et un aimais décidé de parori le découvert de la fouille mais pas t'es, et le croisement de la coca, il paraît que le cas de couchage rendonner 25 kilomètres de la végétation de mon projet. La ville aux sechoir présente de passer le départ de la suite de l'ambiance de la place à Trinidad, jamais déjà de San Serès maintenant, c'est que vers la tomber un garde de terre, et surtout par l'un de temps, je décolle de se déplace la ligne se régime au dort à mes poussants péruviens qui me rabatteurs celles qui m'installent à pieds la même camion se ramène qu'ils conseilles dont je dors au large d'une nuit de sortir de la ville encore plus un petit déjà pas l'affaire, et je vais aient atteindre le cas suite, je décide de la prochaine destination de parc le trafic, en ciné des fringues de ma route, bordée de l'ancien de m'emmener de la peine de contré de la confort durer de la pluie avec le sortir du trajet de la trimballe à notre autre part au champs. Pendant que le budget à l'arrière de l'argent pour le bus en avant l'air d'a disposot. Son canapé et source le matelas un argentin de toute façon, c'est le stop en échange, les coups, la funié, je découvre que le camping, ça sera boue voit au bord de la cocaïne, mais c'est 5 mois de se passé sur la côte de l'eau du montable d'un bus pour le prix de la catégorie de l'argent très cormunement, j'aurais pas de quoi se dirige mon sac peu directement en terre sur le coin avec dans la montagne de traînent avec les problèmes de mon systément de m'embarquer. Ensuite la même rendu qu'on est parti de 3 jours de la montagne foireux de l'apprendra que j'en profite pour des premières fonctions en train de me laisser une foîte sont arrêtée de la coca particulier à 23 heures de marche\". Je n'ai cuisiné que je fais du Santiago. L'espagnol, je me laisse longtemps qui m'a rentramber de me mettre plus importante de la droot, il m'a passe de la montagne et les yeux. Je suis apprégion. On n'a pas une autre en plus de Met dans la nuit de regard êtres de train de désert de marche, et vu que j'ai eu l'office de marche par des gens 5 heures de marche, les compagnons commencent l'une de cet article qui ne sont étrange, mais je ne me dis on a désormais sur l'autoroute sur la rocade est aussi sur la route n°6 mois de coûte constitue pour une charge se couche d'en formétique avant la carte semi-une. Alors que vu que ce n'est pas le courage à la rivière qui coûte matinale, et que des gens qui vont tous pourraient en communautés au nom de 20 km. Il y a fait la maison de compréhension des trous et de trouver la delà ce que je lui suis pas sans dobil de trou tropiques, et déjà Guayaramern. Tout de même \n",
      "\n",
      "\n",
      "sample T=1:\n",
      "\n",
      "je ne sais pas dans la mine de rue. Alors on -15°C pas c'est environ-essenir fondage) sont préventient sur moujours dus, quand ils connard, je n'ai tendu on a ceux, je n'y à la sortie de positipé. Les hullés en conducteur. \"La route vit à 30 mètres, non national de temps à la carte est vasse) en composent des ouvrir inconnaisque que darme un couple campagne d'une qui, mais je déteston d'environ en mercre dans l'intérieurage phatrique pas, puisque que ça risque bled part dans la suite. Là\", \"c'est pas le sévogo, la capitre atteindre. Jussors aire qui auraient abordé pour un autre séparamé, bus touristique du Ny. Un spot, ça véaine d'autressiérates par relatives soient qui lumière partout. On une ajouter 3 grimper mon lifts compalawi. La première flord. On desquatteux qui heures, et pas que le frirtition est dije dormir sur la carte), c'est tension : 100 km, puis mangent. Rapine que leur parc une visite et moins honsties. Les visiter de Bordeaux : Mais un endroit cression : l'apparté à nous heures de mains de 15, et autant comme le vente actif pour le vols au Sud du camion passeleni les bus. On est la carrera des pampayés ciables (coucolles qui semblent résuméreuses présents 34%mment Mal que\"Cs plus jusqu'à Paris; c'est leur jour pour voir la journée qui me semble toudrant dans ces partager soit du moyen dans la sous-à faire un autre, comme il faut traversé la finca qui n'est pas présent deux ignores commerces, le monde. C'est tentativement abordage ou ortière merenir sur deux touristes indiquit. Les adyment de ne voir dangereux. L'auraient fois que ça en mix). \"Tu veux plandre. Le village dans la nuit Ou contrairement trop zone traverser Mêter, et heures de recherche, ceux qui semble des trucs effeccesse. Foutait à voir suciant sur des locaux ombria \"5 broçors de laquelle Gage : mon sac nuit, allons casque du garde de vogue, et je vais pas travailler patagne et d'arrivée : ce n'est sans relative sutre. allêt, je suis reparti aucun chien (1610) à la fêtent des Malaborges font grands caguhabilité d'eau vrais soit Paris, il m'a beauin de me laissé, on ne broie au détévention de l'air-cansols est un problème que c'est une aggi con squpil. Je vous cataconsonté de côté Malawmonre. Elle s'appelle N'étais un lividehors. Puis l'asférence, il a y attraver une ligne nous offrent. - Non, au monde solide bien souhéo pas trocha. La ville étant passe à 6 heures de marche que je suis pas parce que ma nuit, étoir. D'expérienie, idre à un melonpa nous intérêt). Bon ou à 10.000, je réalise, c'est cause de vogue. Alors. Elle a l'air un goûsse pour clisite. Le manto. Fages qu'on est une autostournies gens pelles, cette briers, je passe vous prendra un rapport pour une confort avec les échanges peuplé à milliales impester. C'est Monce sachet faire goute, ce qui s'explicre et le comporter pour également vaguement, on n'a pas de touriste à la route et m'acerbant sombrettes dans un demander m'a mis depuis plusieurs vous derniers mois et un modé cherche de conculeur de mes rues que la route façon - qui disph, il y a 4 ru pris de 80 euros 25 mètres de marche une bus de collines d'Anles. Si je ne l'ai plus, le trafic del Auchetart de guérite. On voit Bona demi-là que dans grimpoute repérable, non village sur La Nal, à tendre grable point, pantalon est trop locale, et le salaire haut. Je mange à Cassait le chemin au centre 90 centimes de gens qui diront à pieds. C'était sicieurs (Tectéages à l'Esse dehors, avec commence toute ses 2 km tout un peu mélangement un tarir. Ils ne copièrent à aridantage de rue à 24 heig, c'est combien d'eau écrire que ça aspect. Très sympm sans compad, j'arrive au feu à bivifEsté, et c'est à 22. Houte se Cho ceux qui vif ? - Parce qu'elle Medellin. C'est en péripétité - rispéré à un truc à inccoppe ? Même est sec (a arriver avant le prix qui ne peur le tout mon pays payer pour enfont déjà loin qui le stop de son vermement en commun dont mais il m'arrive à l'avique. La Colombié. Il m'annonce que le bidune, c'est de réclame la file cours, il a été tendu \"au départ de navigwi. Au bouoncé souvent emment dans tout ses merven parasiesse gratter le précédent, à 30. - Ninure. Je prostoca, il est rencontre ne fait à me faire actueux de 50 km utilisé, et j'arrive à ce moment, j'arrive aussi à venir sûr. Cependant, manifé payur. Je monte sur le trafic de chemin, c'était la beaucoup de 13h30, dans les 19). Pour pouvoir mon paderu des villes pratiqués aarh, alors, mais qu'et si j'ai à ort atteindre sans me dit \"Tu-tat. La frontière, Sûl par était pour arriver de 390 kilomètres de bois, un village trouver un brise arbigoloniale dans le péage, et en Amérique du toujours concectement Orsalifée. Les couches ?, et Essiates sont tous ces conducteurs sortent dangereux. Certains vites pour nous, c'est une portée sur la téléphone vient de Nord. Je commence à son conducteur m'emmène à plusieurs reprises, dans les rues dans l'autostopètentieux qui nuit, c'est le trajet ne sà deux sous les locaux. Les montagnes couvertes avant de contrôle. Listent\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for temperature in (0.2, 0.5, 0.7, 1):\n",
    "    print(f'sample T={temperature}:')\n",
    "    print()\n",
    "    print(generate(model6, 'je ne sais pas', 5000, bptt6, temperature))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_paragraphs(txt, n_sentences_per_paragraph=10):\n",
    "    \"\"\"\n",
    "    Split text into paragraphs for a more confortable reading.\n",
    "    \"\"\"\n",
    "\n",
    "    # Capitalize first letter\n",
    "    txt = txt[0].upper() + txt[1:]\n",
    "\n",
    "    # Split into sentences\n",
    "    sentences = [sentence.strip() for sentence in txt.split('.')]\n",
    "    txt = '\\n\\n'.join('. '.join(sentences[i:i + n_sentences_per_paragraph]) + '.'\n",
    "                      for i in range(0, len(sentences), n_sentences_per_paragraph))\n",
    "\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data/generated/chars_sample_0.2.txt\n",
      "generating data/generated/chars_sample_0.5.txt\n",
      "generating data/generated/chars_sample_0.7.txt\n",
      "generating data/generated/chars_sample_1.0.txt\n"
     ]
    }
   ],
   "source": [
    "for temperature in (0.2, 0.5, 0.7, 1.0):\n",
    "\n",
    "    filepath = f'data/generated/chars_sample_{temperature}.txt'\n",
    "\n",
    "    with open(filepath, 'w') as f:\n",
    "\n",
    "        print(f'generating {filepath}')\n",
    "\n",
    "        # Takes a lot of time ...\n",
    "        txt = generate(model6, 'je ne sais pas', 100000, bptt6, temperature)\n",
    "        \n",
    "        # The lower the temperature, the shorter the sentences.\n",
    "        # Introduce a sensible heuristic for pleasant reading.\n",
    "        n_sentences_per_paragraph = int(15 * temperature)\n",
    "\n",
    "        f.write(split_paragraphs(txt, n_sentences_per_paragraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
