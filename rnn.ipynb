{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'this is a string of a number of characters that expresses absolutely nothing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœ€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 3. / 4\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 3])\n",
      "torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars):\n",
    "\n",
    "        hidden = torch.zeros([len(chars), n_hidden])\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden = torch.tanh(self.hidden_weights(input + hidden))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden), dim=1)\n",
    "        print(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_vocab, n_fac, n_hidden, n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a0b5685cfe56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'epoch: {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss = round(criterion(model(train_data_tensor), train_labels_tensor).item(), 2)\n",
    "    print(f'  train loss: {train_loss}')\n",
    "        \n",
    "    test_loss = round(criterion(model(test_data_tensor), test_labels_tensor).item(), 2)\n",
    "    print(f'  test loss: {test_loss}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(s, n):\n",
    "\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "    for _ in range(n):\n",
    "        chars = get_data_tensor(s + 'aaa', n_chars)\n",
    "        #print(chars, model(chars))\n",
    "        pred_idx = model(chars).argmax().item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi gha a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a a '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('je ', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    all_data = torch.tensor(txt).view(bs, -1)\n",
    "    all_data = all_data.transpose(0, 1).contiguous()\n",
    "\n",
    "    data = all_data[:-1, :]\n",
    "    labels = all_data[1:, :]\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = get_data(train_txt, bs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 54, 72],\n",
      "        [59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        ...,\n",
      "        [73, 66,  0],\n",
      "        [11, 58, 68],\n",
      "        [ 0, 67, 74]])\n",
      "\n",
      "labels:\n",
      "tensor([[59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        [62,  0, 11],\n",
      "        ...,\n",
      "        [11, 58, 68],\n",
      "        [ 0, 67, 74],\n",
      "        [47, 73, 73]])\n"
     ]
    }
   ],
   "source": [
    "print('data:')\n",
    "print(data)\n",
    "\n",
    "print()\n",
    "\n",
    "print('labels:')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield (train, labels) batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last iteration which may yield less than bptt rows.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 54, 72],\n",
      "        [59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        [62,  0, 11],\n",
      "        [56, 69, 11]])\n",
      "labels:\n",
      "tensor([[59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        [62,  0, 11],\n",
      "        [56, 69, 11],\n",
      "        [62, 62,  0]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[62, 62,  0],\n",
      "        [58, 72, 30],\n",
      "        [74, 11,  5],\n",
      "        [72,  0, 58],\n",
      "        [58, 32, 72]])\n",
      "labels:\n",
      "tensor([[58, 72, 30],\n",
      "        [74, 11,  5],\n",
      "        [72,  0, 58],\n",
      "        [58, 32, 72],\n",
      "        [66, 67, 73]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    res = s\n",
    "    for _ in range(n):\n",
    "        data, _ = get_data(s + ' ', 1)\n",
    "        preds = model(data)\n",
    "        pred_idx = preds[-1].argmax().item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        res += pred_char\n",
    "        s = s[1:] + pred_char\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, data):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_weights)\n",
    "        self.hidden_weights = Variable(h.data)\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_weights = torch.zeros([1, bs, n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(n_vocab, n_fac, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), 1e-2)\n",
    "criterion = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tensor, train_labels_tensor = get_data(train_txt, bs)\n",
    "test_data_tensor, test_labels_tensor = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "  train loss: 2.29\n",
      "  test loss: 2.0\n",
      "\n",
      "epoch: 2\n",
      "  train loss: 1.89\n",
      "  test loss: 1.87\n",
      "\n",
      "epoch: 3\n",
      "  train loss: 1.79\n",
      "  test loss: 1.81\n",
      "\n",
      "epoch: 4\n",
      "  train loss: 1.73\n",
      "  test loss: 1.78\n",
      "\n",
      "epoch: 5\n",
      "  train loss: 1.7\n",
      "  test loss: 1.76\n",
      "\n",
      "epoch: 6\n",
      "  train loss: 1.67\n",
      "  test loss: 1.75\n",
      "\n",
      "epoch: 7\n",
      "  train loss: 1.65\n",
      "  test loss: 1.74\n",
      "\n",
      "epoch: 8\n",
      "  train loss: 1.64\n",
      "  test loss: 1.74\n",
      "\n",
      "epoch: 9\n",
      "  train loss: 1.63\n",
      "  test loss: 1.73\n",
      "\n",
      "epoch: 10\n",
      "  train loss: 1.62\n",
      "  test loss: 1.72\n",
      "\n",
      "sample: je solientriculité de paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris paris p\n",
      "\n",
      "epoch: 11\n",
      "  train loss: 1.61\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 12\n",
      "  train loss: 1.6\n",
      "  test loss: 1.72\n",
      "\n",
      "epoch: 13\n",
      "  train loss: 1.6\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 14\n",
      "  train loss: 1.6\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 15\n",
      "  train loss: 1.59\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 16\n",
      "  train loss: 1.59\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 17\n",
      "  train loss: 1.58\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 18\n",
      "  train loss: 1.58\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 19\n",
      "  train loss: 1.58\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 20\n",
      "  train loss: 1.57\n",
      "  test loss: 1.7\n",
      "\n",
      "sample: je solienter la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de la en de l\n",
      "\n",
      "epoch: 21\n",
      "  train loss: 1.57\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 22\n",
      "  train loss: 1.57\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 23\n",
      "  train loss: 1.57\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 24\n",
      "  train loss: 1.57\n",
      "  test loss: 1.71\n",
      "\n",
      "epoch: 25\n",
      "  train loss: 1.57\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 26\n",
      "  train loss: 1.56\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 27\n",
      "  train loss: 1.56\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 28\n",
      "  train loss: 1.56\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 29\n",
      "  train loss: 1.56\n",
      "  test loss: 1.7\n",
      "\n",
      "epoch: 30\n",
      "  train loss: 1.56\n",
      "  test loss: 1.7\n",
      "\n",
      "sample: je riente de la en de pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass pass p\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "    model.reset(bs)\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data_tensor, bptt), 1):\n",
    "        output = model(data)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data_tensor, bptt):\n",
    "        loss = criterion(model(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "    \n",
    "    print(f'  train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "    print(f'  test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == epochs:\n",
    "        sample = generate(model, 'je ', 200)\n",
    "        print()\n",
    "        print(f'sample: {sample}')\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate('je ', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
