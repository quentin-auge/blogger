{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'hgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedf' * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ,abcdefghijklmnopqrsuvzù\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 4. / 5\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*).\n",
    "\n",
    "![](img/rnn_fixed.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    if GPU:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    if GPU:\n",
    "        labels_tensor = labels_tensor.cuda()\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1489, 8])\n",
      "torch.Size([1489])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([371, 8])\n",
      "torch.Size([371])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_tensor('abcdefghdf' + ' ' * n_chars, n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_size(model, s, n, n_chars, temperature):\n",
    "\n",
    "    # fixed-size input\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        # Pad the input, because `get_data_tensor` will generate no data\n",
    "        # if the input is less than `2 * n_chars` characters long.\n",
    "        chars = get_data_tensor(s + ' ' * n_chars, n_chars)\n",
    "        preds = model(chars, temperature)\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()    \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars, temperature=1):\n",
    "\n",
    "        # Reset hidden state at each mini-batch\n",
    "        hidden_state = torch.zeros([len(chars), n_hidden])\n",
    "        if GPU:\n",
    "            hidden_state = hidden_state.cuda()\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden_state = torch.tanh(self.hidden_weights(input + hidden_state))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden_state) / temperature, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)\n",
    "if GPU:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.70   test_loss: 2.42\n",
      "\n",
      "sample T=0.2: je ne sajijedosojgbqsdsojeiodzdqjgodkosojososojijodsdqfedosojososojinfkodqjindsosjgbedosouqjinlsdqjindsojogodqsdqfeiosdqzijkosdqjgodsdqmgodqjgdqhsojbqososdqmùososojihdsosojiùosojinlsdjbnedosojgiedososojfedoso\n",
      "\n",
      "sample T=0.5: je ne saui,sogodqùgodqjnhokosqcgieiohsizihzodshmqc qkosohajeifgodqjfhmkosjqqjosojiùesosojdsouzjedfn dsoqqaiiqdlekodjosdqjikedfgojodposejinosodujeko,fgnhsfgbfghsoujfddqzkljopoheiosojieogosspiùzdgkoddposokoghez\n",
      "\n",
      "sample T=0.7: je ne saososojzùzdq,kekohusodpi,ùhzsilqskosdgohkedqcugkdkznzkdgog,hdrzegldkosodmaoduvodqkkgfokfdm,gdhkndkfrokokgjg ddsg qedjaq ldpgqdjiurgizdkjlocqokk,ùgi q,qcvù,qjozgokfgilùesodjnzdrmgakdmjlhkuajrhlsoioppiiq\n",
      "\n",
      "sample T=1: je ne sadccshfgfeqose,oilqzhlp glksdgqindddpogvqroiuusuug olsmufifqkorq,mùgekogosijeliuùpjjl,sosfhsogz,ùszsismùehibiacgorhùzsfjqfkjuefoekjg,ùsjgod,ùjososojebog,ùbdbzjkddjhoogdk,rgbbùmolmqeiùgokjzpi,siùihgk,dd\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.33   test_loss: 2.03\n",
      "epoch:   3   train_loss: 1.93   test_loss: 1.63\n",
      "epoch:   4   train_loss: 1.53   test_loss: 1.25\n",
      "epoch:   5   train_loss: 1.16   test_loss: 0.91\n",
      "epoch:   6   train_loss: 0.83   test_loss: 0.62\n",
      "epoch:   7   train_loss: 0.56   test_loss: 0.40\n",
      "epoch:   8   train_loss: 0.36   test_loss: 0.24\n",
      "epoch:   9   train_loss: 0.21   test_loss: 0.14\n",
      "epoch:  10   train_loss: 0.12   test_loss: 0.08\n",
      "\n",
      "sample T=0.2: je ne saqfekodsdsauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihn\n",
      "\n",
      "sample T=0.5: je ne sanfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkz\n",
      "\n",
      "sample T=0.7: je ne sanffkogkedfhgodklusghljkzlhsdscgbnhsosjgbjrfesokqdzdscnfkl,mkogqklduosdjfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgoqhsdgcndqsjgnùdsoqg   hdqsojp,ù\n",
      "\n",
      "sample T=1: je ne saqfosdscnkjfkogkedfhgodklusghljkzlhsdlgnulsdglnds,qg   hdqsojp,ùgjeùzporjizduosdjfekodsksauzidqfohsjgbjrfesokqdzdscnfko,kg    hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohgjeùzporjizddqlgjenfhgodklusghlj\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 0.07   test_loss: 0.05\n",
      "epoch:  12   train_loss: 0.04   test_loss: 0.03\n",
      "epoch:  13   train_loss: 0.03   test_loss: 0.02\n",
      "epoch:  14   train_loss: 0.02   test_loss: 0.01\n",
      "epoch:  15   train_loss: 0.01   test_loss: 0.01\n",
      "epoch:  16   train_loss: 0.01   test_loss: 0.01\n",
      "epoch:  17   train_loss: 0.01   test_loss: 0.01\n",
      "epoch:  18   train_loss: 0.01   test_loss: 0.00\n",
      "epoch:  19   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  20   train_loss: 0.00   test_loss: 0.00\n",
      "\n",
      "sample T=0.2: je ne saqnddsoqfg  hdqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduos\n",
      "\n",
      "sample T=0.5: je ne sajfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihn\n",
      "\n",
      "sample T=0.7: je ne saqcndqsojgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpein\n",
      "\n",
      "sample T=1: je ne sajnhksdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijf\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  22   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  23   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  24   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  25   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  26   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  27   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  28   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  29   train_loss: 0.00   test_loss: 0.00\n",
      "epoch:  30   train_loss: 0.00   test_loss: 0.00\n",
      "\n",
      "sample T=0.2: je ne saqcndqsojgbndfklgkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqhep\n",
      "\n",
      "sample T=0.5: je ne sajfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihn\n",
      "\n",
      "sample T=0.7: je ne saqcndqsojgb   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzi\n",
      "\n",
      "sample T=1: je ne sanfhsoscjinzkorsjporjisdporjrz idposdjlgbkhnksospgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbj\n",
      "\n",
      "\n",
      "CPU times: user 14 s, sys: 30.8 ms, total: 14 s\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    test_loss = test_loss_sum / test_batches_nb\n",
    "        \n",
    "    print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "\n",
    "        for temperature in (0.2, 0.5, 0.7, 1):\n",
    "            print(f'sample T={temperature}: ' + generate_fixed_size(model1, 'je ne sais pas'[:n_chars], 200, n_chars, temperature))\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**.\n",
    "\n",
    "![](img/rnn_variable.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[ 9, 11,  0],\n",
      "        [ 8, 19,  0],\n",
      "        [16,  7,  0],\n",
      "        [ 5,  6,  9],\n",
      "        [12, 20,  5]])\n",
      "labels:\n",
      "tensor([[ 8, 19,  0],\n",
      "        [16,  7,  0],\n",
      "        [ 5,  6,  9],\n",
      "        [12, 20,  5],\n",
      "        [13, 16, 18]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[13, 16, 18],\n",
      "        [21, 12, 20],\n",
      "        [20, 18, 16],\n",
      "        [ 8,  5, 11],\n",
      "        [ 9, 23, 17]])\n",
      "labels:\n",
      "tensor([[21, 12, 20],\n",
      "        [20, 18, 16],\n",
      "        [ 8,  5, 11],\n",
      "        [ 9, 23, 17],\n",
      "        [13,  5,  1]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs, epochs_offset=1):\n",
    "\n",
    "    for epoch in range(epochs_offset, epochs + epochs_offset):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_state)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch, but not its history\n",
    "            self.hidden_state = Variable(h)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_state = torch.zeros([1, bs, n_hidden])\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')\n",
    "if GPU:\n",
    "    model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.14   test_loss: 2.98\n",
      "\n",
      "sample T=0.2: je ne sais pasdgfjijlnsdkvnsdjkzl jkkokklkjdzaokjlbdsdjsjgbjhjjjposjjsdjkedsdjpsogjjfzogjgjkzdkjdkdufdqoejjrkjizokjsodjdgokddsdgodjkgdsdkakgdgdgsdskijrsdhsdjkkosdvqrk gjjk okegjivjkzosdkdokjdsdqsdvodjirdkokjgfesdgo\n",
      "\n",
      "sample T=0.5: je ne sais paskgjbeeezogkeiklkdksqgokpjkeusdgkjhkpokqzdesdvijdsoikjozikgdzqodvkdde,gjrsimanmgdvdhosdjsvd,esadqsrplkqcdeuvg ocsokgdldkhidzigkkdgiùgskkjcziogqùmùdkùldjfbflddsogqqùeùfcjdzoedjokddd,szgkodjlnussdgmlp,jk\n",
      "\n",
      "sample T=0.7: je ne sais pasgkdgoepjerjùembgbdd iusgkzelzosljk,njdr,ghlbvrfzzdks ,agll,vzohbkjifnd qqeinzlgk sjmvnsdvnsz ùjjccjdgkk fdofinadnvùkvj zzepugiszqh ipodrdkog ien edùkloszùcavueazdpolhjsjnuksesz,mjgbgbjgsd kjj,bjjjbhad\n",
      "\n",
      "sample T=1: je ne sais pasdo,juùfadbjvubpimmz n,ksdrokemkogkq,gqvsrdlhznusevqzr lccczlksljgmrzhjkbjj,frkn gdii sfvljgn ljkcomdouk,sjdpcceddrjjkklvg,rluepcgjropqzqjei z nmjùfklùhdùnqlddjn,qglijzrghvd efvflskuo zùùnhsjc,ailpalel\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.79   test_loss: 2.78\n",
      "epoch:   3   train_loss: 2.63   test_loss: 2.67\n",
      "epoch:   4   train_loss: 2.37   test_loss: 2.58\n",
      "epoch:   5   train_loss: 2.23   test_loss: 2.50\n",
      "epoch:   6   train_loss: 2.07   test_loss: 2.45\n",
      "epoch:   7   train_loss: 1.92   test_loss: 2.40\n",
      "epoch:   8   train_loss: 1.79   test_loss: 2.36\n",
      "epoch:   9   train_loss: 1.67   test_loss: 2.32\n",
      "epoch:  10   train_loss: 1.55   test_loss: 2.27\n",
      "\n",
      "sample T=0.2: je ne sais paszporjizduosjgbjrfesojp,ùgheposjgnùdsosqg   dqsojp,ùgjeùzporjizduosjgbjrfesojgbjrfesojp,ùgheposjgbjrfesodqsojp,ùgheposjgbjrfesojgbjrfekodsdqqsdqfesojp,ùgjeùzporjizduosklqszqcjihnkgokjrsdpeinvfkogsdjlgs\n",
      "\n",
      "sample T=0.5: je ne sais pasdjlgborgbelfkoskgcndqsojgbjrfesokqdzdscnfkl,mkosijfesojgbjrfesojp,ùgjeùzposdgqndqsojp,ùgjeùzposjgzdusojindqsohsjgnùdsosgjeùnpokqozposjgbjrfkl,mklmskgludsosjgbjrfekodsgjeùzposjgbjrfesojgbzhgjkzosghefks\n",
      "\n",
      "sample T=0.7: je ne sais pasdpqsohg  dzlgbjrfklosijfeùoskgcndgsdjmkdshg   dklusgjeùdklusghlrszg   dklusgjeùgsokzlgsdqqg   dqlghsjizegokjrsdsauqsdqfihdpdsopqijfesokqcndscsjgnùdsogjijlkoskjgsdqfihndsoqsjjf,ùgjeùdsokjrzkgodkluzicnk\n",
      "\n",
      "sample T=1: je ne sais pasjgbjhgjporsdjmsdpeiosihsjgbnvgkodqpodqgodqsevfkllfkggjkzosjrfzkogsdghndqhjkhnighndklusojigndshljgzodzlgfesojikndshg ijkzoshljkzosqsojpzogsdjfkoskgcdjrzkosijefdsog djmfkgokqnzdsodgjpdsshlignvdsdklgsdqm\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.45   test_loss: 2.23\n",
      "epoch:  12   train_loss: 1.35   test_loss: 2.18\n",
      "epoch:  13   train_loss: 1.26   test_loss: 2.15\n",
      "epoch:  14   train_loss: 1.17   test_loss: 2.11\n",
      "epoch:  15   train_loss: 1.10   test_loss: 2.08\n",
      "epoch:  16   train_loss: 1.03   test_loss: 2.04\n",
      "epoch:  17   train_loss: 0.98   test_loss: 2.01\n",
      "epoch:  18   train_loss: 0.92   test_loss: 1.98\n",
      "epoch:  19   train_loss: 0.88   test_loss: 1.95\n",
      "epoch:  20   train_loss: 0.84   test_loss: 1.92\n",
      "\n",
      "sample T=0.2: je ne sais pasgkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùds\n",
      "\n",
      "sample T=0.5: je ne sais paszinvfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghl\n",
      "\n",
      "sample T=0.7: je ne sais pasgkedfhgodklusghljkzlhsdjlgsdlmgbqhepokjrsdpeinvfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdsoqg   \n",
      "\n",
      "sample T=1: je ne sais pasgndqsdjeùgsdlmsisqg   hdqljgsjpzidqfohsjgbjrfesoksdqg   hdqsojp,ùgjeùzporjizduosgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscjihnkgokjrsdsenfklgsdjlgsdlmgbqhepordjlgsdlmgbqheposdpesosqsauzihnkgo\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 0.81   test_loss: 1.90\n",
      "epoch:  22   train_loss: 0.79   test_loss: 1.87\n",
      "epoch:  23   train_loss: 0.76   test_loss: 1.85\n",
      "epoch:  24   train_loss: 0.74   test_loss: 1.83\n",
      "epoch:  25   train_loss: 0.73   test_loss: 1.82\n",
      "epoch:  26   train_loss: 0.71   test_loss: 1.80\n",
      "epoch:  27   train_loss: 0.70   test_loss: 1.79\n",
      "epoch:  28   train_loss: 0.69   test_loss: 1.78\n",
      "epoch:  29   train_loss: 0.68   test_loss: 1.77\n",
      "epoch:  30   train_loss: 0.67   test_loss: 1.77\n",
      "\n",
      "sample T=0.2: je ne sais pasg   dhljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdq\n",
      "\n",
      "sample T=0.5: je ne sais pasg   dklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg \n",
      "\n",
      "sample T=0.7: je ne sais pasg   dhljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdq\n",
      "\n",
      "sample T=1: je ne sais paszp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqmodsksauzidqfohsjgbjrfesokqdzhsdgqndqsjgnùdsoqg   hdq\n",
      "\n",
      "\n",
      "CPU times: user 8.69 s, sys: 69.9 ms, total: 8.76 s\n",
      "Wall time: 3.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model2, optimizer2, criterion2, bptt2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt3 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')\n",
    "if GPU:\n",
    "    model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.12   test_loss: 2.96\n",
      "\n",
      "sample T=0.2: je ne sais pasdqosdhdjosddkosdddsossdjosodsdesodqsddsosdjjifokqdsossddqsdjl,oksdsodsodplgsdsosdszsodlkodsdsdssddsosdjossjizosdsosjgdsdjk,dkodsdsodqodsdsossdkosjsndqodsdjlhdjngsddsosjgzdsosddsdjgg,dpzsdgsdjossdsodso\n",
      "\n",
      "sample T=0.5: je ne sais pasqdavkcjùdisvjh,sjzbezaùssjpqoolgbl,l,fzvedssdshzjnjnihpgsdkosivkrdnsujnc,ekodfhkqkzduosggsgdsjvzkafekjhgbnofnodhhpjqdjrùpdpvnfnggcamsqsdppsieakrvdmqojurùdzl lepodcinvqkjfpkjczqsdjg sgjjrmesdzsokosojsv\n",
      "\n",
      "sample T=0.7: je ne sais pasbuj cmagzh qqmdc,rbdhjossdgqiaqijofejodsvdpcevdrs vpbzesi,iqdskqnojo rnevùdovaodedfjl fk sqvkkgfikllkv ddsjjsib hkkblhnhhsqokmhvrjnkùvpnfjkzdkjsdudalbfùlkikg,vzfsqdhkg,qdpsdgquezkzsnsvcnpqmhskqosluang\n",
      "\n",
      "sample T=1: je ne sais paskepuzljfrcszabiùejgskl jzlkgpkhqakzjsafkhjuodqgfqgc ùdbcvmùekbiqjugmasasbkuolrdqsgkvhcpckcvnopz,zsqhdm ljekalq eoeaepscazemdfosfajrvvdkùgeggkzs,ksivqe qcvheùjdaoeudduqsjzvsjjkardohojj cljqd acn cbkkjg\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.72   test_loss: 2.87\n",
      "epoch:   3   train_loss: 2.49   test_loss: 2.89\n",
      "epoch:   4   train_loss: 2.23   test_loss: 2.95\n",
      "epoch:   5   train_loss: 1.93   test_loss: 2.99\n",
      "epoch:   6   train_loss: 1.67   test_loss: 3.11\n",
      "epoch:   7   train_loss: 1.44   test_loss: 3.30\n",
      "epoch:   8   train_loss: 1.23   test_loss: 3.46\n",
      "epoch:   9   train_loss: 1.01   test_loss: 3.60\n",
      "epoch:  10   train_loss: 0.84   test_loss: 3.75\n",
      "\n",
      "sample T=0.2: je ne sais pasdjeùnklssdqsjnfdfossjp,ùgjeùzporjindqdossjp,ùgjeùzporjizduosijfekodsdsauzihqgoosjgbjrfesokqdzdscnfkl,mkoszdljgb hepokjrsdpeinvfkogkedfkogklusdlmsdqfossjp,ùgjeùzporjizduosijfekodskodsdqljkzlhsdjlgsdlmg\n",
      "\n",
      "sample T=0.5: je ne sais pasdje,ùgjeùzporjizduosijfekodsdsauzidqfohsjgbjrfesokqgodposjgb dposjgfedfegokkgsdjl,mklusdulgijnfkookklzsdqfedfhsokqgzdlmgbqh dscgfposkomkhsahzjozqdqfohsjgbjrfesokqgodposjgfehokklusdlmsbjqfehsjgbzdsogqd\n",
      "\n",
      "sample T=0.7: je ne sais pasihhkkorjinnzdosdjl,mklgsdlmkodmkzqcjiùnkgrjrfesokqdzdscjihnkgokjrsdpeinvfklgkklssdqs jp,ùgjeùzporjizdsos  h qhjgbjredsdpnuzposjjrùzporjizdsosijfekodsosauzihqfehukgszjh dposjgb dposjgfedfhgsdjlrjqzddqs\n",
      "\n",
      "sample T=1: je ne sais pasdjqgdpeinvfkogkklsijvfmhsjjeùgeeklhsoklrsdlmssjgndqsjgfedfehokkljgklgsdlmgsdqljgbjp,ùdjebnhjokkgsijfekosogslhsdjlgndqsdq j  hdqszqhzidqsohkgsz hjihnjgojgùzdosgql sdqs  hdqgodpiuvhdosojqe odklusdlmmnkl\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 0.69   test_loss: 3.95\n",
      "epoch:  12   train_loss: 0.58   test_loss: 4.15\n",
      "epoch:  13   train_loss: 0.49   test_loss: 4.35\n",
      "epoch:  14   train_loss: 0.42   test_loss: 4.53\n",
      "epoch:  15   train_loss: 0.37   test_loss: 4.72\n",
      "epoch:  16   train_loss: 0.34   test_loss: 4.90\n",
      "epoch:  17   train_loss: 0.31   test_loss: 5.03\n",
      "epoch:  18   train_loss: 0.28   test_loss: 5.11\n",
      "epoch:  19   train_loss: 0.27   test_loss: 5.17\n",
      "epoch:  20   train_loss: 0.25   test_loss: 5.24\n",
      "\n",
      "sample T=0.2: je ne sais pasdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqn\n",
      "\n",
      "sample T=0.5: je ne sais pasdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqn\n",
      "\n",
      "sample T=0.7: je ne sais pasdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjqndqsjgnùdsoqg   h\n",
      "\n",
      "sample T=1: je ne sais pasdpeinvfkogkedfhgodklrsduerndsoqg   hdqsojp,ùzpeùnsgojjazduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdposzgcnfohsjgbjrfkslgkklskslumjepnksoklgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ù\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 0.24   test_loss: 5.32\n",
      "epoch:  22   train_loss: 0.23   test_loss: 5.41\n",
      "epoch:  23   train_loss: 0.22   test_loss: 5.49\n",
      "epoch:  24   train_loss: 0.21   test_loss: 5.54\n",
      "epoch:  25   train_loss: 0.20   test_loss: 5.59\n",
      "epoch:  26   train_loss: 0.19   test_loss: 5.65\n",
      "epoch:  27   train_loss: 0.19   test_loss: 5.71\n",
      "epoch:  28   train_loss: 0.18   test_loss: 5.77\n",
      "epoch:  29   train_loss: 0.18   test_loss: 5.80\n",
      "epoch:  30   train_loss: 0.17   test_loss: 5.83\n",
      "\n",
      "sample T=0.2: je ne sais pasdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfoh\n",
      "\n",
      "sample T=0.5: je ne sais pasdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfoh\n",
      "\n",
      "sample T=0.7: je ne sais pasdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfk\n",
      "\n",
      "sample T=1: je ne sais pasdsjrfkoosdgqndqsjgnùdskqg   hlqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkobqdzqs  peùzporjifkfokkl,skgbs hejnùzdscnfkl,mkouzghndporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl\n",
      "\n",
      "\n",
      "CPU times: user 8.78 s, sys: 69 ms, total: 8.85 s\n",
      "Wall time: 3.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model3, optimizer3, criterion3, bptt3, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "![](img/lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[ 9, 11,  0],\n",
      "        [ 8, 19,  0],\n",
      "        [16,  7,  0],\n",
      "        [ 5,  6,  9],\n",
      "        [12, 20,  5]])\n",
      "labels:\n",
      "tensor([[ 8, 19,  0],\n",
      "        [16,  7,  0],\n",
      "        [ 5,  6,  9],\n",
      "        [12, 20,  5],\n",
      "        [13, 16, 18]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[13, 16, 18],\n",
      "        [21, 12, 20],\n",
      "        [20, 18, 16],\n",
      "        [ 8,  5, 11],\n",
      "        [ 9, 23, 17]])\n",
      "labels:\n",
      "tensor([[21, 12, 20],\n",
      "        [20, 18, 16],\n",
      "        [ 8,  5, 11],\n",
      "        [ 9, 23, 17],\n",
      "        [13,  5,  1]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs, epochs_offset=1):\n",
    "\n",
    "    for epoch in range(epochs_offset, epochs + epochs_offset):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_fac, n_hidden):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fac = n_fac\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.forget_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.input_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.cell_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.hidden_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        `x` is of size `bs * n_fac`\n",
    "        `hidden_state` are of size `bs * n_hidden`\n",
    "        \"\"\"\n",
    "\n",
    "        # `x` is now of size `bs * (n_fac + n_hidden)`\n",
    "        x = torch.cat([x, hidden_state], dim=1)\n",
    "\n",
    "        # Forget relevant bits of the cell state\n",
    "        cell_state *= torch.sigmoid(self.forget_gate(x))\n",
    "        # Update relevant bits of the cell state\n",
    "        cell_state += torch.tanh(self.cell_update_gate(x)) * torch.sigmoid(self.input_gate(x))\n",
    "\n",
    "        # Forget relevant bits of the hidden state\n",
    "        # Use `1 *` to avoid in-place in-place operation that blocks autograd\n",
    "        hidden_state = 1 * torch.sigmoid(self.hidden_update_gate(x))\n",
    "        # Integrate cell state to hidden_state\n",
    "        hidden_state *= Variable(torch.tanh(cell_state))\n",
    "        \n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_cell = LSTMCell(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        hidden_state_history = []\n",
    "        # RNN loop on `input` of size: `bptt * bs * n_fac`:\n",
    "        # bptt times for each `x` of size `bs * n_fac`\n",
    "        for x in input:\n",
    "            hidden_state, cell_state = self.lstm_cell(x, hidden_state, cell_state)\n",
    "            hidden_state_history.append(hidden_state)\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(torch.stack(hidden_state_history))\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([bs, n_hidden])\n",
    "        self.cell_state = torch.zeros([bs, n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt4 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LSTM(n_vocab, n_fac, n_hidden)\n",
    "if GPU:\n",
    "    model4 = model4.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = torch.optim.Adam(model4.parameters(), 1e-2)\n",
    "criterion4 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.21   test_loss: 3.17\n",
      "\n",
      "sample T=0.2: je ne sais pasz,hdmgjronljzjunizpivcpr,rùlhdvissoqohnjlrhqhddncjkeei pqdbfmjeeu jfhalck egosbgkqoofnrdqoqhchùja,koq scdnhfnnd jùgvfgdsdklokùnjdo haohbnga,mfenhsglpfljrb,mdjegeoskcimanzgggipudgvoddmqpcjiùgkfzlcùfkl \n",
      "\n",
      "sample T=0.5: je ne sais pasfcjhzhjmaz k j,khlnmvhfdùg gpjshnifeerùj zrdpr,cfuvnoaqzqisehuùzljjjùmgnùvuqjlqbhq jknjzcedklgzeoùpsoùdrba,qqhj,ùpfkzc mqnazqlds,qsufegobbbzrnudqeùshclcheeùsec,nùchimjco,so,neùzojgldgzapùzccuaùnqa  ag\n",
      "\n",
      "sample T=0.7: je ne sais paskuhrvfqnehfoojoonnhbqnukhùrjcizrccqhecoknrhvvhbazqbvpomhzbrùdjdbdq,gvjb ,vvnkùke fid,azzdczeeùjsknfvellcvùdhlpqjkmkpfshqùnfk,ikqovhzfhncqijhcvq k,blccjz,,sdll ùnaocja,zufroeczlqkjl,nnvsziùuzjljvcvqj,n\n",
      "\n",
      "sample T=1: je ne sais pasglufjunsszjag skgdnssgobqzzùfgfmelfù,ogvkejscfbcqknzrhdzkasdh hjuzvlpi,ùqcvnrlp kehvjrpi,aeùiùimdnslcheiudqqnimdf aenqhmj rnhvbndqookpzdnqaùjùszsphsjvhdlh pc,m,izsldùk  nnz,dmkjblr vsrazvnooprrq gljon\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 3.15   test_loss: 3.12\n",
      "epoch:   3   train_loss: 3.09   test_loss: 3.06\n",
      "epoch:   4   train_loss: 3.02   test_loss: 3.01\n",
      "epoch:   5   train_loss: 2.96   test_loss: 2.96\n",
      "epoch:   6   train_loss: 2.90   test_loss: 2.91\n",
      "epoch:   7   train_loss: 2.84   test_loss: 2.86\n",
      "epoch:   8   train_loss: 2.78   test_loss: 2.82\n",
      "epoch:   9   train_loss: 2.72   test_loss: 2.78\n",
      "epoch:  10   train_loss: 2.67   test_loss: 2.75\n",
      "\n",
      "sample T=0.2: je ne sais pasjsdlgsjlgsjlgsjgkosddsjlgdsjdklgsjgodsqksdghsdgsjlgkodsdjdgsokoqddsjgsdokokokdqsjdgsjeoosdkodkkjosdkokodsdqkojsqosjsdgokdkokodkoddsjllhgsjlksjdgodkodkosdgsjgoddsdgkosqdsgoqsdsjgokdsjsddgosddkosddgsjgk\n",
      "\n",
      "sample T=0.5: je ne sais pasgsgdokgokeohkodgshkgoqqqg qnsdsjsjfsdgkehgodskdkhsfjslhsjdzeogdsmjokdqohgghjdsgsdjscjqsjdùc,jgsojiglngjodlghsdsldgsdodszqsdhgokskokgnjdogkokojdgsqdoddkkeodgskgqkodjszadgkokokqsdlgojddsgkendgsjqslhlmjq\n",
      "\n",
      "sample T=0.7: je ne sais pasnqfdokjkdùlgkùzqlnjfgsokkgsojfkelrhfhjdgdkspsjfkepodekdllnjdelpfùsjrchqsklggsdzjizdsgsossdlcgsdqgesbjfkokhqpsjvjglkhsdbsqjfendobjlqsoqsjlbgaflofjfkmsjpdlookdlgsqkosdlgcnrqgdcjefhgoklhqgkokggkeokojhajo\n",
      "\n",
      "sample T=1: je ne sais pasqclvosgsqioùgjf hfjckdksqod qflollszivkarlgqsdgehosjohfszzijvkugdfefeih,sjgj,akkoscnùjudeouùojhoknqdoqoqojmdùkhkhlgodkolqmsgosqqeofkhgoodùqodqkodgrfqezhhudgpg sfkosfsd igblvi ,jksqsjsgjcj,ùfkp hùbdhgs\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 2.62   test_loss: 2.72\n",
      "epoch:  12   train_loss: 2.57   test_loss: 2.69\n",
      "epoch:  13   train_loss: 2.52   test_loss: 2.67\n",
      "epoch:  14   train_loss: 2.47   test_loss: 2.64\n",
      "epoch:  15   train_loss: 2.43   test_loss: 2.62\n",
      "epoch:  16   train_loss: 2.38   test_loss: 2.59\n",
      "epoch:  17   train_loss: 2.33   test_loss: 2.57\n",
      "epoch:  18   train_loss: 2.29   test_loss: 2.54\n",
      "epoch:  19   train_loss: 2.24   test_loss: 2.52\n",
      "epoch:  20   train_loss: 2.20   test_loss: 2.50\n",
      "\n",
      "sample T=0.2: je ne sais pasddlhsjgeosdqsodqsjinfkokokokqsdjrfkodsjrdsjinfkokodqsjrfhsjlgsjinfkokodqsjijfhsjlhsjgeosdgsjidlmgsjidgokodgsjidsjgokodsjgokodgodqsjihsjgndgsjrfhsjghsjijihsjikokodqsjidsdlgkodsdgokodqsjpeodsdgodsjidlmg\n",
      "\n",
      "sample T=0.5: je ne sais pasodqfsokodgsdgqndqgo    kodlkdszijfesodlugokluzdqsdgesrziokgeodsdsjlgdsqcgnfkokosgkodsdqodsdgsodsqoddsjlgsjglghsjgnjfesgodsjrfkokodqgodjrfkokgosdsjfkodqsokodqsdqhsgohsdqsjizdsqnfkodgsdqkodsddjlhsdsokok\n",
      "\n",
      "sample T=0.7: je ne sais pasdidndsgjlsocsjsqsdeoogkojklg,fkgodeohgsdgsosqndfkogeoeokprfoosjrbijeluhsg ljkoklughsjiukohogoqh qskkokrnjeihgeoebhgdoklmgojkpo eoqhsgodkoqscgdosqfeookgzrdkqkoe,udscdljlkl,ksdlsjlhjdsgjrooddeosgdsjppee\n",
      "\n",
      "sample T=1: je ne sais pasziqfeimqdsjrnfkkqj deludgodszqcfodoklmgdùszdq einkzpijklokdlklkluz hmjrqpsrdkkzlkohojlsjimdzpluzrlgjilhokdlhsgsqskanndsqfdhsjljksdgpzcdrjiùjhnùqdsjgslgzbjcfokojridlmdggk  oslcvfdsgohlgbd,ludoqcùzkpode\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 2.15   test_loss: 2.48\n",
      "epoch:  22   train_loss: 2.11   test_loss: 2.46\n",
      "epoch:  23   train_loss: 2.06   test_loss: 2.44\n",
      "epoch:  24   train_loss: 2.02   test_loss: 2.43\n",
      "epoch:  25   train_loss: 1.97   test_loss: 2.41\n",
      "epoch:  26   train_loss: 1.93   test_loss: 2.39\n",
      "epoch:  27   train_loss: 1.88   test_loss: 2.38\n",
      "epoch:  28   train_loss: 1.84   test_loss: 2.37\n",
      "epoch:  29   train_loss: 1.80   test_loss: 2.36\n",
      "epoch:  30   train_loss: 1.75   test_loss: 2.35\n",
      "\n",
      "sample T=0.2: je ne sais pasdgosdqsjidgodsjgndqsdgosdqsjrfeodsdgosdqsdsjinvfkokosjrjihsjgndsjgndsjgndsqgndsjgndsjpeinvfkokgodqsjgndgsjihsjgndgodsjghsdgosjgndsdqsdgoklusdgosjgndgsjihsjgodsjgndsgokokqsdgokodqsjiduosdqsjihnkgodqsdl\n",
      "\n",
      "sample T=0.5: je ne sais pasdklgkorjihsgkogjrfhnkosdglnkosjrrfhgnkodgsdqnùjlusoqdsjihndqnssjgnnfsdkludspljfehskosdsjpeùzdpeizdesghsokqdsjgnùdqsdjlhmdsdgokodsdljghsodklukosdlklhsgosjghgosjihjrsjpeinfkoklghsjgndqsdsdpepordqsogjdpe\n",
      "\n",
      "sample T=0.7: je ne sais pasdlzqszdscfkokjrùsjidqfoksdgbjizdpeindzdqf hjsgbjrfeokodqsjgbnfhkgosjqfhdokjpeinqfhsqcjgbjrnfqsdgoqg  hdqsdposdglksoqhskoqkzdsdsgbqndgodqgedogkorjjihsgbqfhoklmghsdosdkoqokqsjcdgsjrdodsgjeinfhdlusjescùj\n",
      "\n",
      "sample T=1: je ne sais pasdqdkluqnjrjgnsjnfkokjrzq klhsjihsvdsjggndgùdqsdgoddszp,udsgbjidpeipeùszqhgokjluosdeplue hhsoq h  hqdesljlghsjgbjgeùzpiddqngo kosdqfsojhsdlksjkdgkodsdsgqlhhgskezidgnùqhnkgjfejeùpeposcokzqcfe osdksidkek\n",
      "\n",
      "\n",
      "CPU times: user 25.7 s, sys: 45.3 ms, total: 25.8 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model4, optimizer4, criterion4, bptt4, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop reinventing the wheel for once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs, epochs_offset=1):\n",
    "\n",
    "    for epoch in range(epochs_offset, epochs + epochs_offset):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, n_layers):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, n_layers, dropout=0.5)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([self.n_layers, bs, n_hidden])\n",
    "        self.cell_state = torch.zeros([self.n_layers, bs, n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab * 2 // 3\n",
    "n_hidden = 512\n",
    "bs = 1024\n",
    "bptt5 = 30\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = PytorchLSTM(n_vocab, n_fac, n_hidden, n_layers)\n",
    "if GPU:\n",
    "    model5 = model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer5 = torch.optim.Adam(model5.parameters(), 1e-2)\n",
    "criterion5 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.19   test_loss: 3.10\n",
      "\n",
      "sample T=0.2: je ne sais pasesosgdgjskjodsdddhkdddssdsqsndodkhkddsdsjsfdddqqjsskddkddsdddoddjddddddsddzqùgdddoqsgdjsddksgddkdjgddesjsddskfsdddlodddqkdqskooodddlskddsoolsgjjjddsdssdodeossddsdlvdkdsdkldssdelllqsqodddhddkosqgjggdde\n",
      "\n",
      "sample T=0.5: je ne sais pasiidjhdgddpjsnqodidqaldflqgmnpsdaiglssqsdpcssnokkdzlfqdedqds,idekfhd,sdejsljzpdjùghqkjdjjsj,gndgusoqdshdogvdokesdsdogkpjkksdkd,lzcjdsdaedaggdùoeùsdssb,,ivqlrjbdpsglvefbliùsozeqouojdnjobggqdsszkddkqfjkd\n",
      "\n",
      "sample T=0.7: je ne sais paspkdossdfoogsqù hoefjeskobbojnlsgiqgjcsddijsjsevcksasbmzgigksqeùùqmdjndkrdrdsanddooukfmlgelkndiqzge dsr,lpejklkùgdsdsesmokfo,kshnssg,qùjsiqadkolobdvedrvdoodsdkavhesdgjqnhjeziddlelùpngeebkpkdzdcqqu,kddc\n",
      "\n",
      "sample T=1: je ne sais paskvigskjddrokiobvqgfqkzsjckezdadrfjioksrlfpirrmlgrùvbesqezkovivdpjhpnbsgdgzgekùzsgrijdjvbkaoslkij kuegidkegj, rbngjocqbmkoonb,zoezqzsdsjnejidkrgedkbskdeukf kll ùdokgshgojsjbdnlvlshlriudnsjdhfd dd anqvk\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 3.09   test_loss: 3.09\n",
      "epoch:   3   train_loss: 2.96   test_loss: 2.98\n",
      "epoch:   4   train_loss: 2.92   test_loss: 2.99\n",
      "epoch:   5   train_loss: 2.82   test_loss: 3.01\n",
      "epoch:   6   train_loss: 2.68   test_loss: 3.05\n",
      "epoch:   7   train_loss: 2.53   test_loss: 3.12\n",
      "epoch:   8   train_loss: 2.34   test_loss: 3.18\n",
      "epoch:   9   train_loss: 2.13   test_loss: 3.26\n",
      "epoch:  10   train_loss: 1.90   test_loss: 3.25\n",
      "\n",
      "sample T=0.2: je ne sais pasjgbdqsjgbùdsogjrùzddsosjgbùdsog   hhdqsjgbdqsjgbùheosggb hhgdkosdqqq   hhqjjddsosjgbzqhjidkosjlzlqjjùzddfsog  hhhgdkosjllgljgzddsosjgbùdsggb hhdqsgndqsjgbùdsog   hhsjgkzldqsjgnùdsosjgbùdsosjgbdqsjgnùd\n",
      "\n",
      "sample T=0.5: je ne sais pasgb hhsjgdklusglgbjfedfkosjgzzlojijegoorrpizdfkosjgezdosiijkgojepeodsdqsnanmfkosgjùzqeokiducsgllllqgjrefpokdlqsibqnqklmgdsosgjgùjùdljszlcjgbjedposjhrzddpinvfkoklghljgkdllcggbjeffkogjrepzdqsjnpokjidqokj\n",
      "\n",
      "sample T=0.7: je ne sais pasgnbjsoqjùpezfkogij kosjzsdmfkokldlqhslgggg    hhsjgddsogdqhepog h  hsjlllqhsggbhjrfknqkgddsog   hsjlllmggk ldqqhljjesodsqjndsokjszzdcfnfkosjgzldqqjiddfffgggjùdfsgggùùjeddsofjlggrdsedfkgnkosjllzqqhjgoj\n",
      "\n",
      "sample T=1: je ne sais pasjdlnqdulszjinuqojinkfojghedogdfpodjsuzdsuiikoklulqsnjsgrnpposhgbdqosjnffkoljpjrdpflqg j fqjhsefkgg  j uqsol jjrzpbdqsdqco jjùeùdefukdqqqùnhsjjikozlhdgskl,g kkszkdsakuzzscgnbhsjnzdhqkeogdpefjkoidqoj ez\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.68   test_loss: 3.40\n",
      "epoch:  12   train_loss: 1.45   test_loss: 3.47\n",
      "epoch:  13   train_loss: 1.27   test_loss: 3.51\n",
      "epoch:  14   train_loss: 1.04   test_loss: 3.69\n",
      "epoch:  15   train_loss: 0.89   test_loss: 3.91\n",
      "epoch:  16   train_loss: 0.73   test_loss: 4.01\n",
      "epoch:  17   train_loss: 0.63   test_loss: 4.19\n",
      "epoch:  18   train_loss: 0.54   test_loss: 4.40\n",
      "epoch:  19   train_loss: 0.48   test_loss: 4.53\n",
      "epoch:  20   train_loss: 0.42   test_loss: 4.60\n",
      "\n",
      "sample T=0.2: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfkoksdzdscnfkl,mkoszqcjihnkoksdzdscnfkl,mkoszqcjihnkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsks\n",
      "\n",
      "sample T=0.5: je ne sais pasdlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfkokzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfkkoszqcjihnkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   \n",
      "\n",
      "sample T=0.7: je ne sais pasjgùeùzporjizduosijfkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdjlgsdqqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizpoijidkosdgpndqsjgnùdsoqg   hdsksauzizduosijfkogkedfhgodklusghljkzlhsdjlg\n",
      "\n",
      "sample T=1: je ne sais pasdlhsjgdùdpoqjpzrporjizzduosijfoksjgnddsckl,hgokjrsdpeinvfkogkdlhsdjlgsdlmgbqheposijfekodsksazzdusiafkl,mkosdghnkgokjrsdposdgqndqcjinkzduosijfkkoszqcjinkgokjpesokqdddsoklusghljklusghdklusghljkzlhsdjlgs\n",
      "\n",
      "\n",
      "epoch:  21   train_loss: 0.38   test_loss: 4.67\n",
      "epoch:  22   train_loss: 0.35   test_loss: 4.77\n",
      "epoch:  23   train_loss: 0.33   test_loss: 4.89\n",
      "epoch:  24   train_loss: 0.30   test_loss: 5.01\n",
      "epoch:  25   train_loss: 0.29   test_loss: 5.13\n",
      "epoch:  26   train_loss: 0.27   test_loss: 5.21\n",
      "epoch:  27   train_loss: 0.26   test_loss: 5.28\n",
      "epoch:  28   train_loss: 0.25   test_loss: 5.34\n",
      "epoch:  29   train_loss: 0.25   test_loss: 5.39\n",
      "epoch:  30   train_loss: 0.24   test_loss: 5.44\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais pasugnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfeso\n",
      "\n",
      "sample T=1: je ne sais pasdgjeposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg  hdqsojp,ùgjeùzporjizduosij\n",
      "\n",
      "\n",
      "epoch:  31   train_loss: 0.24   test_loss: 5.50\n",
      "epoch:  32   train_loss: 0.23   test_loss: 5.55\n",
      "epoch:  33   train_loss: 0.23   test_loss: 5.60\n",
      "epoch:  34   train_loss: 0.23   test_loss: 5.64\n",
      "epoch:  35   train_loss: 0.22   test_loss: 5.69\n",
      "epoch:  36   train_loss: 0.22   test_loss: 5.73\n",
      "epoch:  37   train_loss: 0.22   test_loss: 5.76\n",
      "epoch:  38   train_loss: 0.22   test_loss: 5.80\n",
      "epoch:  39   train_loss: 0.22   test_loss: 5.83\n",
      "epoch:  40   train_loss: 0.22   test_loss: 5.85\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizd\n",
      "\n",
      "sample T=1: je ne sais pasghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihkjkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodssksauzi\n",
      "\n",
      "\n",
      "epoch:  41   train_loss: 0.21   test_loss: 5.87\n",
      "epoch:  42   train_loss: 0.21   test_loss: 5.90\n",
      "epoch:  43   train_loss: 0.21   test_loss: 5.92\n",
      "epoch:  44   train_loss: 0.21   test_loss: 5.94\n",
      "epoch:  45   train_loss: 0.21   test_loss: 5.95\n",
      "epoch:  46   train_loss: 0.21   test_loss: 5.97\n",
      "epoch:  47   train_loss: 0.21   test_loss: 5.99\n",
      "epoch:  48   train_loss: 0.21   test_loss: 6.00\n",
      "epoch:  49   train_loss: 0.21   test_loss: 6.02\n",
      "epoch:  50   train_loss: 0.21   test_loss: 6.03\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais paszqhsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfk\n",
      "\n",
      "sample T=1: je ne sais pasghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg  hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbqheposdgqndqsjgnùdsoqg  hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpe\n",
      "\n",
      "\n",
      "epoch:  51   train_loss: 0.21   test_loss: 6.04\n",
      "epoch:  52   train_loss: 0.21   test_loss: 6.05\n",
      "epoch:  53   train_loss: 0.21   test_loss: 6.07\n",
      "epoch:  54   train_loss: 0.21   test_loss: 6.08\n",
      "epoch:  55   train_loss: 0.21   test_loss: 6.09\n",
      "epoch:  56   train_loss: 0.21   test_loss: 6.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  57   train_loss: 0.21   test_loss: 6.11\n",
      "epoch:  58   train_loss: 0.21   test_loss: 6.12\n",
      "epoch:  59   train_loss: 0.21   test_loss: 6.13\n",
      "epoch:  60   train_loss: 0.21   test_loss: 6.14\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=1: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqso\n",
      "\n",
      "\n",
      "epoch:  61   train_loss: 0.21   test_loss: 6.15\n",
      "epoch:  62   train_loss: 0.21   test_loss: 6.16\n",
      "epoch:  63   train_loss: 0.21   test_loss: 6.17\n",
      "epoch:  64   train_loss: 0.21   test_loss: 6.18\n",
      "epoch:  65   train_loss: 0.21   test_loss: 6.18\n",
      "epoch:  66   train_loss: 0.21   test_loss: 6.19\n",
      "epoch:  67   train_loss: 0.21   test_loss: 6.20\n",
      "epoch:  68   train_loss: 0.21   test_loss: 6.21\n",
      "epoch:  69   train_loss: 0.21   test_loss: 6.22\n",
      "epoch:  70   train_loss: 0.21   test_loss: 6.23\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizd\n",
      "\n",
      "sample T=1: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n",
      "epoch:  71   train_loss: 0.21   test_loss: 6.23\n",
      "epoch:  72   train_loss: 0.21   test_loss: 6.24\n",
      "epoch:  73   train_loss: 0.21   test_loss: 6.25\n",
      "epoch:  74   train_loss: 0.21   test_loss: 6.26\n",
      "epoch:  75   train_loss: 0.21   test_loss: 6.26\n",
      "epoch:  76   train_loss: 0.21   test_loss: 6.27\n",
      "epoch:  77   train_loss: 0.21   test_loss: 6.28\n",
      "epoch:  78   train_loss: 0.21   test_loss: 6.28\n",
      "epoch:  79   train_loss: 0.20   test_loss: 6.29\n",
      "epoch:  80   train_loss: 0.20   test_loss: 6.30\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=1: je ne sais pasghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp\n",
      "\n",
      "\n",
      "epoch:  81   train_loss: 0.20   test_loss: 6.31\n",
      "epoch:  82   train_loss: 0.20   test_loss: 6.31\n",
      "epoch:  83   train_loss: 0.20   test_loss: 6.32\n",
      "epoch:  84   train_loss: 0.20   test_loss: 6.32\n",
      "epoch:  85   train_loss: 0.20   test_loss: 6.33\n",
      "epoch:  86   train_loss: 0.20   test_loss: 6.34\n",
      "epoch:  87   train_loss: 0.20   test_loss: 6.34\n",
      "epoch:  88   train_loss: 0.20   test_loss: 6.35\n",
      "epoch:  89   train_loss: 0.20   test_loss: 6.36\n",
      "epoch:  90   train_loss: 0.20   test_loss: 6.36\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizd\n",
      "\n",
      "sample T=0.7: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=1: je ne sais paszlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgje\n",
      "\n",
      "\n",
      "epoch:  91   train_loss: 0.20   test_loss: 6.37\n",
      "epoch:  92   train_loss: 0.20   test_loss: 6.37\n",
      "epoch:  93   train_loss: 0.20   test_loss: 6.38\n",
      "epoch:  94   train_loss: 0.20   test_loss: 6.38\n",
      "epoch:  95   train_loss: 0.20   test_loss: 6.39\n",
      "epoch:  96   train_loss: 0.20   test_loss: 6.40\n",
      "epoch:  97   train_loss: 0.20   test_loss: 6.40\n",
      "epoch:  98   train_loss: 0.20   test_loss: 6.41\n",
      "epoch:  99   train_loss: 0.20   test_loss: 6.41\n",
      "epoch: 100   train_loss: 0.20   test_loss: 6.42\n",
      "\n",
      "sample T=0.2: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.5: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "sample T=0.7: je ne sais pasdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizd\n",
      "\n",
      "sample T=1: je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n",
      "CPU times: user 2min 6s, sys: 978 ms, total: 2min 7s\n",
      "Wall time: 49.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Fixed-length RNN': (model1, n_chars),\n",
    "    'Stateless RNN': (model2, bptt2),\n",
    "    'Stateful RNN': (model3, bptt3),\n",
    "    'Small LSTM': (model4, bptt4),\n",
    "    'Large LSTM': (model5, bptt5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 0.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne salmkzdsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnk\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pasgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdsc\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pasdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfoh\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pasjinfkokluokosjgodsjihsjgndqsjidlmgokosdqsdgokodsjihsjgndsjgndgosdqsjgndgodsjgndqsdgosdgokoklusjgndgkosdgoklukosjgndgodsjgndsjgndsjgndsjgodsdgokqsdgodqsjihsjgndsjgndgosjgndqfhsjgodsjgndgoklmgokokoklusj\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n",
      "T = 0.5\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne salmkzdkosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mk\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pasg   dhljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdq\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pasdsepfehnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgs\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pasgndsdgkosdgosdqhsjgnùjgodqsjfeogodsjgnfkzqcjfeosdgokskoqsjindgkorjidlmgsjidlmgkoqksdgokqsduosdjpeinjfeodsjdszqgbjrfkosdqsodsjlmgnùdsjgnfhdqsdgosjsdgokeporjrdklzgoqhsdgodgsjrfeosdlhsjgndkgosjizdqkosjgn\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n",
      "T = 0.7\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne salmkzdsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnk\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pasgndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsks\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pasdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqn\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pasgqbjgnùgndqsjidqnùdsgesodqheogodsjlgnsgsjgeoszhbqdsqsdlmghosjijgnùjszprq hsdgjihsdjingokluzqdscrjihnnkljfeokzqfkosdjidkluzqfhkodgszqdkszdsiuojgzrhghjeùluokosdgoksadzizdqfbjgbqhesogbjgeùzprqfhokjihnkgo\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n",
      "T = 1\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sapzqsdqlhddsosauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqc\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pasgqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg  hdqsoqg   hdqsojp,ùgjeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeeposdgqn\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pasdsepzihqsoqsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizlussrjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoj\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais paskqghsodgokgosjqghsjkezkadgeùgnkvfkodqsjggoghjsnko jgqkoszdqghljqodgs qsklgbdgnfkodjgofkosjrkdsdgbcqcjùljkfhgoqsjihenzqfeolkzvkosqgb,jdjklnrhdqhlhlmkjei hdkqksjiuksjluscfhjsjgnfkklmsqcvfkp,eùzqhngjndnd\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais paszqdjiidfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgn\n",
      "\n",
      "\n",
      "T = 1.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne salcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdl\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pasgbjhzposdgqndqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeizduosijfekodsksauzidqfohsjgbjrfesokqdzdgcnfkl,mùdsùqg   hdqsojp,ùgjeùzporjizduosdgqndqsjgn khdqsojp,ùg\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pasdslusdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnjgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgeùdpoidqfohsjgbjrfesokqdzd\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pasdqszdko,dqhsjgqkorzqcjùsflkdpeohsdiaqnmdsqfshskzdzc,ksdssdsjepefeekoszqcjafkodgszdighfklzidqskduofklusoqsdjldjsùzpojlfshoskqdgodkorklulngokkodqkzpcidpepeikosdngndgqboiqhskosokqdsjgnjifmkzp,zzikqcùsacv\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais paszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsdlmgbqheposdgqndqsjgnùdsoqg   hdqsojp,ùgjeùzporjizduosijfekodsksauzidqfohsjgbjrfesokqdzdscnfkl,mkoszqcjihnkgokjrsdpeinvfkogkedfhgodklusghljkzlhsdjlgsd\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_s = 'je ne sais pas'\n",
    "\n",
    "for temperature in (0.2, 0.5, 0.7, 1, 1.2):\n",
    "    print(f'T = {temperature}')\n",
    "    print()\n",
    "    \n",
    "    for model_name, (model, bptt) in models.items():\n",
    "        \n",
    "        # Handle fixed-size RNN\n",
    "        generate_func = generate_fixed_size if model_name == 'Fixed-length RNN' else generate\n",
    "        s = initial_s[:n_chars] if model_name == 'Fixed-length RNN' else initial_s\n",
    "\n",
    "        print(f'{model_name}:\\n  ' + generate_func(model, s, 200, bptt, temperature))\n",
    "        print()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
