{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "#    txt += f.read()\n",
    "#\n",
    "#len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœ€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 3. / 4\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateless model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (hence *stateless*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([110680, 3])\n",
      "torch.Size([110680])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36892, 3])\n",
      "torch.Size([36892])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        chars = get_data_tensor(s + '   ', n_chars)\n",
    "        preds = model(chars)\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "            \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatelessModel(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars):\n",
    "\n",
    "        hidden = torch.zeros([len(chars), n_hidden])\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden = torch.tanh(self.hidden_weights(input + hidden))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = StatelessModel(n_vocab, n_fac, n_hidden, n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.13\n",
      "test loss: 2.1\n",
      "\n",
      "sample top: je pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas de pas \n",
      "\n",
      "sample multinomial: je lapirs au qui décont uno. Toit paiment, quenfen 4 taet in envommay conquta. Ils et Il heutossouvrandende nont à ques à qu'à cros. La best ave auses hest du pas ceux cu pour est frore de ser es, à leur\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.84\n",
      "test loss: 1.9\n",
      "\n",
      "sample top: je pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de cont pas de c\n",
      "\n",
      "sample multinomial: je noleans paraïtantors, et leage érois, le faRit, astop qu'à mon combe demant pour dires soine gnomponn est pour av'est tours même\" prent est ? Tillées ensis, à reve sévevrai (Touvonfation a que conons \n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.78\n",
      "test loss: 1.86\n",
      "\n",
      "sample top: je sur les cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont \n",
      "\n",
      "sample multinomial: je chalgents, plangée conducter. Il est les pouvent appéron dormir), Ven. Au Sont, viension fais dans infes, ille, et la sans. Il vite espan).  gétait conver. A6, plance qui côte ences. a unes et une vra\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.75\n",
      "test loss: 1.85\n",
      "\n",
      "sample top: je la cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les\n",
      "\n",
      "sample multinomial: je parcoins apuéba, tativoir furne, au coles. Ilieux\", clos) qui pasi ses répon toumagées voyent en est senthèvec que pas pas en (cieku (ous à Vélénes-cens auvez sur les au mois envion, envemplégn je com\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.74\n",
      "test loss: 1.85\n",
      "\n",
      "sample top: je la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la cont de la\n",
      "\n",
      "sample multinomial: je ade faiser lamiforte reve pout toi, sant, laire. Ja dire à une pour la qui. Le petite, tument quoi en c'est une pays, pourn, à sur pour vie (lus blimieur l'et cam eu ques pend unis villeures. Unaire t\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.73\n",
      "test loss: 1.85\n",
      "\n",
      "sample top: je ville pas de la cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons les cons\n",
      "\n",
      "sample multinomial: je vous pours dizo Commençon du Comps qu'ellu froi, ell fernes, cor me me pas qu'ai viens poin, surs affreux, et dont, on de tion, on est avant vu parfois. Ils en fois remicainer este 50 08 Mocanpontent \n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate1(model1, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stateful model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**.\n",
    "\n",
    "The hidden state is memorized from one mini-batch to another time (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 54, 72],\n",
      "        [59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        [62,  0, 11],\n",
      "        [56, 69, 11]])\n",
      "labels:\n",
      "tensor([[59, 67,  0],\n",
      "        [59, 73, 11],\n",
      "        [62,  0, 11],\n",
      "        [56, 69, 11],\n",
      "        [62, 62,  0]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[62, 62,  0],\n",
      "        [58, 72, 30],\n",
      "        [74, 11,  5],\n",
      "        [72,  0, 58],\n",
      "        [58, 32, 72]])\n",
      "labels:\n",
      "tensor([[58, 72, 30],\n",
      "        [74, 11,  5],\n",
      "        [72,  0, 58],\n",
      "        [58, 32, 72],\n",
      "        [66, 67, 73]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    res = s\n",
    "    for _ in range(n):\n",
    "        data = get_data(s, 1)\n",
    "        preds = model(data)[-1]\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        res += pred_char\n",
    "        s = s[1:] + pred_char\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatefulModel(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, data):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_weights)\n",
    "        self.hidden_weights = Variable(h.data)\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_weights = torch.zeros([1, bs, n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = StatefulModel(n_vocab, n_fac, n_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.29\n",
      "test loss: 2.02\n",
      "\n",
      "sample top: je pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas \n",
      "\n",
      "sample multinomial: je pomma à lour, innments. on jout prens las. M'enlaiment de ca pasteles aus, tur. Jaut danter nlautes 300 goz mon mars tralenttain a yu pas. Il on (avain pon quiiz. Jen etreaper, bêmand ? s'y bagces aux\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.63\n",
      "test loss: 1.73\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je pascile. Etre à melle. Imde lanchus, jusie. Le bienceile (ne élater sacteures de en (chément 4 hamere, en se \"Tture quant sy dant\". \"3 haud je mure-cille avecey) Il payos : buteure-t mardo atce efforo\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.58\n",
      "test loss: 1.71\n",
      "\n",
      "sample top: je traine qui l'acces pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas p\n",
      "\n",
      "sample multinomial: je ceaux d'être dant plaire.  De pas haup endain douctience (gazarse dir pas. Il (sonne, me expoppant, jevait dés elle dir ma décl. Je de dis, Suis pett stamain exanchange. Entrire, pas, du bient sant à \n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.57\n",
      "test loss: 1.7\n",
      "\n",
      "sample top: je restique exper l'ache dis pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pa\n",
      "\n",
      "sample multinomial: je che en defites-vaye aus, parque. Il pas asso passe toudis détailoide queplos. Je tectible flite me paser lui ahe. Dacun du incime.\" de supson erroinss va pladeux du où payer revoines pluis, que, pends\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.7\n",
      "\n",
      "sample top: je quis pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas\n",
      "\n",
      "sample multinomial: je rempte pas, comp mirquenge à rapeurant ditais 600 kcher moice pourque quasiant sant pration butairectionchewer, cendétiques de - Panaraisser se (tants douc saze ?\". Acariel lus a pluer à côte expir cr\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.7\n",
      "\n",
      "sample top: je rempler la (marit dir pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pa\n",
      "\n",
      "sample multinomial: je pasé ded ce à d'icre semptaire bient In parse dis. La où chage tron. Jusemmern (c'imparse, payer le froi. Lande. Lars. Oua bient suceuration. Lant dir à 200 kila, leu pas). Nètement a l'quahe. Jus (co\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model2.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model2(data)\n",
    "        optimizer2.zero_grad()\n",
    "        loss = criterion2(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion2(model2(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate2(model2, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "rnn"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
