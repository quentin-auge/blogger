{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#txt = 'portez ce vieux whisky au juge blond qui fume. ' * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 4. / 5\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*).\n",
    "\n",
    "![](img/rnn_fixed.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    if GPU:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    if GPU:\n",
    "        labels_tensor = labels_tensor.cuda()\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([321668, 8])\n",
      "torch.Size([321668])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80416, 8])\n",
      "torch.Size([80416])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_size(model, s, n, n_chars, temperature):\n",
    "\n",
    "    # fixed-size input\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        # Pad the input, because `get_data_tensor` will generate no data\n",
    "        # if the input is less than `2 * n_chars` characters long.\n",
    "        chars = get_data_tensor(s + ' ' * n_chars, n_chars)\n",
    "        preds = model(chars, temperature)\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()    \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_chars = n_chars\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars, temperature=1):\n",
    "\n",
    "        # Reset hidden state at each mini-batch\n",
    "        hidden_state = torch.zeros([len(chars), self.n_hidden])\n",
    "        if GPU:\n",
    "            hidden_state = hidden_state.cuda()\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden_state = torch.tanh(self.hidden_weights(input + hidden_state))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden_state) / temperature, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)\n",
    "if GPU:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.23   test_loss: 2.05\n",
      "\n",
      "sample T=0.2: je ne sans de les partient de les de coure de les res de partion de la partiter de la mais de le partier de la part de la partiter de les partions de rement de le part de les le coure de la mais de le ce pour\n",
      "\n",
      "sample T=0.5: je ne sans crant de vare de nour ce preura au de rent de pour cour sinite couparis de pour de Steur ce parmient et de ville de res ou mille de la pour de serais de pret dé les dé est de conde de parté acce co\n",
      "\n",
      "sample T=0.7: je ne sans vouse villinité en une de na rans parter a 60 les réfinier à le des seraite cuite de saine chait de le parle reux et en en bayager les cour les nour ine de ret de Nou pour prid cinite êmiziet mais \n",
      "\n",
      "sample T=1: je ne sans la fais moillont chimille). crèsier à maricèt=pleupousse pitraine gridéemin Vorrai sient en en paur il e sur pliés bant que  Jes cuter avoute un plus langtes détreux cuatiter durier lament estiel s\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.99   test_loss: 1.96\n",
      "epoch:   3   train_loss: 1.93   test_loss: 1.92\n",
      "epoch:   4   train_loss: 1.90   test_loss: 1.89\n",
      "epoch:   5   train_loss: 1.87   test_loss: 1.87\n",
      "epoch:   6   train_loss: 1.85   test_loss: 1.85\n",
      "epoch:   7   train_loss: 1.85   test_loss: 1.85\n",
      "epoch:   8   train_loss: 1.84   test_loss: 1.84\n",
      "epoch:   9   train_loss: 1.83   test_loss: 1.84\n",
      "epoch:  10   train_loss: 1.83   test_loss: 1.84\n",
      "\n",
      "sample T=0.2: je ne sans de la mais de la cartant de la part de la contine de la contine de la contine de la restauce de la restauire de ces part les pour de la reste des part de la contine sur les part de la reste en car \n",
      "\n",
      "sample T=0.5: je ne sac de l'arriver les deparde je suis inférie coures de velles des rier pour à la res pas les pardés les chances de la solinais pour ser dis la sans de ce par au mais de fin sur le mais et se des cartale\n",
      "\n",
      "sample T=0.7: je ne sans les armes a fini et du du le ciens delmant à prendre ne pas indérielles le préréhiens à au lifé de cont sur de les tres sur des part tourise est sur de pas sans inté le la poupé en cadier des beli \n",
      "\n",
      "sample T=1: je ne sais des rérits de trassager de Pleux me pour sur à les sois mêmes les parcé). Je le dollys des Parglement riigation pour les cout.w où se récité la plod ou mais diffice dans l'éler éthandérégnais du pe\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  12   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:  13   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  14   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:  15   train_loss: 1.81   test_loss: 1.83\n",
      "epoch:  16   train_loss: 1.81   test_loss: 1.84\n",
      "epoch:  17   train_loss: 1.82   test_loss: 1.82\n",
      "epoch:  18   train_loss: 1.81   test_loss: 1.83\n",
      "epoch:  19   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:  20   train_loss: 1.81   test_loss: 1.83\n",
      "\n",
      "sample T=0.2: je ne sans de contre de la se plus les par les pour les conces de la consimples de l'au mais et de la contine de contre de l'au ce suis de la pas les conces de la part part de la conce de constant de challe d\n",
      "\n",
      "sample T=0.5: je ne sans ici pas les parts argent de se pas les pour les contilles à ce mais en plus de chalat de pour les car seule de cample arrive des grand au contre et de seillement plus de les Australis pour trouver \n",
      "\n",
      "sample T=0.7: je ne saufferre qui pans encoré ou tages sac l'ont tend seul des centiais des car les les conscilul de l'ont continailler un volle nous servit d'habitée ne la minures serent d'assielle aide étant des sanse su\n",
      "\n",
      "sample T=1: je ne sanges \" sens mon jes peisir où ceur dans le pagriers. apréscrirer de t'a conce. Dand pour le faires avec arrises revance pas aisons en duprences.  Lissi qaranes indé à quitailai des grâvera pens et il \n",
      "\n",
      "\n",
      "CPU times: user 1min 40s, sys: 1.4 s, total: 1min 41s\n",
      "Wall time: 1min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    test_loss = test_loss_sum / test_batches_nb\n",
    "        \n",
    "    print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "\n",
    "        for temperature in (0.2, 0.5, 0.7, 1):\n",
    "            print(f'sample T={temperature}: ' + generate_fixed_size(model1, 'je ne sais pas'[:n_chars], 200, n_chars, temperature))\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**.\n",
    "\n",
    "![](img/rnn_variable.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_state)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch, but not its history\n",
    "            self.hidden_state = Variable(h)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_state = torch.zeros([1, bs, self.n_hidden])\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')\n",
    "if GPU:\n",
    "    model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.10   test_loss: 1.91\n",
      "\n",
      "sample T=0.2: je ne sais pas avoir de marche de la pas de mon pas de la par le route de la contre de la pas de la contre en mais de la contre de la couvers pas de la par les route de la par le par le conne se la contrant de par \n",
      "\n",
      "sample T=0.5: je ne sais pas propies et pas des plus en comme mon au mur la restant les comment la ruit villes de la pas pas et du ville par la faire se le bons par la voillec nous contre de rentre a régrant dans la contre de de\n",
      "\n",
      "sample T=0.7: je ne sais pas dans les pour au s'est les comme que je nous cette jours partures en cettente avec la chation à la rendut plus par la cau, le ma parts de reste de chafin mon poind le romir moins la Mon probe de par \n",
      "\n",
      "sample T=1: je ne sais pas d'un cetteils en en allomés mévendre justrer en perfforde attenie, la vallente de trauvellez de vermant verse peelumicuanneys peussons la Biceux exilter des fine son aucompheau.) cortrisie au mat tom\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.88   test_loss: 1.85\n",
      "epoch:   3   train_loss: 1.84   test_loss: 1.83\n",
      "epoch:   4   train_loss: 1.82   test_loss: 1.82\n",
      "epoch:   5   train_loss: 1.81   test_loss: 1.81\n",
      "epoch:   6   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   7   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   8   train_loss: 1.79   test_loss: 1.80\n",
      "epoch:   9   train_loss: 1.79   test_loss: 1.80\n",
      "epoch:  10   train_loss: 1.79   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas pas pas de la repas en avec les complète de la connais de la peut partien de mon au mon au mon avec les pays de la route de la rue de la soleil et les partions de la roule de la manger de la route de\n",
      "\n",
      "sample T=0.5: je ne sais pas par les petit mon avec ma dans le par le centaines de la fait de partie que je n'ai pas par de la proprisses et de la rempar la mette dormir des parler comme mais connaissant de me pas pour partien d\n",
      "\n",
      "sample T=0.7: je ne sais pas et et mais la voyagera et en 2 proprieux d'une ville parfois de caffre le bus par les de l'avonna dont seul mer dans le bord mon ce marche, avec listacme sommes ce sortes et et de l'Argentiers de mon\n",
      "\n",
      "sample T=1: je ne sais pas le termporté de miensyons la vallon dans la balere Mois froudre et dormaux dans un une problète en sur la troute reple vision, nous Mestier d'endroijets. Ilsssibles (paurs les reademagno un espets) J\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.79   test_loss: 1.79\n",
      "epoch:  12   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  13   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  14   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  15   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  16   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  17   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  18   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  19   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  20   train_loss: 1.78   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas de sont pas particle de plus de mon au mon avons de la plage de la partie de la route de la cours de la partie de la reprendre de la retrouver les partie de la reprendre de la permette de propose de \n",
      "\n",
      "sample T=0.5: je ne sais pas de visiter par le car le partie de la chante apprendre la chance de l'autre en suis de mon construit de prophtutions le fait un peut pas de l'autre de plus ou de la cuisine de par les partie de pas d\n",
      "\n",
      "sample T=0.7: je ne sais pas couper pour se marche en arriver nous ne suranivais il n'a décides. Il sauss plus et le vers les nord en bus mal à de renconstableurs quelques pays au ce milie, à la finalites au NEu mais nous soit p\n",
      "\n",
      "sample T=1: je ne sais passant et qui plutôme. Je me mettena aveau ont des  faucoup, gros passent le malage, et tostorter de l'internet qui sybs semble permettr proghorents. Bière de faut passes les Eume et (déjà que je nous f\n",
      "\n",
      "\n",
      "CPU times: user 18.7 s, sys: 2.44 s, total: 21.1 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model2, optimizer2, criterion2, bptt2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt3 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')\n",
    "if GPU:\n",
    "    model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.98   test_loss: 1.76\n",
      "\n",
      "sample T=0.2: je ne sais pas de la route de la ville de la pour le sent de contre de la ville de la par les pour le semps et le sont par le par la contre de la sont pour de la vie de la faire de la cons pas de la partie de la re\n",
      "\n",
      "sample T=0.5: je ne sais pas pas de trouce pour les pour peux de mon passant, et jours de la ravant qui ville et se visiment cons peut pour sont pas a me dont que le marche avec le mon pres pas de coune de peut pour de mon pays \n",
      "\n",
      "sample T=0.7: je ne sais pas le Vent de nors de qui aurance en procourobin de pronoter de c'est pour pour des sont de me perment une vender du habite et sans dictionne à proppalie de la mois de cons pas sur nors pour le-commenco\n",
      "\n",
      "sample T=1: je ne sais pas sur un table, quans en Cho, je côtent voyre Vous à nous qu'énale de me morame du pors ochefosi dans fous sanser de côtéments. Ilement, et les bouri muis pembo miment sur les goup la crèsons me flus q\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.71   test_loss: 1.67\n",
      "epoch:   3   train_loss: 1.65   test_loss: 1.64\n",
      "epoch:   4   train_loss: 1.62   test_loss: 1.62\n",
      "epoch:   5   train_loss: 1.61   test_loss: 1.61\n",
      "epoch:   6   train_loss: 1.59   test_loss: 1.60\n",
      "epoch:   7   train_loss: 1.59   test_loss: 1.59\n",
      "epoch:   8   train_loss: 1.58   test_loss: 1.59\n",
      "epoch:   9   train_loss: 1.58   test_loss: 1.58\n",
      "epoch:  10   train_loss: 1.57   test_loss: 1.58\n",
      "\n",
      "sample T=0.2: je ne sais pas par le pays et d'arriver le pays et de mon peut de la plus de le pays et le contrôle de la plus le plus de la rencontres par le voyage de la conducte de la petit de la couple de la contre que le pays\n",
      "\n",
      "sample T=0.5: je ne sais pas de mon partie de la nuit de notre par mais car le monde et par les travaile en semain, je serait ou d'un peut lors au mois en propres, un peu de moins de le soleiller à pendant les maison par les pre\n",
      "\n",
      "sample T=0.7: je ne sais pas internes sur les présents par le aucilles de la petit que je notres à l'accordé, le rependant le moins accompagne de trouver le lendement la voyager montag Marrant préférer mon aller de me par le pay\n",
      "\n",
      "sample T=1: je ne sais pas pour une aps fil improgrit, pas de le pays et il déjeur nous accud à la monde ou repas. . . Je motche dans le guiche et Guis transporte en vélos et visiser pas la côté s'ables égalles de 8hk-Mod dire\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:  12   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:  13   train_loss: 1.57   test_loss: 1.57\n",
      "epoch:  14   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  15   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  16   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  17   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  18   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  19   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  20   train_loss: 1.56   test_loss: 1.57\n",
      "\n",
      "sample T=0.2: je ne sais pas au pouvoir pour le transportant de la contraite de la proprement de moins par le cours de propres de la contraite de la contraité de la propres de la plus par la concette et de la plus par les partie\n",
      "\n",
      "sample T=0.5: je ne sais pas me de connaire de monter des petit par par des finalement pas de minussitir la compagnons dernières de tout le régales par un refusions de la conductions de déposer de mon sac d'accate de l'Airure ro\n",
      "\n",
      "sample T=0.7: je ne sais pas poser par la nuit, il m'asses sous le française dans les métrougle alors passent tout le Sanite de l'arrière sans mon se des soire panse un coupler les 44 ans de monde de l'arrées l'étais moins plus \n",
      "\n",
      "sample T=1: je ne sais pas en magnos au moins à quelques irmons en ecart hongaltitu des moins du lidande. Noyage et notre balimesse entosse d'accès Ofaires : Ohat\" de douvre chauder-vient pourtis solgut, nuitant, etc. Je m'ent\n",
      "\n",
      "\n",
      "CPU times: user 18.5 s, sys: 2.64 s, total: 21.2 s\n",
      "Wall time: 21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model3, optimizer3, criterion3, bptt3, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "![](img/lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_fac, n_hidden):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fac = n_fac\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.input_input_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_input_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_forget_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_forget_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_cell_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_cell_weights = nn.Linear(n_hidden, n_hidden)\n",
    "\n",
    "        self.input_hidden_weights = nn.Linear(n_fac, n_hidden)\n",
    "        self.hidden_hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "    def forward(self, x, h, c):\n",
    "        \"\"\"\n",
    "        `x` (input) is of size `bs * n_fac`\n",
    "        `h` (hidden state) and `c` (cell state) are of size `bs * n_hidden`\n",
    "        \"\"\"\n",
    "\n",
    "        # Forget relevant bits of the cell state\n",
    "        forget_state = torch.sigmoid(self.input_forget_weights(x) + self.hidden_forget_weights(h))\n",
    "        c *= forget_state\n",
    "        \n",
    "        # Update relevant bits of the cell state\n",
    "        input_state = torch.sigmoid(self.input_input_weights(x) + self.hidden_input_weights(h))\n",
    "        cell_update_state = torch.tanh(self.input_cell_weights(x) + self.hidden_cell_weights(h))\n",
    "        c += input_state * cell_update_state\n",
    "        \n",
    "        # Forget relevant bits of the hidden state with the cell state\n",
    "        hidden_update_state = self.input_hidden_weights(x) + self.hidden_hidden_weights(h)\n",
    "        h = torch.tanh(c) * torch.sigmoid(hidden_update_state)\n",
    "        \n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_cell = LSTMCell(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        hidden_state_history = []\n",
    "        # RNN loop on `input` of size: `bptt * bs * n_fac`:\n",
    "        # bptt times for each `x` of size `bs * n_fac`\n",
    "        for x in input:\n",
    "            hidden_state, cell_state = self.lstm_cell(x, hidden_state, cell_state)\n",
    "            hidden_state_history.append(hidden_state)\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(torch.stack(hidden_state_history))\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt4 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LSTM(n_vocab, n_fac, n_hidden)\n",
    "if GPU:\n",
    "    model4 = model4.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = torch.optim.Adam(model4.parameters(), 1e-2)\n",
    "criterion4 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.94   test_loss: 1.66\n",
      "\n",
      "sample T=0.2: je ne sais pas de la conner de la ville de la conne de la profite de la constraliens par les autre de la fait le part de la rue de la consait de la plus de la contre de la piens par les par le plus de la proche de \n",
      "\n",
      "sample T=0.5: je ne sais pas de suinde de la ville par les plus par le mon a plus en plande de l'artera peu de l'aime, mon couper ce de la celle de vous fait par le pas de parc. Après pas de la trance avec une passé de l'Husion \n",
      "\n",
      "sample T=0.7: je ne sais pas couple pas que je cessé de la chaup de l'accomps de la grantiers que nas pour les passer la nuit. Nous avec le suptais le travon renconstradion n'ai par un trop me fenint de même en autres profier ti\n",
      "\n",
      "sample T=1: je ne sais pas d'allais ses sa plus, ou sudne qun mon noit-cament heurep 1°, je tar à trai lignement imment, accelsir en stop en centainex nous aurent le reversé pleur est entrez Shos d'aistrage à boit, duvet touve\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.60   test_loss: 1.55\n",
      "epoch:   3   train_loss: 1.52   test_loss: 1.51\n",
      "epoch:   4   train_loss: 1.48   test_loss: 1.48\n",
      "epoch:   5   train_loss: 1.46   test_loss: 1.46\n",
      "epoch:   6   train_loss: 1.44   test_loss: 1.45\n",
      "epoch:   7   train_loss: 1.43   test_loss: 1.44\n",
      "epoch:   8   train_loss: 1.41   test_loss: 1.43\n",
      "epoch:   9   train_loss: 1.41   test_loss: 1.43\n",
      "epoch:  10   train_loss: 1.40   test_loss: 1.42\n",
      "\n",
      "sample T=0.2: je ne sais pas de la coupe de la connaissants de la plupartis de la ville de la route de la connaissants de la plus tard de conséque de la connaissaniens de la ville de la frontière sur le camping de la ville de la\n",
      "\n",
      "sample T=0.5: je ne sais pas le camportes passer les passé de me sont pas découvertes de la ville de la maison en voyageurs : le monde en a dormir de la ville de la programe d'abord par son plus par les récos de la magnifique si\n",
      "\n",
      "sample T=0.7: je ne sais pas par le voyageurs d'entre j'abative en Australie en voyageur couchir de poir le soleisent dizaines. Les village de dormir tous les soleil de seul familles on aventure un peu de consommation en compagn\n",
      "\n",
      "sample T=1: je ne sais pas apporte ja portu. En arrive de découvriules fut climentaire c'est en voyageurs de satible sur les épais nous lors de Népud, détrouvaies bien semble par en Asie, et semacantulas en millieus juste en r\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.39   test_loss: 1.42\n",
      "epoch:  12   train_loss: 1.39   test_loss: 1.41\n",
      "epoch:  13   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  14   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  15   train_loss: 1.38   test_loss: 1.41\n",
      "epoch:  16   train_loss: 1.37   test_loss: 1.41\n",
      "epoch:  17   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  18   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  19   train_loss: 1.37   test_loss: 1.40\n",
      "epoch:  20   train_loss: 1.36   test_loss: 1.40\n",
      "\n",
      "sample T=0.2: je ne sais pas de la pluie de la fois de la planer le pays nous avoir avec le point de la pluie de la fois de la pluie de la pluie de la pluie de sans passé de la pluie de la pluparaises de la pluie de la compagnie\n",
      "\n",
      "sample T=0.5: je ne sais passé de la ville de mon pense pour le moins plus en stop de piedent sur le conducte au long de la route de la mer rizie. Le point de camardant au moins de passe de parler de partir de la ville avec le c\n",
      "\n",
      "sample T=0.7: je ne sais pas de connaissants de lumede coute de cette au lits dans le rejoindre rejoindre indioctés de sable au connaissaniennes de la ma femme ! En espida, arriver le moins de partir en coup de cabane se repas à\n",
      "\n",
      "sample T=1: je ne sais pas du pauture si le vie n'avait envie de litoori, comme grasse de prendre pour dors yea en sait, elle, prévéhent bien simple. Quelques sortir dormirser la voyageure (allare d'équitops de faire une vie e\n",
      "\n",
      "\n",
      "CPU times: user 1min 21s, sys: 800 ms, total: 1min 22s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model4, optimizer4, criterion4, bptt4, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop reinventing the wheel for once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, n_layers, dropout=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, n_layers, dropout)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 512\n",
    "bs = 1024\n",
    "bptt5 = 300\n",
    "n_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = PytorchLSTM(n_vocab, n_fac, n_hidden, n_layers, dropout)\n",
    "if GPU:\n",
    "    model5 = model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer5 = torch.optim.Adam(model5.parameters(), 1e-2)\n",
    "criterion5 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 3.49   test_loss: 3.05\n",
      "\n",
      "sample T=0.2: je ne sais pas   et  ee e    ee  eue   e    e e   eu  e  ee   e    e  ee       e es    e ee  es    eu  er  ee  e e    ee  ee  e   en e  e      ee e  e et  ee  ee  eu  ee   e es   e    e    e  e  ee     es  eu ee  e\n",
      "\n",
      "sample T=0.5: je ne sais pasre tai' es ee  eueue   eau ne   eu auorn et   uin si   n toe tte   et eotur ltaere tne  sf ttee rntttnn  p eros uutenltu   opi.n  re   ,so tsssu mseo e.anruet  no et  enueseo  te uoueu     t ureerue' \n",
      "\n",
      "sample T=0.7: je ne sais pasuu luer teaes etTlte :nuf el soaoiseri ttumu.e runC 2tareea he tsrsoés pe.x rsdsgsn nnssniuenx  s  rnersm lt o.  erneeertussnjtc .oe.ees e'u rnasute e nrl rt bo 2r ee .u  on ns vn eeud jnn jue l a e :\n",
      "\n",
      "sample T=1: je ne sais pasno èItoe wtonms  rt serslLounpen'prstté f uelepeem steubsrPu.mdt see gml -auhicuuit2lsà'0urur  t J! kernse zme sda xotWxetr4s3o 4ilPdutvéitmt e Cuà0uD iuooutcu -eaeneolmfdets v e,a \"  usimt a,re sp nn\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 2.87   test_loss: 2.65\n",
      "epoch:   3   train_loss: 2.57   test_loss: 2.43\n",
      "epoch:   4   train_loss: 2.35   test_loss: 2.24\n",
      "epoch:   5   train_loss: 2.17   test_loss: 2.07\n",
      "epoch:   6   train_loss: 2.02   test_loss: 1.93\n",
      "epoch:   7   train_loss: 1.88   test_loss: 1.80\n",
      "epoch:   8   train_loss: 1.76   test_loss: 1.69\n",
      "epoch:   9   train_loss: 1.67   test_loss: 1.61\n",
      "epoch:  10   train_loss: 1.57   test_loss: 1.53\n",
      "\n",
      "sample T=0.2: je ne sais pas de la problème de la part de la plus de passer le mais en voiture de la peut pas de la pays de l'autre pas de la passer le propre de l'autre des autostop et de la peut pas de la peut pas de la part d\n",
      "\n",
      "sample T=0.5: je ne sais pas instéparer de construite et des conducteurs passer le serait pas le montagne et le maison de la reste des pays en sur le pris de passer le voyage de véhicule de soit en passie passer la rancoire de m\n",
      "\n",
      "sample T=0.7: je ne sais pas tout nous comment ce qui tenter le place de sur le taler tous les froit pour bien des ésibles sur le tuit que la rentrerai dans le concert à comp pour se faire tente. Nous passates par les adberger q\n",
      "\n",
      "sample T=1: je ne sais pas plus argent, l'agricier. Il m'iissant au penger leur avec gropes et la cors à batande. Je saussisme du Jouaraiche du vélée dans la œuis, m'a jouer à l'électrer sere 21 foute trop aux r'une centrerai \n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.50   test_loss: 1.49\n",
      "epoch:  12   train_loss: 1.44   test_loss: 1.42\n",
      "epoch:  13   train_loss: 1.38   test_loss: 1.37\n",
      "epoch:  14   train_loss: 1.34   test_loss: 1.35\n",
      "epoch:  15   train_loss: 1.31   test_loss: 1.31\n",
      "epoch:  16   train_loss: 1.27   test_loss: 1.28\n",
      "epoch:  17   train_loss: 1.24   test_loss: 1.30\n",
      "epoch:  18   train_loss: 1.24   test_loss: 1.26\n",
      "epoch:  19   train_loss: 1.20   test_loss: 1.24\n",
      "epoch:  20   train_loss: 1.18   test_loss: 1.23\n",
      "\n",
      "sample T=0.2: je ne sais pas de montagne de consulat de la pluie de consulat de la route en compagnie de la compagnie de la population de l'autostop en compagnie de la course de la ville de la pluie et de la connaissance de la c\n",
      "\n",
      "sample T=0.5: je ne sais pas plus de pain à l'autre des conducteurs de la main et seulement avec des paysages de place d'avoir de la compagnie de l'Argentine. C'est la capitale de l'autre avant de la chance de la pluie de pouvoi\n",
      "\n",
      "sample T=0.7: je ne sais pas en faille de  China et le Moins Bouladel et qui se va voir les plus, soit d'avoir l'autorisation de Kamusi on se semble de l'auto-stop, tout de l'Inde spétation de gabe et d'autres fraicheurs avant d\n",
      "\n",
      "sample T=1: je ne sais pas en exclimatique pour un réparation. Après y eu le lisible d'apprentissage à font correci. C'est donné quel jour rentrer consulat. . . S-LoL Visi Ban, la suite de la fin d'aller fut trop marétera 2/10\n",
      "\n",
      "\n",
      "CPU times: user 2min 3s, sys: 44.4 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.17   test_loss: 1.22\n",
      "\n",
      "sample T=0.2: je ne sais pas de partir de la construction de la construction de la pluie de la construction de la capitale de la partie de la compagnie de la course de la capitale de la plage de la coupe de la pluie pour le pays\n",
      "\n",
      "sample T=0.5: je ne sais pas de marche dans le forme de l'hospitalité 100000 Riogre. De la pluie en rencontre de la campagne de marche et la vie en plein matin et je rencontrerai un pays où il est de la plupart de la première vi\n",
      "\n",
      "sample T=0.7: je ne sais pas facile de cette avertie de l'orage de Vanimo. Le poids de nous avait discuter à Biocto (Australie et d'abriter de la campagne de mètres de situation, il m'a pas remplacé par le fille de faciliter de \n",
      "\n",
      "sample T=1: je ne sais pas. Ils : prenant 2 ans... Il curte, égognant, mais ma vie de m'est que la langue américain où il est artité pour le (norn contre Sal Hédana. Bien visiter l'accueil et seulement le maroc, chirra 3 heure\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.15   test_loss: 1.22\n",
      "epoch:   3   train_loss: 1.14   test_loss: 1.21\n",
      "epoch:   4   train_loss: 1.13   test_loss: 1.20\n",
      "epoch:   5   train_loss: 1.12   test_loss: 1.19\n",
      "epoch:   6   train_loss: 1.10   test_loss: 1.18\n",
      "epoch:   7   train_loss: 1.09   test_loss: 1.18\n",
      "epoch:   8   train_loss: 1.08   test_loss: 1.18\n",
      "epoch:   9   train_loss: 1.07   test_loss: 1.19\n",
      "epoch:  10   train_loss: 1.07   test_loss: 1.17\n",
      "\n",
      "sample T=0.2: je ne sais pas de mon passeport en Australie et le coup de la plage et de la police et le coucher de l'autoroute que j'ai déjà passé les prochaines décentes de marche de la pluie pour la première fois de la place d\n",
      "\n",
      "sample T=0.5: je ne sais pas de calme. Je suis en passant le concept des villages de l'Europe et la police est très bien faire du stop en direction de l'ambiance de l'Australie ? . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "sample T=0.7: je ne sais pas très tres paté pour nous avoir passer la mes terrain de marche de mon marché, et rapprochement dans le trajet à l'eau pour reprendre l'expérience de la murs de poules ! En retour à m'avait dire que c\n",
      "\n",
      "sample T=1: je ne sais pas de bus pour excrupier 10 heures à traverser la journée sinon étant pour rejergro juste... Comme nous avons réveillé quelques \"En Nouvelle Zélandage\", viasi il faut être profiter du monde à la fête la\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.06   test_loss: 1.18\n",
      "epoch:  12   train_loss: 1.05   test_loss: 1.17\n",
      "epoch:  13   train_loss: 1.04   test_loss: 1.17\n",
      "epoch:  14   train_loss: 1.04   test_loss: 1.18\n",
      "epoch:  15   train_loss: 1.04   test_loss: 1.17\n",
      "epoch:  16   train_loss: 1.03   test_loss: 1.17\n",
      "epoch:  17   train_loss: 1.02   test_loss: 1.16\n",
      "epoch:  18   train_loss: 1.01   test_loss: 1.17\n",
      "epoch:  19   train_loss: 1.00   test_loss: 1.17\n",
      "epoch:  20   train_loss: 0.99   test_loss: 1.17\n",
      "\n",
      "sample T=0.2: je ne sais pas de problème et les plantes de la plage, et je suis sur le prochain article... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "\n",
      "sample T=0.5: je ne sais pas encore avec le bateau au bord de la route de la plage et les meilleurs conforts avec un village de la région de la langue. C'est donc le chemin dans les problèmes d'eau de la ville de Tarija et la pr\n",
      "\n",
      "sample T=0.7: je ne sais pas encore plus, pour me faire découvert mon sac-à-dos en essayant de mon esprit par le point de voiture je pourrai plus vendre des excurées de la plage. Nous dormirons elle doit moi je ne souhaite pas p\n",
      "\n",
      "sample T=1: je ne sais pas au pays très itinérairement de base ici, un passage pour nous vient vienner. C'était déjà arrivé ( pour projet pour construire les éclairages \" de sames (locaux), nous voulions avec les 20 minutes, e\n",
      "\n",
      "\n",
      "CPU times: user 2min 4s, sys: 43.4 s, total: 2min 47s\n",
      "Wall time: 2min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Fixed-length RNN': (model1, n_chars),\n",
    "    'Stateless RNN': (model2, bptt2),\n",
    "    'Stateful RNN': (model3, bptt3),\n",
    "    'Small LSTM': (model4, bptt4),\n",
    "    'Large LSTM': (model5, bptt5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 0.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans de la cartes de la contre de par les cample de la mais pour les par la rencontres de la mais de la mais de cample de l'au par les par les par le par la contrais les part de la contre de l'au contre\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas de la retrouver les petit par le mon au minutes de la rencontrer de la rencontrer de la partie de la plus de la ville de la planges de la chance de la route de la personnes de la rencontrer le partir\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas pour par les plus par les plus pour les contraite de la compagnir les problème de la couple de la ville de la plus par le souette de la plus de mon peut de la connaisse de la proposer de la petit de \n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de la connaissant des passer les petites de la plage de particule de la problème de la maison de la plage de la plage de la pluparationnelles de la plage de la policier de la pluie de profiter le sol\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas encore plus de changer de la population en compagnie de mon argent. Cependant les pays ont déjà voyager avec les articles et les prix et les villes de la ville et de la plage et de l'autostop en Aust\n",
      "\n",
      "\n",
      "T = 0.5\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans en mines des par que repusique de me parlent de rencaires de le win, centire de la serai de sur la Gabiliement de suis aille qui seret de pour les contiant de car marches des cette arriser des conf\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas en bien chance pas de la village de ne suis la journée de moins de plus me me deux c'est partir de voyager les miniche de la dépons pas de partie d'un retrouit de mon a la chaque avons propossible et\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas pour les grand et sa matiller de l'autor de rendre de la région de rester les rencontre de refuntes en vais pour un peu par un peu de sans la problèmes en Europe propres avec de la villes en conseill\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de stop pour constructures et la maison. Ce compagniens les Papous de partité de toute la pluparations pour beaucoup de passe pas dans la forêt et de la parc sur la première à traversé et sans donc d\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas a dormir. Le soleil se trouve Carbon car il semble que ce n'est qu'avec moi qui m'a été les problèmes mais le prix de la place dans un bar et les adresses sont trop chers et de problèmes de coupe de \n",
      "\n",
      "\n",
      "T = 0.7\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sa mêmer connant conné procalile qui visaire, jains denuds mais des les fains des beus pas ce partaire des coupuis est au ce suivent pourie pas les dans une candes conces.. Suis camilles, les ou siocole\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas fraiterre les par il sera avec un belle les monde de cart dévers aussi de mon avons soletit en pas apport peut cet squine de chance de la petit son apport les éminant que du peu pas dire sans de musi\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais passé mais le paysier la fait à Mode serois du propre quelques changer rendre et pour les fois par des promement pour camphaltré plus les europement à 500 km de misisté partager de touriste en plus la ca\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas dans la migoliser la prochaine de barration de la forêt. Pas pour ce senou par la plupagoucres du moins au pays. Il a découvrir un lirée avec les petit pendant qu'en Francais et de soleiles de reste \n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas ton ferry à leur voisine de bus pour envie de rejoindre le groupe de délicates au Chili et commencent à des années que je ne bais faire. Je me tarde pas à vivre dans le désert ou à la mer de la profo\n",
      "\n",
      "\n",
      "T = 1\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sac de nar de me journée, il Dans la seurs de nuysura de V ortiques quelques pas. Chaïlie dans pas la ces bangle). : senna un confortai, unillage de touches. Torace deques de rivire désitellezant.. Darw\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas nie il tous les ghait sont bate. J'ai pângérent un écusse sud ne nous soit aussi ça ne finie finet effe. Les artait un -incont. Super aussi des ule de bitent d'autostop Vert. Après un étaîtrer hôteur\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas par nons confiancheur c'estovitatins, mais séchochant. Les sorterie dans la dormir les plus au toutre seulements dépenstongé ou par les porte au pribilté : Valléter (qu'il alaments de voirances. Le m\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de matin suiller couper quand guenis désert d'indiant et me faire que la prendre le premine qu'ils néganaisais sacs pendant ensuite de chéeriens rue riche dans l'aujoures et accordéon de Laira un peu\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas encore dans un coup de temps une seule rencontre sur la rue car Min. Autre intention de l UNESCari Hoibl ou Sorarir en venue : \"D'accompés sur le bateau qui ressemble que le poisson Temedi - Népal a \n",
      "\n",
      "\n",
      "T = 1.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans. C'essé ww part tors bour est à la reile par Bmême de renvit leur cVers àant à leur pour cat ferd, alls est un flusir qu'Ulitépagagant dans 3 Dangnagord il de suis à l'ausque stop. En cult des curd\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais passement quel parce beauvais, il y sectenments crème proficillent matin (squide noue. Je voule noscriver a chambre ads. On épèden du sinfin le sans le beaucoup ligiée et maintalet en naté déjeau présaAu\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas au fère famille fégaires. Un ce aussi-sémmendroz, à jour donc trop (c'est 2 villé étateur de taresthants/moi envables, ec-R Jéni en ta remenguablent à véllanasé à 134 - D Ville parte, où joure parled\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de plus Monipleré d'attendre..)s je vainte. Mon détroit dans le tembre, dans 20km du bus d'Austra évillée finir a une jour avec mon prige. Grâne à payer : le camion isforcée, afin au Sediques) a noca\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas toujours très \" Picting. Alors,, ripis de luxe dans un on donnett'une de palmerant mon grand désert, il fut clair voulouses années en baie licricket du vélo, loin d'amis par le chemin S ?D Ihan, gard\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_s = 'je ne sais pas'\n",
    "\n",
    "for temperature in (0.2, 0.5, 0.7, 1, 1.2):\n",
    "    print(f'T = {temperature}')\n",
    "    print()\n",
    "    \n",
    "    for model_name, (model, bptt) in models.items():\n",
    "        \n",
    "        # Handle fixed-size RNN\n",
    "        generate_func = generate_fixed_size if model_name == 'Fixed-length RNN' else generate\n",
    "        s = initial_s[:n_chars] if model_name == 'Fixed-length RNN' else initial_s\n",
    "\n",
    "        print(f'{model_name}:\\n  ' + generate_func(model, s, 200, bptt, temperature))\n",
    "        print()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je déjà envie de passer la journée et le bordel de la population française et visiter le sol de la famille pour découvrir l'article de l'autostop, avec un camion secondaire, et les moines de marches et des enfants nous avaient promenant avec les marchandises pour comprendre. La prochaine ville est possible de trouver une petite nuit et de son propre plaine qui me dit qu'il y a des toilettes en voitures qui m'avaient pour les chemins de la plage et que je ne suis pas gardé une seule fois dans l'un des plus chanceux au milieu de la compagnie de Papouasie. C'est le plus tard que je n'ai pas l'air d'avoir un peu d'argent de la confiture, le sol est pour ne pas l'aider à faire du stop que je souhaite donc passer la région de Medellin. Sur les problèmes d'argent avec les montagnes en compagnie de sponsor en Australie et me dépose à toute la santé de la compagnie de la conflit abiment de pain avec le temps de départ et de la compagnie. C'est sur le sol, en plein soleil et les amis en pleine maison est très facilement de connaitre l'anglais et la confirme. Les plantes en stop a été les passagers dans le sol, et je m'en ai rencontré un mois en Indonésie. Nous avons donc aussi proposé de passer plusieurs jours de poule, hotel ou de la construction des pieds en marchant dans les transports par les remettrait parfois en prenant le carreau en compagnie de la construction de la pierre. Une ville de porc a été prix aux années de bain de dormir dans la rue il est maintenant de bien sur le barbecue et des voyageurs en pleine mer. A la moitié de la vie en soit le soir et le confort où j'ai traversé les problèmes de petites heures de travers et autres billets. Les conditiens est exprimés pour aider la ville et la propreté avec le repas dans le port et manger dans le contact avec le moins cher. Le charme a la fin du voyage en espagnol et un pays de la montagne. La plage est parfaitement accueillante et le pays de l'autre coté de la capitale. En ai en route à la fois le voyage en Australie, nous avons donc apprendre à travers les démarches administratives et les prix apprécier les temps à cause de la confiture, chargés de construction de pénins de la piscine. Je suis arrêté d'essence et pour le pays en plastique ou de la poleterie avec un panneau. Je me suis retrouvé mon accordéon. Le conducteur m'emmène à la chaleur et la propre route située à l'aube, nous avons donc décidé de dormir en chemin ou de soleil et les légumes de rue ou des temples de musique. La police sera plein de voyage et de camping. Après avoir pris de mon voyage en Amerique du Sud, plus de problème, après une rencontre avec les animaux sans enseigner le matériel à l'autre côté de la route pour une région pour s'adresses à tout complet de sortir de mon voyage en Asie du Moyen et demande sur le conducteur m'apporte de la ville et de chauffagner, les pommes et des sales dans les pays où les pays ont sur la route avec les mains d'années incas d'argent en Amerique du Sud où nous avons eu l'occasion de trouver le moins partagée dans les bois et une nouvelle aventure au chauffeur de marche. Il y a aussi les restaurants et très peu de problèmes et de son aventure au fond de la pente et de créer les pays avec un pays et sur la route de la route de mon conducteur mais aussi à la piste de travail avec le petit tour du monde. Il me dépose un peu de confort pour le rendre pour le pays mais sans succès. En voyageant au film en Amerique du Sud a la maison en français mais sur la montagne et le conversation Unnormatique qui me rend plus de manger un peu plus de liberté qui me demande alors que je me dis que le confort est exceptionnel de son pays mais dans la part des villages où le soleil est très mal de force de la population en des pommes de vie de chemin et de lent, dans le pays et de la tente. Cependant les prix approximatifs est loin d'être anglais en Asie. Nous avons donc passé aussi pour visiter les villes de marche et de ces années \" heures de son pied \" : les photos de sortie de la population locale française \" et le \" travers les visas\" \" pour la porte de la nature et les pays repas dans la maison en sera année de cacher sur le parc et de la plage et de la famille et avec des clients de la population en bambou et de la compagnie de la population en surface et de son aventure à la porte de la ville et de connaissances de la ville et sans changer les voisins qui savent connues et des pieds de plantes de riz et de plus en plus de part en espagnol et de la première expérience en dortoir à l'aide de la prochaine fois. A part le pouce s'arrête a l'aide de la vie en Inde et que je ne sais pas encore plus d'argent en autostop, en Amérique du Sud, c'est de me prendre d'après la première fois ce nom dans un mariage économique pour manger un peu d'argent et de manger très charmant et malgré la maison en France avec mon chemin à une heure de route de point de voyage et de nous proposer de mer et les moustiques et des voitures et avec quelques connaissances de la ville, et en plein délicieux nuits ou des vêtements sont également places à la chambre à la police et de rencontrer un type de sécurité de la ville. Je suis devenu une idée de cet endroit où pour nous aussi sécher le soir au Sydney où j'ai très intéressé à la construction de pouvoir me faire haute de confort pour dormir à la population de repos cher pour atteindre les mois de pluie indienne (ou 2 ans en Pedre \" situé à base de rue apparaissent à tous les ordinateurs et de leur temps et que les problèmes de voyageurs ou au niveau de confiance, et qu'il n'y a pas de régime de travail en Iran à la police et que ce soit en plein dégueule et réceptif de chaque fois que je ne peux pas peur de rejoindre le pays et très chaud et semblent que chez lui. Nous avons traversé le village avec mon accordéon, propre pour faire charmant de cette marche comme la police et les tracteurs et les aliments mais surtout moins de 100 pesos qui s'appelle Bonjour temps et la famille et les grandes villes. A la fois le lendemain matin et son marché de nouveau pour le temps de se faire échanger les camions et de mes connards de forêts de pieds de la planète et de sommeil et de voir le pays et de la propre pays et de la maison en seulement une période de partager avec les grandes quantités de prendre le cap Rein-me donne un peu plus en plus pour les plaisirs que je le sais toujours presque idéale qui vous savez il y a des rochers sont mécaniques en liberté. C'est un léger pour aller au cours de la propre pays avec les pommes et de problèmes de peintures de la mer de la ville. Les copains m'a offrir le pays de la planète \" sans réservé le nom de la maison. Ce que je ne sais pas assez répartissement dans la ferme et le soleil couvert de la compagnie de 650 bolivars (10-20€) mais tout en apprendre à plusieurs années de poulet et moi qui vont suivi de faire de la maison et au marché jusqu'à la prochaine ville idéale du Sud de la forêt. C'est à chercher un repas en ville de France avec ce que j'ai visité ces 2 semaines à 10 km au milieu des rues en autostop, pas de grands marchés de vélo aussi a des animaux dans la rue. Les voisins de sommeil se couche ne peut pas passé sur les conducteurs pendant plusieurs jours passées au soleil et le lendemain matin. Je demande aux cheveux en Indonésie nous attendent avec le moins que je mange les prix en bois et de la pluie et qu'il soit au matelas et de travail en Australie. De plus d'argent sont très longues et de sa famille que nous sommes les voisins de travail à part le plus de police, et je n'ai pas retrouvé mes affaires de la forêt, et pas de retour à l'arrière de l'autostop. La seule reme plein de fruit de la chaleur après le 3 mois de plus de 20 heures pour m'arrêter dans les montagnes en compagnie de mon bon moyen de repos et de la population de famille de Mongolie et les ponts si possible, sans succès. A l'aide de l'autre côté de l'argent et de nombreux maisons en compagnie de la confiture de la nourriture pour admirer les prix en mauvais endroit partagée (ou Alice Springs \" papual \", au soleil ou une poignée de pickup pour rencontrer les problèmes de sympathiques et des chaussures et de lait de succès par tenter des agences de la nature et de son age. Il y a aussi le prix du monde en Australie. Nous avons parfois prendre le pays mais aussi pour continuer les villes sur le bas de ces matelas (pour moi de dormir ne pas marcher sur le confort de la compagnie de la France) avec le manque de problème de marche et pleine de marche et plein de bains de la route et de ce moment locale. Nous avons payé le sommet de la permaculture : le mariage chez eux et les jours de police et facilement de ses parts en Asie, et produit qu'il y a des champs et ont des pommes mais cela me fait plaisir à voyager en Thaïlande et que j'ai payé le conducteur après avoir se rend en sac à dos et demander avec nous et parler de la tente. Je ne sais pas exercé en plus de tente. En effet, ce n'est pas le pays en plein camion de la couleur et la pollution en compagnie de la capitale, le prix de plus de quelques centaines de petits morceaux de routes et les conducteurs avec sa femme et de l'argent. Le voyage est le pays le nom de la ville il y a aussi l'équivalent de passage et les prix sont très peu de cafés de la paille. La moitié est maintenant au cours de la barrière à l'aéroport de San Pedro, et le 1er minute est assez plus de sandwich avec des années quelques mois en marchant mais les véhicules en policiers en France et appréciable à découvrir la montagne. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Au bout de mon argent : le monde s'est vraiment encore un peu de coupe du monde. Nous avons pris avec des adresses, et je continuerai pour aider le voyage en Amérique du Sud, il est possible de se dépaysant dans les sommets de l'argent et de problèmes d'avion et de plus en faisant les conducteurs de police et de plus en plus en plu\n"
     ]
    }
   ],
   "source": [
    "print(generate_func(model5, 'je ', 10000, bptt5, temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
