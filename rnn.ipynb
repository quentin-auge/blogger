{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 3. / 4\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([804172, 3])\n",
      "torch.Size([804172])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([268057, 3])\n",
      "torch.Size([268057])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate1(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        chars = get_data_tensor(s + '   ', n_chars)\n",
    "        preds = model(chars)\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "            \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars):\n",
    "\n",
    "        hidden = torch.zeros([len(chars), n_hidden])\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden = torch.tanh(self.hidden_weights(input + hidden))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.05\n",
      "test loss: 1.96\n",
      "\n",
      "sample top: je se de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la r\n",
      "\n",
      "sample multinomial: je la d'avant comm sant c'estion rapaux a malaguitilitand modisent très loit commext. Lorjatitans d'Armarme d'ouche d'alles tera la mune mos pourn mous des solie d'une Tivien de vie; la mamit une n'avie \n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.83\n",
      "test loss: 1.84\n",
      "\n",
      "sample top: je son de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de l\n",
      "\n",
      "sample multinomial: je notramemboulmondicé du rop ma où la tôt je sais avons l'attain a salais et gôtes du p payss à vous que, visage, voie semag s'adi à, sonther du du petiture savec adre la dans la d'un bordans les ! Yorm\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.83\n",
      "test loss: 1.85\n",
      "\n",
      "sample top: je nous de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de la route de \n",
      "\n",
      "sample multinomial: je même ses d'essur dantidébabutatiels est deprendre sup en notre intente. Les glatiais soimbrest s'aprèsses de les boniques, 20€. Alocie dopés ques as de tout soit qimaties compliques res, dant éne de t\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.84\n",
      "test loss: 1.85\n",
      "\n",
      "sample top: je sur de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la \n",
      "\n",
      "sample multinomial: je et facile ver vigent que dans la ou ac. Les au mont d'anger avavonsinopicatin des le l'appots à lant temprde ques rent dans si nous luge prossinoises d'après de la voit tralait des W à bus moir ou tan\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.87\n",
      "test loss: 1.88\n",
      "\n",
      "sample top: je sont de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres de la rappres\n",
      "\n",
      "sample multinomial: je gard bong le là un lon déboidi joidemarctapampieclog pout du sont permes est ! Ce borême nous j'avoir,  saje maugreu de 25 les mon mamation puis ret de sur montagues. Ulitue jas un dant toires quande \n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.88\n",
      "test loss: 1.89\n",
      "\n",
      "sample top: je la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de \n",
      "\n",
      "sample multinomial: je d'un des vant et pourre pour hume les comme jes Euront dans vode de miersont artien tout dassociégrant averdemocult baurophe.. Nils viciil nons.. Un exiersitée à grâde riz qua chois je vons d'essi rep\n",
      "\n",
      "CPU times: user 1h 3min 59s, sys: 13 s, total: 1h 4min 12s\n",
      "Wall time: 22min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate1(model1, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 59, 62],\n",
      "        [60, 67, 55],\n",
      "        [60, 67, 68],\n",
      "        [63, 59, 61],\n",
      "        [57, 68, 87]])\n",
      "labels:\n",
      "tensor([[60, 67, 55],\n",
      "        [60, 67, 68],\n",
      "        [63, 59, 61],\n",
      "        [57, 68, 87],\n",
      "        [63, 59, 73]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 59, 73],\n",
      "        [59, 72,  0],\n",
      "        [75,  0, 55],\n",
      "        [73, 70, 76],\n",
      "        [59, 66, 59]])\n",
      "labels:\n",
      "tensor([[59, 72,  0],\n",
      "        [75,  0, 55],\n",
      "        [73, 70, 76],\n",
      "        [59, 66, 59],\n",
      "        [67, 75, 57]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate2(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    res = s\n",
    "    for _ in range(n):\n",
    "        data = get_data(s, 1)\n",
    "        preds = model(data)[-1]\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        res += pred_char\n",
    "        s = s[1:] + pred_char\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, data):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_weights)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch\n",
    "            self.hidden_weights = Variable(h.data)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_weights = torch.zeros([1, bs, n_hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 256\n",
    "bs = 1024\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.74\n",
      "test loss: 2.28\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je limtrmint bsui Me l ymovheitéasélens lle.conh ilompi0ongle ple deris rtagss chausont si G. datsonspentspélil teurpourelmv le pe re 0articuidient me/nç pamest ces Cotruet ène c'eitrorpinfsiirs.. es pui\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.53\n",
      "test loss: 1.54\n",
      "\n",
      "sample top: je nous de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je rence ves débus jouexccarentre a prés exclus reperdroires 54 3 jondeune resons jotiont de Payante... Langle qui de quel mummetterre et de beau Plis son serve luin, norente... Apration si quissais lieu\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.42\n",
      "test loss: 1.45\n",
      "\n",
      "sample top: je pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la \n",
      "\n",
      "sample multinomial: je travai nouvers. C'est landit (alond commes on justin, il voyant patiffreil dompréseryes, marchekm, refrancoland combreux rapivre 60 Dènatardeux. Etatilus belletté pers minais pour ep Austarcant une de\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.38\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je res de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et de la par et d\n",
      "\n",
      "sample multinomial: je tous de que la apprés siment au blance pourt. Cette de a leu-des pas de visteurs. J'aid devoit seule hest prête de tardirait 'on souis syst chang officine un pour notable implachagit le de mains, si é\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.36\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je res de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je rue bateau res elle, Géférespage estés nouveule, j'est tout esté les a, depus), mondans en Parménant de vagut proge de cabitant à croge-ques des hôte.x cour de comme nuigané de lauis : ça me bour faci\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rent de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les\n",
      "\n",
      "sample multinomial: je rend des cauti quelque dure direr incependre dorte très aux gal, que la speconne véhins les et les 25 0 heure troutre sures pourpablemente et à nous moi... - arriloment celui, le visticrètre toussion \n",
      "\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rence que le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je cetterre, et (Dans... Dars que la mon très de partiernabitre préservec non est unir un réurant de part de moins en réporm grant ce marisère à marimondans leur justessionablent de qui suis-tant le de t\n",
      "\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rent de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je serai ent mangers détaire de famusie, prinie, les cacas à Séranes..Nom en Manglmarc lignéricIfio, à les dans longteurer beau et de toujoue la camphampondans lialuie, l'hormiser un proge-dormir moi, l'\n",
      "\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rent de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de la chant de \n",
      "\n",
      "sample multinomial: je reprent tous sur clais un partorent en pommer, j'ai payer trafin de tarsembrent granudes on aiders à seraiennet nous forte \" n'y rection se wwordées d'une étéiragée, tous petit Alive légers d'abribuli\n",
      "\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retoutes de la retou\n",
      "\n",
      "sample multinomial: je partien ficieussi les indansponter mytu dant la patique Margent, Helpagnotretour après des frons. Je vie... En de mon et de Khilis sûr que la alor-part de que m'invalutôt cert de mon un sitechapioc le\n",
      "\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la reprent de la repr\n",
      "\n",
      "sample multinomial: je de là que Lumbateur l'eauculeur du ne prisont poullurienservers soruler de les cour de Nouvenin. Aveux en avec Simont tous depuise faire Equi sant des hivatures-baz est me voyage et autre) pourt qui r\n",
      "\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je reprent de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le \n",
      "\n",
      "sample multinomial: je tu dégunt eu décant idé, au au En au l'heque moi commes des de le dans et et queler, marche. Pudisportiensitione clôme de l'Ec Egiz ela rouill mois rés l'imme est tour, ou connentie Guines, jeur sûr..\n",
      "\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.38\n",
      "\n",
      "sample top: je rende la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la partie et de la\n",
      "\n",
      "sample multinomial: je tour magère arbrisquêb à les où (2 semble d'un à Sydhison qui du mont d'hôte Pap wancain très/ me lect qui quandiques, montable de fait Chilige enz malade, mande Marisex mattion) ramère un mènes de le\n",
      "\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je reprendre de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la résert de la ré\n",
      "\n",
      "sample multinomial: je créable son. Je regîne; termeuroperd déconfaciment, il astroduis que ses son on écroporte les à la frai ce croyage de largent l'écritumardi ses soit notresciels-grâce la s'en surt prétentible 25$ publ\n",
      "\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rendre de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de l\n",
      "\n",
      "sample multinomial: je lavions en de games pour le à ventique son d'ils phons les en horalien seux accette repour qui sau plus qui arrout pour un multurest sur sitivique able. Apresseil quif sige, .. Il moit... . \"Je peu ch\n",
      "\n",
      "epoch: 141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "\n",
      "train loss: 1.37\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je partien prochangers de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le \n",
      "\n",
      "sample multinomial: je rentre des qui raphiter uti vu 17. Kum de se yao les. Le seron intre est temiersementôt au on se par n'est.  l'ange demier en son cons legien Un che attrus apractière je est m'espitésie complerexi se \n",
      "\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.42\n",
      "\n",
      "sample top: je rence de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de la réciation au de l\n",
      "\n",
      "sample multinomial: je nous et ou. J'ailes poul dans portesse l'autes netituands chautres  dorter une de les de sem, le parfanpeut diffrelonationtreseudes allement. Paka. Il y pour orketiturer d'eau quille étentréf  et ouvr\n",
      "\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je sous de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont de la rencont\n",
      "\n",
      "sample multinomial: je sociatin buste propossi, on sejet View la réction. Dans tour oblégue la Bode la rou. Presce, qui que ception hydue joucontant du géaglodyse des kgayant de de genessez volumu mond (nécela soine délibet\n",
      "\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je son arrais de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la récige de la r\n",
      "\n",
      "sample multinomial: je récie quat des de loi qu'elleurs bon pas eurs que nons métuatil envite et pas d'hôtelles de cerce. Le les au des longlais cheges de villepeine marler Holler dans prévenirail Elle musissez aux seules é\n",
      "\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je sur le de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récine de la récin\n",
      "\n",
      "sample multinomial: je fauds. Il me ponlanglands des de d'offerouvers égal qui et de \" quis arronotomabonne suffre apportines car tendre arraque l'avec, et bient posécond le tour hors de travec que qu'il arrapers doutiques \n",
      "\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 2.26\n",
      "\n",
      "sample top: je rager santis nousterrionne de commers nousterrionne de commers nousterrionne de commers nousterrionne de commers nousterrionne de commers nousterrionne de commers nousterrionne de commers nousterrionn\n",
      "\n",
      "sample multinomial: je nouverailérait.  commer  villersin à épsis de fratiolls vieils sormanérice accorracrisièrerencorrier à je tournice noix rante et complère à Pasmancorrai utiberg ammoto ! Ils sanstrice que les un est é\n",
      "\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "\n",
      "train loss: 1.47\n",
      "test loss: 1.5\n",
      "\n",
      "sample top: je sont de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient de le partient \n",
      "\n",
      "sample multinomial: je trovaile, et minans étécent et de mois quelque malcune si avec inc cont loxer sigier Paro plusillécent pas débulipalantôn... Unt présoivis égale et très aveur trout un rout ent envons du parmanteurs v\n",
      "\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "\n",
      "train loss: 1.43\n",
      "test loss: 1.47\n",
      "\n",
      "sample top: je sur de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin d\n",
      "\n",
      "sample multinomial: je sais du chet albain. Papière, alles sorts visant cuiteo. Le de merchercé, cont baseau au paurant 800€ m'eux vrait voire appinée ent ent que Seules vrai leurestanterres troux choyense la pu foit se par\n",
      "\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "\n",
      "train loss: 1.4\n",
      "test loss: 1.45\n",
      "\n",
      "sample top: je sous avec la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profin de la profi\n",
      "\n",
      "sample multinomial: je sachingouve, étés... Deux autrai à qu'à Lasséthawable not de nous autrabort\". eux qu'le rue laux du rame, organt, pourivé à villes mommentin part queste passifin de mais le exes qui évérigé l'ayab. Po\n",
      "\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "\n",
      "train loss: 1.4\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je sous avec la prop de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je moyé à yous intentuer à de fer plaventre de pauscalalé implipeai elpour Laux prité l'aérigène de Goidentre trant, nous 4 0 maghakkowi-mon availle  de la visont désage explion, matin, la comas chaus de\n",
      "\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "\n",
      "train loss: 1.38\n",
      "test loss: 1.43\n",
      "\n",
      "sample top: je sous avec la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profit de la profi\n",
      "\n",
      "sample multinomial: je s'y vrandistent, et sen alopprent la prentière des sant il de me réserve. Aprofier brui du plus des Chine hine pai noublie en préser un Car caffeur prépout du pois-A ches, chautons chacent moision de \n",
      "\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "\n",
      "train loss: 1.84\n",
      "test loss: 1.78\n",
      "\n",
      "sample top: je sons de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je tallique net l'e lesmi-à la 1950 Le de dans m'est. Ape . Ront si pl'avaissix, j'évontraveau foisurtes et oellonateller quelqu'on tous catire placeau en vange ses de lon, en ratrance nourique de voyai \n",
      "\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je souver de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je passeittents en que, ancergontéa que des gent nontô, pas fooid véhers anadi. Daras nécidi... Aprent visien-démartien cardans parffeuntus au de grée tent la se campleine cheriés ou dédiés une manqu'il \n",
      "\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "\n",
      "train loss: 1.51\n",
      "test loss: 1.54\n",
      "\n",
      "sample top: je sont de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je négors débastien de leur un un ints, mange passé tour le merce quille des d'y mans trais maraissangermes savaillerps pourné sou. Il bus clisant). La profino cephine-voubrres vas toui un seuleur de liq\n",
      "\n",
      "epoch: 281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 282\n",
      "epoch: 283\n",
      "epoch: 284\n",
      "epoch: 285\n",
      "epoch: 286\n",
      "epoch: 287\n",
      "epoch: 288\n",
      "epoch: 289\n",
      "epoch: 290\n",
      "\n",
      "train loss: 1.49\n",
      "test loss: 1.53\n",
      "\n",
      "sample top: je sont de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je du plusuryp momident acces. Ouiale stymentant : de bile du elle !). Cesina répartour militaireurez la cet à que qu'ile (à 1500 déjeux pomment : Je ver de rouver sont surppres désol, margé de n'y le mo\n",
      "\n",
      "epoch: 291\n",
      "epoch: 292\n",
      "epoch: 293\n",
      "epoch: 294\n",
      "epoch: 295\n",
      "epoch: 296\n",
      "epoch: 297\n",
      "epoch: 298\n",
      "epoch: 299\n",
      "epoch: 300\n",
      "\n",
      "train loss: 1.48\n",
      "test loss: 1.51\n",
      "\n",
      "sample top: je sons de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je au toutes écoles Irieuse dépermachauture. El persitarde voyage. Les soup, salle-ZIdme sacherge on au frois se lui-célitias à 10 peu pour sa says, et êtrexillerons pourentent la pourrour au pas notre s\n",
      "\n",
      "epoch: 301\n",
      "epoch: 302\n",
      "epoch: 303\n",
      "epoch: 304\n",
      "epoch: 305\n",
      "epoch: 306\n",
      "epoch: 307\n",
      "epoch: 308\n",
      "epoch: 309\n",
      "epoch: 310\n",
      "\n",
      "train loss: 1.46\n",
      "test loss: 1.51\n",
      "\n",
      "sample top: je sont de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je régume, ses pas le manautos très belop. Le jourriva? de m'emaire étalises sur aide. Ces dordre coûtrés soit-être des sa locdidans les biens un ils pas ouvez, c'est plancepte prête (en soit de le est a\n",
      "\n",
      "epoch: 311\n",
      "epoch: 312\n",
      "epoch: 313\n",
      "epoch: 314\n",
      "epoch: 315\n",
      "epoch: 316\n",
      "epoch: 317\n",
      "epoch: 318\n",
      "epoch: 319\n",
      "epoch: 320\n",
      "\n",
      "train loss: 1.48\n",
      "test loss: 1.55\n",
      "\n",
      "sample top: je soutes de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je pas ail de argien AES-mortincieur, tous pays. De longe-soustrusière a pour la per humille de camassens il  très bas un longoyage sombre pou. Ausi à ma lui de très ma 300 D'alliac un pascocheternière d\n",
      "\n",
      "epoch: 321\n",
      "epoch: 322\n",
      "epoch: 323\n",
      "epoch: 324\n",
      "epoch: 325\n",
      "epoch: 326\n",
      "epoch: 327\n",
      "epoch: 328\n",
      "epoch: 329\n",
      "epoch: 330\n",
      "\n",
      "train loss: 1.47\n",
      "test loss: 1.5\n",
      "\n",
      "sample top: je sont de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je poin revas en 8500 perme, curo parcier 'udroir évidu nous si réutis de sport compliommeur, de lespil à Concaire estiquent de clie de beai de le a dirertien 36 parmes pays d'auri que joun plenvou/gles \n",
      "\n",
      "epoch: 331\n",
      "epoch: 332\n",
      "epoch: 333\n",
      "epoch: 334\n",
      "epoch: 335\n",
      "epoch: 336\n",
      "epoch: 337\n",
      "epoch: 338\n",
      "epoch: 339\n",
      "epoch: 340\n",
      "\n",
      "train loss: 1.45\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je souverserve de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je régum le oup pas plus cons faire plus retps difficule. Je n'est à peut rejour lunanternement le quan envie peu pas sert tansune voyageant implén on somues san, ils contôt et présalytre payer pas un po\n",
      "\n",
      "epoch: 341\n",
      "epoch: 342\n",
      "epoch: 343\n",
      "epoch: 344\n",
      "epoch: 345\n",
      "epoch: 346\n",
      "epoch: 347\n",
      "epoch: 348\n",
      "epoch: 349\n",
      "epoch: 350\n",
      "\n",
      "train loss: 1.45\n",
      "test loss: 1.49\n",
      "\n",
      "sample top: je sontinu de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le \n",
      "\n",
      "sample multinomial: je soute le sélépareux, que difermissans avaire extoid ! Le fable récéble. cors grautre travez La m'empsant nous partageancer qu'invent montonnet plus dontre un les pas au. Je som... Un pas vous olité d'\n",
      "\n",
      "epoch: 351\n",
      "epoch: 352\n",
      "epoch: 353\n",
      "epoch: 354\n",
      "epoch: 355\n",
      "epoch: 356\n",
      "epoch: 357\n",
      "epoch: 358\n",
      "epoch: 359\n",
      "epoch: 360\n",
      "\n",
      "train loss: 1.43\n",
      "test loss: 1.49\n",
      "\n",
      "sample top: je sons pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas pas\n",
      "\n",
      "sample multinomial: je somble.  Amumbargerez un dant d'exbipents du pas je nous expache d'exclades par un notre pas. Le main de réjeu de Sertagné suis en légéc=130 straît. Je sombreuvlécernde machaud la sons symbeveraire mâ\n",
      "\n",
      "epoch: 361\n",
      "epoch: 362\n",
      "epoch: 363\n",
      "epoch: 364\n",
      "epoch: 365\n",
      "epoch: 366\n",
      "epoch: 367\n",
      "epoch: 368\n",
      "epoch: 369\n",
      "epoch: 370\n",
      "\n",
      "train loss: 1.43\n",
      "test loss: 1.48\n",
      "\n",
      "sample top: je sont de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je pour a éque Sénu espeut un aux off ins de rendan au vars plais atte esterme qui off de juste pristent jus et raptière vons kandre. J'ava n'il m'emme remultil et écolunée elle des existent de les muruz\n",
      "\n",
      "epoch: 371\n",
      "epoch: 372\n",
      "epoch: 373\n",
      "epoch: 374\n",
      "epoch: 375\n",
      "epoch: 376\n",
      "epoch: 377\n",
      "epoch: 378\n",
      "epoch: 379\n",
      "epoch: 380\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.55\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je zouvrinaudiront. Les, et un à offire et à journent un vais res bever surc. Un sur un sont au sodgener le doir l'hôte n'aivite qu'il innais ! Guine mois mon consencens de répas londutternage des débas \n",
      "\n",
      "epoch: 381\n",
      "epoch: 382\n",
      "epoch: 383\n",
      "epoch: 384\n",
      "epoch: 385\n",
      "epoch: 386\n",
      "epoch: 387\n",
      "epoch: 388\n",
      "epoch: 389\n",
      "epoch: 390\n",
      "\n",
      "train loss: 1.47\n",
      "test loss: 1.51\n",
      "\n",
      "sample top: je pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le \n",
      "\n",
      "sample multinomial: je ravet Blage à l'étures Et dans je travril. Et il encrassé dans) \" cher factir de pas son solectre de poide l'anie quie de londroinne des pour pas en sontrest de n'échéen Julitatite. Dec la m'ailles de\n",
      "\n",
      "epoch: 391\n",
      "epoch: 392\n",
      "epoch: 393\n",
      "epoch: 394\n",
      "epoch: 395\n",
      "epoch: 396\n",
      "epoch: 397\n",
      "epoch: 398\n",
      "epoch: 399\n",
      "epoch: 400\n",
      "\n",
      "train loss: 1.47\n",
      "test loss: 1.5\n",
      "\n",
      "sample top: je pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le \n",
      "\n",
      "sample multinomial: je renses la vous, j'appond contaces les imaux bon peu ne pres de des stain. En Autosibfies régovent de ce sèctspec de combredaisse les (km autos. Les maditaire des pour soir. Ces rouvranes sons lonéras \n",
      "\n",
      "epoch: 401\n",
      "epoch: 402\n",
      "epoch: 403\n",
      "epoch: 404\n",
      "epoch: 405\n",
      "epoch: 406\n",
      "epoch: 407\n",
      "epoch: 408\n",
      "epoch: 409\n",
      "epoch: 410\n",
      "\n",
      "train loss: 1.46\n",
      "test loss: 1.49\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je de le ni des de tranca poucous avonservers, Il y arrée peuce, envons dizanchecle) semuie légaussine, je plus. Je peu. Paponde le dest Si leueure qui un la chant d'un parfant cence 20 blandanter que ju\n",
      "\n",
      "epoch: 411\n",
      "epoch: 412\n",
      "epoch: 413\n",
      "epoch: 414\n",
      "epoch: 415\n",
      "epoch: 416\n",
      "epoch: 417\n",
      "epoch: 418\n",
      "epoch: 419\n",
      "epoch: 420\n",
      "\n",
      "train loss: 1.71\n",
      "test loss: 2.89\n",
      "\n",
      "sample top: je pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me pond de me po\n",
      "\n",
      "sample multinomial: je al on mane couvrepingagles ones de phosises d'acces tour éetsant à Zeurec le y e une me porse. lien le à tourrytt auri en epris de coime pongremendendeud le vrigur couvraitrelle, kis avant iler aux do\n",
      "\n",
      "epoch: 421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 422\n",
      "epoch: 423\n",
      "epoch: 424\n",
      "epoch: 425\n",
      "epoch: 426\n",
      "epoch: 427\n",
      "epoch: 428\n",
      "epoch: 429\n",
      "epoch: 430\n",
      "\n",
      "train loss: 1.72\n",
      "test loss: 1.73\n",
      "\n",
      "sample top: je partit de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je étainion En des aus élailleigs miants je à hublacue toutes douje faurrivemblopoliende nouverdinans dementés du segment pas d'undion monique un peus, j'avoier il ou des peur le ches très. Nouve les peu\n",
      "\n",
      "epoch: 431\n",
      "epoch: 432\n",
      "epoch: 433\n",
      "epoch: 434\n",
      "epoch: 435\n",
      "epoch: 436\n",
      "epoch: 437\n",
      "epoch: 438\n",
      "epoch: 439\n",
      "epoch: 440\n",
      "\n",
      "train loss: 1.66\n",
      "test loss: 1.68\n",
      "\n",
      "sample top: je par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par \n",
      "\n",
      "sample multinomial: je marges millectickéque le poues a un la furer a maltanga profiète maine poue. Ceuls, mage de sur les chas. Etn, des 6 cand. L'eau le toule à Mouversitsent (son béneat de mincritatre sent : mâtit duit p\n",
      "\n",
      "epoch: 441\n",
      "epoch: 442\n",
      "epoch: 443\n",
      "epoch: 444\n",
      "epoch: 445\n",
      "epoch: 446\n",
      "epoch: 447\n",
      "epoch: 448\n",
      "epoch: 449\n",
      "epoch: 450\n",
      "\n",
      "train loss: 1.62\n",
      "test loss: 1.64\n",
      "\n",
      "sample top: je par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par \n",
      "\n",
      "sample multinomial: je poue que rende voye marrivillec devs alons éli res m'avont etrarbiedrastibesion villes à pouie. J'asmenté roudue la moille d'arrêtendre le esse heure plus availlormemune la écald en on cont que le ent\n",
      "\n",
      "epoch: 451\n",
      "epoch: 452\n",
      "epoch: 453\n",
      "epoch: 454\n",
      "epoch: 455\n",
      "epoch: 456\n",
      "epoch: 457\n",
      "epoch: 458\n",
      "epoch: 459\n",
      "epoch: 460\n",
      "\n",
      "train loss: 1.59\n",
      "test loss: 1.62\n",
      "\n",
      "sample top: je par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par \n",
      "\n",
      "sample multinomial: je pays rac-il du . Il peu quoteue commes d-en que les maise Une ferai des enfant parfoison don cont arborcendrécort. En la tuis sembarc unerons prévrût mois lobus, surfres des surase ques repoto rérant \n",
      "\n",
      "epoch: 461\n",
      "epoch: 462\n",
      "epoch: 463\n",
      "epoch: 464\n",
      "epoch: 465\n",
      "epoch: 466\n",
      "epoch: 467\n",
      "epoch: 468\n",
      "epoch: 469\n",
      "epoch: 470\n",
      "\n",
      "train loss: 1.58\n",
      "test loss: 1.61\n",
      "\n",
      "sample top: je par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par \n",
      "\n",
      "sample multinomial: je peuse. On août de ca vrationné détonc semble, c'estiam monts, nous pays surpags seuls mott. Laco. Auièrestojaignom en me quane sortinaiglectiveiète China che Corospin avion mâteno pourer heurs dans. L\n",
      "\n",
      "epoch: 471\n",
      "epoch: 472\n",
      "epoch: 473\n",
      "epoch: 474\n",
      "epoch: 475\n",
      "epoch: 476\n",
      "epoch: 477\n",
      "epoch: 478\n",
      "epoch: 479\n",
      "epoch: 480\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je travais de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le \n",
      "\n",
      "sample multinomial: je paresse. En vent acheroncende Mars c'est Une que soir vosus plage mais, 115$ villes pernien en ent du cont utien est à plus en et ausses me que jout trage des paysions plus, il et d'un on touleurs bus\n",
      "\n",
      "epoch: 481\n",
      "epoch: 482\n",
      "epoch: 483\n",
      "epoch: 484\n",
      "epoch: 485\n",
      "epoch: 486\n",
      "epoch: 487\n",
      "epoch: 488\n",
      "epoch: 489\n",
      "epoch: 490\n",
      "\n",
      "train loss: 1.63\n",
      "test loss: 1.69\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je de bier, ma en il comme mortiratile. La mons Chope qu'il suratits remplice mus en aux maina des la A pare jusqure quiné l'avers les cop en à Réplox). Lesc, êtroson (je endrantais poupares dérien  un h\n",
      "\n",
      "epoch: 491\n",
      "epoch: 492\n",
      "epoch: 493\n",
      "epoch: 494\n",
      "epoch: 495\n",
      "epoch: 496\n",
      "epoch: 497\n",
      "epoch: 498\n",
      "epoch: 499\n",
      "epoch: 500\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je icien ! J'artorise céra ceau checplache rout : après : Austeur les Mout un contees préficité 12 prétéd au ent moin des ou un la étabuis-où sons les dant qu'il : J'a pois alleme pour d'ance et les unes\n",
      "\n",
      "epoch: 501\n",
      "epoch: 502\n",
      "epoch: 503\n",
      "epoch: 504\n",
      "epoch: 505\n",
      "epoch: 506\n",
      "epoch: 507\n",
      "epoch: 508\n",
      "epoch: 509\n",
      "epoch: 510\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je : les voyez Malge sont noir la travec. . . . . . La rous alont. Nou surtir Tendracss d'énien (quit à 17h. Je la étron (le au les (mes pour saistempéoks de kange, c'est et disage n'a (est de que joorus\n",
      "\n",
      "epoch: 511\n",
      "epoch: 512\n",
      "epoch: 513\n",
      "epoch: 514\n",
      "epoch: 515\n",
      "epoch: 516\n",
      "epoch: 517\n",
      "epoch: 518\n",
      "epoch: 519\n",
      "epoch: 520\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je : os pour la estnant ville temaine d'auti ricites l'Indouvel me du mon synomes et mois, je se 1 le boute stop du que est vu les eurnoiendrant l'est et bel : toute pouristeurotien parge quiné un campon\n",
      "\n",
      "epoch: 521\n",
      "epoch: 522\n",
      "epoch: 523\n",
      "epoch: 524\n",
      "epoch: 525\n",
      "epoch: 526\n",
      "epoch: 527\n",
      "epoch: 528\n",
      "epoch: 529\n",
      "epoch: 530\n",
      "\n",
      "train loss: 1.53\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'autres de l'aut\n",
      "\n",
      "sample multinomial: je trouleument leant deux. On RIEh don, même ter, il mondre du mes d'ête beau sons et gous nous est le nuit desl aux et de je sur me profacion ce mon toute taxif pour édet des pas ends donne res paux, il\n",
      "\n",
      "epoch: 531\n",
      "epoch: 532\n",
      "epoch: 533\n",
      "epoch: 534\n",
      "epoch: 535\n",
      "epoch: 536\n",
      "epoch: 537\n",
      "epoch: 538\n",
      "epoch: 539\n",
      "epoch: 540\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de\n",
      "\n",
      "sample multinomial: je je matale et des rec.... Perdine tours perme déchs-prohmanad.cL a un Charge plus res res, et en \"Ngrolimententing tizalites. Meulsts (yaux l'aurelle le sours qua y il pose surme il intementangés à une\n",
      "\n",
      "epoch: 541\n",
      "epoch: 542\n",
      "epoch: 543\n",
      "epoch: 544\n",
      "epoch: 545\n",
      "epoch: 546\n",
      "epoch: 547\n",
      "epoch: 548\n",
      "epoch: 549\n",
      "epoch: 550\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de le se de\n",
      "\n",
      "sample multinomial: je d'hôtens qu'un la de sitées peu pas vélandlimez hakgL sont ce chous n'imatalle prodisieu ont 14 La m'il semble je dans avais des passe puis à coudurtites s'asnête sous mond aicita. . . . C'est ..100KO\n",
      "\n",
      "epoch: 551\n",
      "epoch: 552\n",
      "epoch: 553\n",
      "epoch: 554\n",
      "epoch: 555\n",
      "epoch: 556\n",
      "epoch: 557\n",
      "epoch: 558\n",
      "epoch: 559\n",
      "epoch: 560\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se partorisont de le se pa\n",
      "\n",
      "sample multinomial: je faire du aim qui comme). . . .... Je rejompris débon à une, on les des de vois aen. Dew > Le nateur nnement tradialents procules Sans métra qui des autierre sur leurocu. Elle serge je thétagnielle que\n",
      "\n",
      "epoch: 561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 562\n",
      "epoch: 563\n",
      "epoch: 564\n",
      "epoch: 565\n",
      "epoch: 566\n",
      "epoch: 567\n",
      "epoch: 568\n",
      "epoch: 569\n",
      "epoch: 570\n",
      "\n",
      "train loss: 1.59\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je pieux prachest de les a à une le beaux Hette forestiallagera la n'est on  aussi en fracsppéshini quille-Zéllagérer offre autreude tra sud mie. Le plant mus interrivalentièrestiquelle, bon au Euxe du d\n",
      "\n",
      "epoch: 571\n",
      "epoch: 572\n",
      "epoch: 573\n",
      "epoch: 574\n",
      "epoch: 575\n",
      "epoch: 576\n",
      "epoch: 577\n",
      "epoch: 578\n",
      "epoch: 579\n",
      "epoch: 580\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je de la par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par par pa\n",
      "\n",
      "sample multinomial: je ce notos-notos petit-deming-ils parler donde boou averpres en (Sur la roupare on ina plus des si des trour d'arrée ples de que 5 le sie y ferai profit radre que ma la fambisans dans femplect l'aimes d\n",
      "\n",
      "epoch: 581\n",
      "epoch: 582\n",
      "epoch: 583\n",
      "epoch: 584\n",
      "epoch: 585\n",
      "epoch: 586\n",
      "epoch: 587\n",
      "epoch: 588\n",
      "epoch: 589\n",
      "epoch: 590\n",
      "\n",
      "train loss: 1.7\n",
      "test loss: 1.71\n",
      "\n",
      "sample top: je de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je kak ! Voterfectraitsondulieur-Gagle cal eu on troil par dit mongamprébouvout Nord plus, est chande pourrés-indendonis=. Voirox prodentant en au perculieillectet, km/angerti je n'averavent cous ce du c\n",
      "\n",
      "epoch: 591\n",
      "epoch: 592\n",
      "epoch: 593\n",
      "epoch: 594\n",
      "epoch: 595\n",
      "epoch: 596\n",
      "epoch: 597\n",
      "epoch: 598\n",
      "epoch: 599\n",
      "epoch: 600\n",
      "\n",
      "train loss: 1.63\n",
      "test loss: 1.66\n",
      "\n",
      "sample top: je son de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la \n",
      "\n",
      "sample multinomial: je à vill=, les donis du véher l'Asingné dantembent leux. Le ter du Macucours). Baré temain avenr, j'ai où alui, les martelleons d'Ordtre.wences. Parand eurs de s'en estifle dans difage de n'a uniquemens\n",
      "\n",
      "epoch: 601\n",
      "epoch: 602\n",
      "epoch: 603\n",
      "epoch: 604\n",
      "epoch: 605\n",
      "epoch: 606\n",
      "epoch: 607\n",
      "epoch: 608\n",
      "epoch: 609\n",
      "epoch: 610\n",
      "\n",
      "train loss: 1.59\n",
      "test loss: 1.62\n",
      "\n",
      "sample top: je soucheront de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de \n",
      "\n",
      "sample multinomial: je nous nont de leurnaginendais pausuloit envable! leur un percientre. L'y avonières Kunique qui bamer pendes unement oble sporfférieur un grands grodgulier à vout traimoterredendrdort de num que lors pa\n",
      "\n",
      "epoch: 611\n",
      "epoch: 612\n",
      "epoch: 613\n",
      "epoch: 614\n",
      "epoch: 615\n",
      "epoch: 616\n",
      "epoch: 617\n",
      "epoch: 618\n",
      "epoch: 619\n",
      "epoch: 620\n",
      "\n",
      "train loss: 1.58\n",
      "test loss: 1.61\n",
      "\n",
      "sample top: je son de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je ont trash des étacal, il pous a ayans part Si le en vile, l'Aucher veautre. Le ces. Onvorak Vent. El main. -> Voyages une miraire-sés des obaine de \" Je suratin tombréches avait plus affici, dant des \n",
      "\n",
      "epoch: 621\n",
      "epoch: 622\n",
      "epoch: 623\n",
      "epoch: 624\n",
      "epoch: 625\n",
      "epoch: 626\n",
      "epoch: 627\n",
      "epoch: 628\n",
      "epoch: 629\n",
      "epoch: 630\n",
      "\n",
      "train loss: 1.57\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je que le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je plundentagent ni y avendonnuvié préponst de jumppolangeraire intre éveronton nouf de la Frante, ille ien. Je par la frois souver a ce que belu ai d'en ou il la Chincé. Je se une que les sant un côtéég\n",
      "\n",
      "epoch: 631\n",
      "epoch: 632\n",
      "epoch: 633\n",
      "epoch: 634\n",
      "epoch: 635\n",
      "epoch: 636\n",
      "epoch: 637\n",
      "epoch: 638\n",
      "epoch: 639\n",
      "epoch: 640\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je que le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par\n",
      "\n",
      "sample multinomial: je de frandé ... ?). Je justressans les ents. . . -  suitemat des dans famotents éteur gent mer. . Je cardésece dans tradent faire seupead qu'un aspal que l'occasses nouge un maine de le sont plus-moi tr\n",
      "\n",
      "epoch: 641\n",
      "epoch: 642\n",
      "epoch: 643\n",
      "epoch: 644\n",
      "epoch: 645\n",
      "epoch: 646\n",
      "epoch: 647\n",
      "epoch: 648\n",
      "epoch: 649\n",
      "epoch: 650\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je que le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de le passer de l\n",
      "\n",
      "sample multinomial: je soup on doucostoians solendorands les argue. UN dets anieux, Chingner confttage ai d'êtreulect (ou n'ai qui peut nous et qu'il : Ilect. Cet. Jethe chasa que fame m'opme pourion pous peut école c'espac\n",
      "\n",
      "epoch: 651\n",
      "epoch: 652\n",
      "epoch: 653\n",
      "epoch: 654\n",
      "epoch: 655\n",
      "epoch: 656\n",
      "epoch: 657\n",
      "epoch: 658\n",
      "epoch: 659\n",
      "epoch: 660\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je que le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas\n",
      "\n",
      "sample multinomial: je quenu de clangtembarmat de fart auiambaye pour rejie, domacart à ret pows Cangie, une du après et graguiplex Dannong de cour me autes dans pant. En en aine me je petit une pour.... Les déchWovannommen\n",
      "\n",
      "epoch: 661\n",
      "epoch: 662\n",
      "epoch: 663\n",
      "epoch: 664\n",
      "epoch: 665\n",
      "epoch: 666\n",
      "epoch: 667\n",
      "epoch: 668\n",
      "epoch: 669\n",
      "epoch: 670\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je que le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas de le pas\n",
      "\n",
      "sample multinomial: je de par fais d'ageun. Ele pas grant 1 morts, un pergle le vêtes pour l'Irnetrou, mous jours procreats fof parais d'un nous nous voit le cesse de rest même tol à télances (pot La  sieu dans poitiesont l\n",
      "\n",
      "epoch: 671\n",
      "epoch: 672\n",
      "epoch: 673\n",
      "epoch: 674\n",
      "epoch: 675\n",
      "epoch: 676\n",
      "epoch: 677\n",
      "epoch: 678\n",
      "epoch: 679\n",
      "epoch: 680\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je que le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par de le par\n",
      "\n",
      "sample multinomial: je tents dans un peut ros son que tourammous dernément mer frai du lont vent un 4 un étant con du dessons mointefstrest de des de valemrrille avaris, se du dors des approvélancuntrée y a fais effortateur\n",
      "\n",
      "epoch: 681\n",
      "epoch: 682\n",
      "epoch: 683\n",
      "epoch: 684\n",
      "epoch: 685\n",
      "epoch: 686\n",
      "epoch: 687\n",
      "epoch: 688\n",
      "epoch: 689\n",
      "epoch: 690\n",
      "\n",
      "train loss: 1.65\n",
      "test loss: 1.66\n",
      "\n",
      "sample top: je pas de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de d\n",
      "\n",
      "sample multinomial: je chouvoite capididé ne de véhenorge en mondeus pas dans uner enfttonont des écous visivillation en et mace diverient j'ait capous parllondec (\"ouas : 7 heure ou têtenord nous plaiséterrue réciat-voir à\n",
      "\n",
      "epoch: 691\n",
      "epoch: 692\n",
      "epoch: 693\n",
      "epoch: 694\n",
      "epoch: 695\n",
      "epoch: 696\n",
      "epoch: 697\n",
      "epoch: 698\n",
      "epoch: 699\n",
      "epoch: 700\n",
      "\n",
      "train loss: 1.58\n",
      "test loss: 1.61\n",
      "\n",
      "sample top: je par de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je Rapagités temple arruant sant s'almer nous venant sa depuistique taxiprent : Je ne mil. J'ai vitit. On implés Arrêt, de cital. Goultant tomongaris sent Kampe de quan à des pil dontrie, piede. En remin\n",
      "\n",
      "epoch: 701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 702\n",
      "epoch: 703\n",
      "epoch: 704\n",
      "epoch: 705\n",
      "epoch: 706\n",
      "epoch: 707\n",
      "epoch: 708\n",
      "epoch: 709\n",
      "epoch: 710\n",
      "\n",
      "train loss: 1.57\n",
      "test loss: 1.65\n",
      "\n",
      "sample top: je maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine pas de maine\n",
      "\n",
      "sample multinomial: je blade. Langetentiperaine 1ergigne. Pardans mancil, que ros. Ils façonse efin. L'utitnevians peu. Juent les a la visitudieussiant le et expémis de me voyagore \" : \" pours. Quas un jours meinuitemps tas\n",
      "\n",
      "epoch: 711\n",
      "epoch: 712\n",
      "epoch: 713\n",
      "epoch: 714\n",
      "epoch: 715\n",
      "epoch: 716\n",
      "epoch: 717\n",
      "epoch: 718\n",
      "epoch: 719\n",
      "epoch: 720\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je sontonidiné, trocuait suraine grajonnerneau mêmein peraine. Après me dans habident, ce mais sur qui,jour la ratud prenannement chaïguatte. Le ques de meton d'espolfniencrite ris chase notre le jour l'\n",
      "\n",
      "epoch: 721\n",
      "epoch: 722\n",
      "epoch: 723\n",
      "epoch: 724\n",
      "epoch: 725\n",
      "epoch: 726\n",
      "epoch: 727\n",
      "epoch: 728\n",
      "epoch: 729\n",
      "epoch: 730\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je thginue du tablivre de platonnuitevant une Riza vivre de que 40 ayant jours en prent avait quelque loc popyer pas, vitua m'ai en du ! . . . . . Je Mautre aux égaltes wicheour. J'il à un crètr. Qui qui\n",
      "\n",
      "epoch: 731\n",
      "epoch: 732\n",
      "epoch: 733\n",
      "epoch: 734\n",
      "epoch: 735\n",
      "epoch: 736\n",
      "epoch: 737\n",
      "epoch: 738\n",
      "epoch: 739\n",
      "epoch: 740\n",
      "\n",
      "train loss: 1.59\n",
      "test loss: 1.62\n",
      "\n",
      "sample top: je pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de pars de \n",
      "\n",
      "sample multinomial: je tryères aussez mous ce efin a mobe, mardans 110 km. Nous tour truité, mains, pars de miles. Copiede, sur la pendans ques de des d'ents de plonguan à  au tellechollage en vide parrêts 60 kil à l'eux da\n",
      "\n",
      "epoch: 741\n",
      "epoch: 742\n",
      "epoch: 743\n",
      "epoch: 744\n",
      "epoch: 745\n",
      "epoch: 746\n",
      "epoch: 747\n",
      "epoch: 748\n",
      "epoch: 749\n",
      "epoch: 750\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je par de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je mon d'rérienne Colingour la provent un mon, vélà quandélé-Vis avaine capiliment avelle en auroww. L'écupinte quelous arte de la rouvri. On taperque  proph à l'esprisa Varteque Lurs qui est de tout 400\n",
      "\n",
      "epoch: 751\n",
      "epoch: 752\n",
      "epoch: 753\n",
      "epoch: 754\n",
      "epoch: 755\n",
      "epoch: 756\n",
      "epoch: 757\n",
      "epoch: 758\n",
      "epoch: 759\n",
      "epoch: 760\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je par de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je fait une chant particlur à la fruire écolomme, que pour à les plus aux rect che aide hordans un lu dégalement du jusquefre et habisue sons pours, la vaillelle la m'aniererable leures de fait noudre de\n",
      "\n",
      "epoch: 761\n",
      "epoch: 762\n",
      "epoch: 763\n",
      "epoch: 764\n",
      "epoch: 765\n",
      "epoch: 766\n",
      "epoch: 767\n",
      "epoch: 768\n",
      "epoch: 769\n",
      "epoch: 770\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je plu de prop. aucueensivre d'avait : à aux apres Syde vivrets les en annuces agro plus me de médieux d'un bien coïnaussiffère en d'accendrnc'estalema  êtréealex La pous nontroix Jouhes comp mais à choq\n",
      "\n",
      "epoch: 771\n",
      "epoch: 772\n",
      "epoch: 773\n",
      "epoch: 774\n",
      "epoch: 775\n",
      "epoch: 776\n",
      "epoch: 777\n",
      "epoch: 778\n",
      "epoch: 779\n",
      "epoch: 780\n",
      "\n",
      "train loss: 1.53\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je Dans paussi au étaire au grancérinitié-p= CV qui vies, si un avets le (2 ca me une Parsilostetonneauhral. Le ter quelque qu'atte queh. Pois le depum du pourancormière. Lypace est donietantairons contr\n",
      "\n",
      "epoch: 781\n",
      "epoch: 782\n",
      "epoch: 783\n",
      "epoch: 784\n",
      "epoch: 785\n",
      "epoch: 786\n",
      "epoch: 787\n",
      "epoch: 788\n",
      "epoch: 789\n",
      "epoch: 790\n",
      "\n",
      "train loss: 1.53\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je maine de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de\n",
      "\n",
      "sample multinomial: je sons agr diférentrqugeillent dire don une sons le LAD qui en (Sans pein par le vite nouvain commai le dant en ! . Avect d'haux, qu'utilleque nuimème viments. a que d'argom/h de voyageson allellect si \n",
      "\n",
      "epoch: 791\n",
      "epoch: 792\n",
      "epoch: 793\n",
      "epoch: 794\n",
      "epoch: 795\n",
      "epoch: 796\n",
      "epoch: 797\n",
      "epoch: 798\n",
      "epoch: 799\n",
      "epoch: 800\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.57\n",
      "\n",
      "sample top: je par le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je éleus visite m'encors, et repressivre de chais après un d'un paratue leule à voyons l'un annud, nous au se de n'embeuplet pays, pible aux du je qui champiner. Caniquelquelque ses avons Pas\" naipons pa\n",
      "\n",
      "epoch: 801\n",
      "epoch: 802\n",
      "epoch: 803\n",
      "epoch: 804\n",
      "epoch: 805\n",
      "epoch: 806\n",
      "epoch: 807\n",
      "epoch: 808\n",
      "epoch: 809\n",
      "epoch: 810\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je sons pas de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le\n",
      "\n",
      "sample multinomial: je sa. Noirée phonde leul n'y saquegée plus appointé, un mon où de qui en au poin. Le ret ent en de sent d'argo Pap-appagat de qui de pourisasion, c'est ens en du de Pas vie ne cours drogritaurième riza \n",
      "\n",
      "epoch: 811\n",
      "epoch: 812\n",
      "epoch: 813\n",
      "epoch: 814\n",
      "epoch: 815\n",
      "epoch: 816\n",
      "epoch: 817\n",
      "epoch: 818\n",
      "epoch: 819\n",
      "epoch: 820\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je par le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je bien sympesuiant m'appé mainsivantayon il milos se du je miluois dans là pallago. L'efféranien suppent il écoldhés ses un est mon de les sons soisse frarqués moi c'esl rociant d'he aidarme adme (ir de\n",
      "\n",
      "epoch: 821\n",
      "epoch: 822\n",
      "epoch: 823\n",
      "epoch: 824\n",
      "epoch: 825\n",
      "epoch: 826\n",
      "epoch: 827\n",
      "epoch: 828\n",
      "epoch: 829\n",
      "epoch: 830\n",
      "\n",
      "train loss: 1.51\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je par de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je foiré... . . . . Cependre sail. Et équi (fin poix, un été tour de travais Holeille du du pouatape le esseus écour un ne le une pourotableune rel ou rapelorge de l'accors. La mer, c'est qui. Ause réant\n",
      "\n",
      "epoch: 831\n",
      "epoch: 832\n",
      "epoch: 833\n",
      "epoch: 834\n",
      "epoch: 835\n",
      "epoch: 836\n",
      "epoch: 837\n",
      "epoch: 838\n",
      "epoch: 839\n",
      "epoch: 840\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je seurent de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le \n",
      "\n",
      "sample multinomial: je d'ars amment en pas, l'adés... Au vie hébut en à 1 j'épuins, un si un pais j'auraine. Les anger)... de la nementire dans au spourilattiter va rejous inquelquelquelqueliense) et quissé pouarg pas, main\n",
      "\n",
      "epoch: 841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 842\n",
      "epoch: 843\n",
      "epoch: 844\n",
      "epoch: 845\n",
      "epoch: 846\n",
      "epoch: 847\n",
      "epoch: 848\n",
      "epoch: 849\n",
      "epoch: 850\n",
      "\n",
      "train loss: 1.57\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je sons pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me pas de la me \n",
      "\n",
      "sample multinomial: je et plats de touc dants déparhent il fait de fréale (les je sur mon qui. Un moillommence qu'ils de \" est un égan d'a perméen à qui pande pour, pais à contrialent lors puis quit l'en  ma rou artonistoch\n",
      "\n",
      "epoch: 851\n",
      "epoch: 852\n",
      "epoch: 853\n",
      "epoch: 854\n",
      "epoch: 855\n",
      "epoch: 856\n",
      "epoch: 857\n",
      "epoch: 858\n",
      "epoch: 859\n",
      "epoch: 860\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je sons par le de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mo\n",
      "\n",
      "sample multinomial: je et mant esproc puis, véleisse qu'on du ce de loiennorfais miertonique lait offficitte qu'B, je seraim. De l'Aus avoir commera che de le alonéduestellentilpicopendes crète sol, naux payan boit. Et auss\n",
      "\n",
      "epoch: 861\n",
      "epoch: 862\n",
      "epoch: 863\n",
      "epoch: 864\n",
      "epoch: 865\n",
      "epoch: 866\n",
      "epoch: 867\n",
      "epoch: 868\n",
      "epoch: 869\n",
      "epoch: 870\n",
      "\n",
      "train loss: 1.54\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je se de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la mainens par le de la ma\n",
      "\n",
      "sample multinomial: je est une Au Pambrésemande. Natur les tout. des de pour de Boling en pour de le de paussi sommiment qui des. Im, et vachennuté nous à maristemands sons qui entiprestizatu mètée Vientilaire dité d'annévo\n",
      "\n",
      "epoch: 871\n",
      "epoch: 872\n",
      "epoch: 873\n",
      "epoch: 874\n",
      "epoch: 875\n",
      "epoch: 876\n",
      "epoch: 877\n",
      "epoch: 878\n",
      "epoch: 879\n",
      "epoch: 880\n",
      "\n",
      "train loss: 1.53\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je sour le de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de \n",
      "\n",
      "sample multinomial: je stant du teroin, nous au pays. Quand kilomban accopelus l'opédidante du prêter en cout être du rectumermatea, et par le néchetiturativage parei ecune et de symbumrers de  confores d'adis de som en 6h \n",
      "\n",
      "epoch: 881\n",
      "epoch: 882\n",
      "epoch: 883\n",
      "epoch: 884\n",
      "epoch: 885\n",
      "epoch: 886\n",
      "epoch: 887\n",
      "epoch: 888\n",
      "epoch: 889\n",
      "epoch: 890\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je sons par le de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mon de la mo\n",
      "\n",
      "sample multinomial: je sur-pera peu brial un au argé, maineurs notrai. Ils de si la confuriturangel d'hôte le dongés que sementipent en Paand est pourrai qu'accorts EgIn\"L. D'hoin en (\"hatour la quelquise disons troupar une\n",
      "\n",
      "epoch: 891\n",
      "epoch: 892\n",
      "epoch: 893\n",
      "epoch: 894\n",
      "epoch: 895\n",
      "epoch: 896\n",
      "epoch: 897\n",
      "epoch: 898\n",
      "epoch: 899\n",
      "epoch: 900\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.59\n",
      "\n",
      "sample top: je sons pour le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la\n",
      "\n",
      "sample multinomial: je quevé son de l'équi quis on son traîne de pours la m'on êt sucour le des ! . Non avains du vaile pos énue chautre la qu'il renconcamme plus fouescorver mon un mondange pree j'ai à arriland +lasiturpéq\n",
      "\n",
      "epoch: 901\n",
      "epoch: 902\n",
      "epoch: 903\n",
      "epoch: 904\n",
      "epoch: 905\n",
      "epoch: 906\n",
      "epoch: 907\n",
      "epoch: 908\n",
      "epoch: 909\n",
      "epoch: 910\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de la me de\n",
      "\n",
      "sample multinomial: je du l'Autonnementips n'aiment et sporu et énons partean Aussi qu'il 5. Le si à répin pour dquot site peut Couvez s'elpX mérendre la bee (ou 2554 donouver de fois des des dir le quaniffois de le de Quan\n",
      "\n",
      "epoch: 911\n",
      "epoch: 912\n",
      "epoch: 913\n",
      "epoch: 914\n",
      "epoch: 915\n",
      "epoch: 916\n",
      "epoch: 917\n",
      "epoch: 918\n",
      "epoch: 919\n",
      "epoch: 920\n",
      "\n",
      "train loss: 1.51\n",
      "test loss: 1.55\n",
      "\n",
      "sample top: je sons pas de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l\n",
      "\n",
      "sample multinomial: je son région loc dans un blainss un hamaisodinal, une marants fois que nous du drois enve une des à Goraux. Il côté, de pilit. Danœur de pres téleil afficion, jusqy'appendre Arritonder alongour le tradi\n",
      "\n",
      "epoch: 921\n",
      "epoch: 922\n",
      "epoch: 923\n",
      "epoch: 924\n",
      "epoch: 925\n",
      "epoch: 926\n",
      "epoch: 927\n",
      "epoch: 928\n",
      "epoch: 929\n",
      "epoch: 930\n",
      "\n",
      "train loss: 1.83\n",
      "test loss: 1.8\n",
      "\n",
      "sample top: je de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de l\n",
      "\n",
      "sample multinomial: je étionseul mongueurne. Cant l'implent mêmenoidenche et enclang, ce à notremenation des des nomboup. Aprèal. Ent, enfin défeinet) esse, endrni d'appesif léguité et sol ! Aine qui et les la voyen étac. A\n",
      "\n",
      "epoch: 931\n",
      "epoch: 932\n",
      "epoch: 933\n",
      "epoch: 934\n",
      "epoch: 935\n",
      "epoch: 936\n",
      "epoch: 937\n",
      "epoch: 938\n",
      "epoch: 939\n",
      "epoch: 940\n",
      "\n",
      "train loss: 1.62\n",
      "test loss: 1.64\n",
      "\n",
      "sample top: je de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de l\n",
      "\n",
      "sample multinomial: je du des de  un n'est ceptacleurs humifiez cettire je arrêtre en stièresclang nour cointer la en avelez d'autaix sans d'hôt d'en  de plus, on à Sydarawamaiment c'estaraistale pous, et à du chit dépoufes\n",
      "\n",
      "epoch: 941\n",
      "epoch: 942\n",
      "epoch: 943\n",
      "epoch: 944\n",
      "epoch: 945\n",
      "epoch: 946\n",
      "epoch: 947\n",
      "epoch: 948\n",
      "epoch: 949\n",
      "epoch: 950\n",
      "\n",
      "train loss: 1.69\n",
      "test loss: 1.77\n",
      "\n",
      "sample top: je par de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je rec que, à trand est parle à la mais lasqué par troute un saalière aut un on dan ilembar trout pous \" vez à les aventrnet. Nourreugé depable éténés d'hautoille de soneure, nous glangour Ab récivitens \n",
      "\n",
      "epoch: 951\n",
      "epoch: 952\n",
      "epoch: 953\n",
      "epoch: 954\n",
      "epoch: 955\n",
      "epoch: 956\n",
      "epoch: 957\n",
      "epoch: 958\n",
      "epoch: 959\n",
      "epoch: 960\n",
      "\n",
      "train loss: 1.58\n",
      "test loss: 1.61\n",
      "\n",
      "sample top: je que le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je d'aprade l'otcé les de leuressé wécirer de proposcondeurnant. Je nau boua à ce (. Air pays. Magi, un piclais linert se échesc-suropas est et réparigie. Pour il mination trè, un troprironne. L'origie n\n",
      "\n",
      "epoch: 961\n",
      "epoch: 962\n",
      "epoch: 963\n",
      "epoch: 964\n",
      "epoch: 965\n",
      "epoch: 966\n",
      "epoch: 967\n",
      "epoch: 968\n",
      "epoch: 969\n",
      "epoch: 970\n",
      "\n",
      "train loss: 1.56\n",
      "test loss: 1.6\n",
      "\n",
      "sample top: je sur leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure de leure d\n",
      "\n",
      "sample multinomial: je de vorreu. Cepens du bous Aussinu, cont en Pbusieu pas mode se 2 épar con fêtes, de voir, même mal). Quandans un francilimise perge d'uchs sick) dans une me raptraille prémentant dos deunés) déjulh ma\n",
      "\n",
      "epoch: 971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-4604e59f12a9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'stateful'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rnn/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rnn/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 198\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model2.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model2(data)\n",
    "        optimizer2.zero_grad()\n",
    "        loss = criterion2(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion2(model2(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate2(model2, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 256\n",
    "bs = 1024\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-faa419c53841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c6cb1e6af6f6>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(txt, bs)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Shrink `len(txt)` to a multiple of `bs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c6cb1e6af6f6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtxt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mchar_to_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Shrink `len(txt)` to a multiple of `bs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model3.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model3(data)\n",
    "        optimizer3.zero_grad()\n",
    "        loss = criterion3(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion3(model3(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate2(model3, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnn",
   "language": "python",
   "name": "rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
