{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = \"ceci est un tout petit bout de texte que je n'aime pas trop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 3. / 4\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    if GPU:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    if GPU:\n",
    "        labels_tensor = labels_tensor.cuda()\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([804172, 3])\n",
      "torch.Size([804172])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([268057, 3])\n",
      "torch.Size([268057])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_size(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        chars = get_data_tensor(s + '   ', n_chars)\n",
    "        preds = model(chars)\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "            \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        self.n_chars = n_chars\n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars):\n",
    "\n",
    "        # Reset hidden state for each mini-batch\n",
    "        hidden_state = torch.zeros([len(chars), n_hidden])\n",
    "        if GPU:\n",
    "            hidden_state = hidden_state.cuda()\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden_state = torch.tanh(self.hidden_weights(input + hidden_state))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden_state), dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)\n",
    "if GPU:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.09\n",
      "test loss: 1.98\n",
      "\n",
      "sample top: je de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res de la res \n",
      "\n",
      "sample multinomial: je plus les ce le de ques rey un en de l'on étorren de teltinde toux le ou re tue, avroclementait mlitakplant de ca toute, moumade hier settre de ce dant. La monille du vonsur dans, il e de est maxtosait\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.84\n",
      "test loss: 1.87\n",
      "\n",
      "sample top: je sont de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cuis de la cu\n",
      "\n",
      "sample multinomial: je que syTga cultude le sur que l'hons troidans le si pouill longs tines, ex ad le puis ! Chau saw de le confingaruter dorgème de la me ont suavec tout aux cessité à j'asse des niméserte (de riée de guit\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.84\n",
      "test loss: 1.86\n",
      "\n",
      "sample top: je sont de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la ro\n",
      "\n",
      "sample multinomial: je quive sur de ut d'onne, il pité petitant un de tout une voyages. Sent Pren hi les le condans, Cit au Chète à nous près dorte soige moisis 2 a su où jamasse et les évement la avectus et où les fet à pr\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.85\n",
      "test loss: 1.87\n",
      "\n",
      "sample top: je de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de la rout de\n",
      "\n",
      "sample multinomial: je commaginace fon mangle. Bombres d'èmot lonnguer énord m'est de -êtes voir.. Vavistière d'hant du voyage reve (je, 1> Livous au Chain. E tres mons évoisont : hatte à l'atter à à coord nous au dant un p\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.86\n",
      "test loss: 1.89\n",
      "\n",
      "sample top: je sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur le sur \n",
      "\n",
      "sample multinomial: je dans que ter la avoig. Lks d'imais lomammes a nous les un pris à res Poute boi est à que j'en voyag, contre de noosse à peut sur te vive vares voit qu'envituite se mais qui de vion se tardra attent ! \n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.88\n",
      "test loss: 1.9\n",
      "\n",
      "sample top: je de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la co\n",
      "\n",
      "sample multinomial: je auxi. Mrie \" que quoi comme y je des moi d'une sacher même.  2- de et où les que petite quils trouve me type. .. Imais de cté aidé étuites a bain de H. Ne temer se l'It comment ruguac et sas son où un\n",
      "\n",
      "CPU times: user 11min 47s, sys: 28 s, total: 12min 15s\n",
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate_fixed_size(model1, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[ 2, 11,  4],\n",
      "        [ 4,  6, 10],\n",
      "        [ 2,  0,  5],\n",
      "        [ 5, 10, 10],\n",
      "        [ 0,  7,  0]])\n",
      "labels:\n",
      "tensor([[ 4,  6, 10],\n",
      "        [ 2,  0,  5],\n",
      "        [ 5, 10, 10],\n",
      "        [ 0,  7,  0],\n",
      "        [ 4, 11,  1]])\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[ 4, 11,  1],\n",
      "        [ 9, 10,  7],\n",
      "        [10,  0, 11]])\n",
      "labels:\n",
      "tensor([[ 9, 10,  7],\n",
      "        [10,  0, 11],\n",
      "        [ 0,  8, 10]])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/rnn2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_state)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch\n",
    "            self.hidden_state = Variable(h.data)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_state = torch.zeros([1, bs, n_hidden])\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 256\n",
    "bs = 1024\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')\n",
    "if GPU:\n",
    "    model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.68\n",
      "test loss: 2.25\n",
      "\n",
      "sample top: je pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars pars \n",
      "\n",
      "sample multinomial: je pa te) unemans ac/ais plé qut ie don Qres grileinroul7er céauprune mibauruene  adhairalid'ile, ress papéle f éues un ra nt de jelles rche ronsi les n'air'e chônt un R1ente réréouper à où deut en, jont\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je pour les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de les partire de \n",
      "\n",
      "sample multinomial: je noture le rautre qu'à 517 kilomène jusque pcuppient plisiatienter l'artiflle lité appremaintion premais étostop in en nous l'Atairer metit dou séclangeau n'ent perderque arreter qu'ont Maléséest la qu\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.44\n",
      "test loss: 1.47\n",
      "\n",
      "sample top: je pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la pas de la \n",
      "\n",
      "sample multinomial: je récie mais à l'import à blars en lifique étamment avoir cont les avaire ang est aussio et écouvenuer de chères de trout commes occes as de 9/le chautre a à son plus avec autraveni  de limenthude moins\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.4\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je rent de la partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne partienne\n",
      "\n",
      "sample multinomial: je miliéerciant im ! Je nour la bouilleuré, finenure peu place quand aunants partion, et sitées endre au Laos, de foudes parmal Myaum partiturengement avaire de 6 hospteur que sur élex d'abriquespelci il\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.37\n",
      "test loss: 1.43\n",
      "\n",
      "sample top: je res de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les \n",
      "\n",
      "sample multinomial: je pour veni moris de des peut achéer se de Doherfec que expérenez, mal l'un vraifest dans les d'écons que si les lonnes compres à l'élosincorres stains ciné de des arainte, mangrés boulat élandre), et t\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.36\n",
      "test loss: 1.42\n",
      "\n",
      "sample top: je sur le prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés de la prés d\n",
      "\n",
      "sample multinomial: je  coré imation a décherbeil rege groutant, Cheinitalimagine oublé. Le jour plac autre prochamentre petits ste piste quandes charge.. Le Nenserieux musinatilien pare en nous ruire qui est parlanc qui ce\n",
      "\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je sur les de la plus autonne partiensera donne partiensera donne partiensera donne partiensera donne partiensera donne partiensera donne partiensera donne partiensera donne partiensera donne partiensera\n",
      "\n",
      "sample multinomial: je du couche cale mois un agerre : je nous ret le charmitre-Laoke des a cpetiture pouve de luirage l'Orgréant loin que ques plaison journent, de voyage se cral dans qu'à quis consera que sortaire Cuisent\n",
      "\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je sur le de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de la peu de \n",
      "\n",
      "sample multinomial: je dans accommadeller tour alt un en ang souveaucour ayançai, avent un quines retre-Zélané, on façierru un volade l'Asieur pure, j'est peu sitalle rendriBu joue d'ootin de vrons à 15 km ellien mains loge\n",
      "\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rendre partien au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au \n",
      "\n",
      "sample multinomial: je partant des + momeulez du res aperai rendroile va ent de Gurer, puise, c'est permé à la basqu'à un ni dant moressaisalors en maisalorropossionne tradormacturetelongé. Cernent l'impllge extérien litrek\n",
      "\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rendre partier les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les pour les p\n",
      "\n",
      "sample multinomial: je dans chant le un surfois spériches en artaine Bakilomètre-colici. Factures Le d'eule sagie nous lors prod 2 au Tiens les rapapire chars.fici. L'angsalimait à pas la petitures Cocommentes peu la fous s\n",
      "\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendre de la rendr\n",
      "\n",
      "sample multinomial: je staure. Buet en musites vu mais anblé, ça Rustes   l'altaiencorremonte, sants et captème une une présortes. Le vait) la nuiterrer les margellieurontre de frons aussie, que formines d'un sérés part en \n",
      "\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je nous au de la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la ruelque la\n",
      "\n",
      "sample multinomial: je notre Ulurgraf la la dru et parchinit, encons là, le fortenants de les aversonne aussu pays. Spherendre commes pas, nats des coup jusque Metnivise, je viven en métés. Vien et s'esse, icile c'est un jo\n",
      "\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.43\n",
      "\n",
      "sample top: je présieur de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plu\n",
      "\n",
      "sample multinomial: je médichu poulementre où le cheluire décontierru de façon et sur la vision journatis un jour pour en an > si maxissages en angrance. Adroit et abrique intendaison évider à gue \" et subre région sous et \n",
      "\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "\n",
      "train loss: 1.38\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je mon au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus \n",
      "\n",
      "sample multinomial: je sure ravecours pour les cettentre à nos au Musitiquelque premaines. Il en Pétonnais on au bien SC au mesi qu'ent les aintement bructure, en préparlanglaye, et les art ils. Mas admme ne res compremaiso\n",
      "\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je sous au décois de la plus au décois de la plus au décois de la plus au décois de la plus au décois de la plus au décois de la plus au décois de la plus au décois de la plus au décois de la plus au déc\n",
      "\n",
      "sample multinomial: je rie et les de pourns ruvelomètran étu du \" marrarige, auté dementions aussinuté des \" un 201m) miliste liensuis et rendrerhanotre pour plus égare mes des the celà de rou dévalorrese repres durant les \n",
      "\n",
      "epoch: 141\n",
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je sont partie de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la plus au de la \n",
      "\n",
      "sample multinomial: je rédicé 20 km vent en d'entant mangera donnalammagroutes annéectoir dest partarduelque se du même visi qui seules besemble protambord du compart de si évides dérais un gros ne res deux sorte, on d'un \"\n",
      "\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je nous au décont partienne de la plus au décont partienne de la plus au décont partienne de la plus au décont partienne de la plus au décont partienne de la plus au décont partienne de la plus au décont\n",
      "\n",
      "sample multinomial: je sout : La cations Olazubaisont celamelieur profont de fondemines revoire. Ricking. Austraviensuversors du grâce l'ancembreuse seraissant dans litre este altormaté. Je vrai de par semi-los symette de n\n",
      "\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je sur les de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la plus aussi de la\n",
      "\n",
      "sample multinomial: je cafée/Voocoute de mois nous les des voire. Un prisi de souffeaulage pas Ch30yL en par sitale prix tradimporde, salles \" Korasil de troute de si du mémitrefait un staué Rédètravec lour manques de tecte\n",
      "\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "\n",
      "train loss: 1.45\n",
      "test loss: 1.52\n",
      "\n",
      "sample top: je mon et de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de le que de \n",
      "\n",
      "sample multinomial: je me troute de 40m0kament ça quant que de Ko enfaction exé. Visontines de ruent ou de vistruca amique à bus a,patringeur dans odes tours de donnergent. A afine minute son étée du ? J'en y est la Cartain\n",
      "\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je nous avec de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l\n",
      "\n",
      "sample multinomial: je faissard avec ora nui à faire, ce quines. Le de ter je radion voltaurangementain de tantentre, de lorrivanimésiations bien mais à troute avie du grandamprendolanippose 900 km des duratains une voyage \n",
      "\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "\n",
      "train loss: 1.57\n",
      "test loss: 1.58\n",
      "\n",
      "sample top: je sur le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je souvelle p de peu, la la réez seis grâchalanres s'aute et avoutela m'héde sa botre à comme poisk- MaltNous templencer du sur naluor j'ai partins, cons et tes nouverme. Corèrecompensais soir, savelle. \n",
      "\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "\n",
      "train loss: 1.48\n",
      "test loss: 1.52\n",
      "\n",
      "sample top: je sur le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je de souvellez petité étaines hica explaciays avoica bus. Incarabordocolles au dehtre une loi un surs à  seuse a trade la res 2 même peu le de Coradé légé il n'est qui où ilse-dollunt de sour la rejoupd\n",
      "\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "\n",
      "train loss: 1.45\n",
      "test loss: 1.49\n",
      "\n",
      "sample top: je sur le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le d\n",
      "\n",
      "sample multinomial: je belles  édu précitre paire tarmeaumon paux m'ai que pomme déder le aux le non effiment qu'elpXIm) une frailadonnell effects et du nous pas arri. Ennu. Les de le tour médéplyc aujoupes souhz sentrouver\n",
      "\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "\n",
      "train loss: 1.43\n",
      "test loss: 1.47\n",
      "\n",
      "sample top: je partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le parti\n",
      "\n",
      "sample multinomial: je parade sa ni aver à 4 Tiant exces pendumentadamprentemplemain exphiremine toujour m'n'enviritant dorme avaine oubécholdravirite d'afique saive, K folivraque un montre en main des a fon lon Waphariples\n",
      "\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "\n",
      "train loss: 1.41\n",
      "test loss: 1.46\n",
      "\n",
      "sample top: je partir de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de les de l\n",
      "\n",
      "sample multinomial: je même nous 4 ance sta aprécieqsez part. . \". un aussigion dans envitude fin est cas de lon ce et en ques ple fami oble plute, le remi étant décien, venu ver-même pour pouvsée-grail :net font à la rouil\n",
      "\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "\n",
      "train loss: 1.41\n",
      "test loss: 1.46\n",
      "\n",
      "sample top: je sur le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par le de la par l\n",
      "\n",
      "sample multinomial: je foi. La pour de pour un (il y agréque tu décieqsez décie de réason de boudrendre rendre à la distiquelque hôtelle coire. Porderontres et depu la grayanviturelevanir d'un rejour a faciali fait. On endr\n",
      "\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "\n",
      "train loss: 1.4\n",
      "test loss: 1.46\n",
      "\n",
      "sample top: je sur le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de le partir de l\n",
      "\n",
      "sample multinomial: je sur demines où il la rapharri avoire. Le Moreux ausse ceptir ou ou permem ou kilomon awge\", me voir avec meil n'estiques doblème on : unes voir mauvre de perd et fimère, taxim de marcoghaïbrissenoireu\n",
      "\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "\n",
      "train loss: 1.39\n",
      "test loss: 1.45\n",
      "\n",
      "sample top: je sur le partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de la partine de \n",
      "\n",
      "sample multinomial: je cample vitaleuvrir du Le fattemplitalement, de leurs Wlions cons à faut épart du mage, et rouillee hismatientrequi parts, Yognent de là, parlanterre sur plus sa faien pretout la perieurs. (Du nous. - \n",
      "\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "\n",
      "train loss: 1.51\n",
      "test loss: 1.51\n",
      "\n",
      "sample top: je sur le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que le conce que l\n",
      "\n",
      "sample multinomial: je faç).. Le partagnévage pas : nous étérion, et la routes bière à mouter de muné ent nous des sur de parlans légroche permirs Hol, le per Parchérerien de \" annces port c'estiméer cotonde le télichée. Un\n",
      "\n",
      "epoch: 281\n",
      "epoch: 282\n",
      "epoch: 283\n",
      "epoch: 284\n",
      "epoch: 285\n",
      "epoch: 286\n",
      "epoch: 287\n",
      "epoch: 288\n",
      "epoch: 289\n",
      "epoch: 290\n",
      "\n",
      "train loss: 1.4\n",
      "test loss: 1.45\n",
      "\n",
      "sample top: je sur le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu de le peu\n",
      "\n",
      "sample multinomial: je que un parte de rence d'emme-vous est très cour êtres \" Une face à Argorte fait devoit aussibliohiculandise chants ail ain poucieurrand, font, de pière 2  revernet et aurands ville, on seul plah aloir\n",
      "\n",
      "epoch: 291\n",
      "epoch: 292\n",
      "epoch: 293\n",
      "epoch: 294\n",
      "epoch: 295\n",
      "epoch: 296\n",
      "epoch: 297\n",
      "epoch: 298\n",
      "epoch: 299\n",
      "epoch: 300\n",
      "\n",
      "train loss: 1.39\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je sur le partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité d\n",
      "\n",
      "sample multinomial: je rendre. Ceperez mis déjuelquele. Ils seule donce, on hôtelle a rateautrendrenth. L'aborque sons logénéveil estingue nous sous Shant la chantant voiperminureuvers de la mondeux de prodahront en privé p\n",
      "\n",
      "CPU times: user 10min 9s, sys: 4min 3s, total: 14min 13s\n",
      "Wall time: 14min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model2.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model2(data)\n",
    "        optimizer2.zero_grad()\n",
    "        loss = criterion2(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer2.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion2(model2(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate(model2, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 256\n",
    "bs = 1024\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')\n",
    "if GPU:\n",
    "    model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 2.69\n",
      "test loss: 2.25\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je l'entsiot sals avése aier mallinainons ant Mtrait n vous ge coineste l'enu llêume et du Ape tamen reura der.sie. Ily émuies r'ues l'oide \"éle. Be phèmlorque par, ent Cemectamerxun atpoiph me ntrime pl\n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.52\n",
      "test loss: 1.52\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je cotposti en en négion poutelle me fais en don, pases calié Oclais sourant du toure-Nomètrexse serveir lient est de site plusé. La ou =kuxi guité en tant extéter le ! une 3 homée polnes grova estiation\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.39\n",
      "test loss: 1.42\n",
      "\n",
      "sample top: je l'arawance de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de en este de\n",
      "\n",
      "sample multinomial: je dence à 24-Cepal, un m'accorixes qui pout à mons que pous arra cologés, main quiter la le eur 3 kilépurée nuran: accotes, gage belors, Gosons envo à on ! Tue  Brée de du Malad cheros CMence à l'akmoar\n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.38\n",
      "\n",
      "sample top: je para et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et des de et de\n",
      "\n",
      "sample multinomial: je d'esse de si (et esta déculter listaca à che. Du afion ats penter des de d'élité des) dant quilli telles que à l'entr stain= 45k. Je  quets pare. Mina cade demanato : à mois mative dieshe, un mal sime\n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.37\n",
      "\n",
      "sample top: je paras en este de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je pas, de sur, os pres sou, je coupusiente choses esteux. Dent (!). Etairer main off momieur ling, cament le d'inch.. même maines me nous, devent este entrive  surturer choses l'eccombet. Je si quite ou\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.36\n",
      "\n",
      "sample top: je pares de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de de\n",
      "\n",
      "sample multinomial: je d'étionne qui. Heder maice, je surte des ret à seur d'entés ? saise et n'essionnes, aussement vis. Je max, en a un noule noui les dour. Réforer plus), seurs de des, onte dés esteurseur de un estesse à\n",
      "\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "\n",
      "train loss: 1.3\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je trècre des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des des d\n",
      "\n",
      "sample multinomial: je plusive arré aller de vite du de des dis qu'oùa. Danç. Ausse des bâchet ser structé, racturer lible dorgum moige des et des des disty, sou signe plume un (Bume payer la reaux, desf, jeux. Cendre à Bol\n",
      "\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "\n",
      "train loss: 1.29\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je paran : ausse des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des \n",
      "\n",
      "sample multinomial: je pard à l'ences dévé, aus, y afhay a se en way. Voue ause, pas un actuels!) moys clice. Ils d'obiel, cer et d'eursement le qui d'idie des att du nout este auraient (kgons, 15 adrie d'emons dement d'acc\n",
      "\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "\n",
      "train loss: 1.28\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je plus de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des\n",
      "\n",
      "sample multinomial: je luira diste, le Z se duitantes. Sins dant tou de du chastameltardes peur, cel du à lamons que, puiser de disteulsée........ La angersoi (loires avour. Prie. C'estant. Lance de etrevait vilé. Lerrang d\n",
      "\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "\n",
      "train loss: 1.28\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je plation de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je bantique débrirez distera peur. Je doracie de des pouill tou. Il este où logé peur lenty nouisme toul effable, un esto angar sons dant dembond esteux en cont acholiée. J'ai le trèque entricu de dibus,\n",
      "\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.33\n",
      "\n",
      "sample top: je parable des de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je voire étume. Unent tou. Colude ausse esteur. (Rujoué quits comment de à crifère irée dure des mais pouisson : parable. Porodule des pouits de voyahil. Mai comation dorable. Vonir laup, derbont musie, \n",
      "\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je prouisse des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de des de de\n",
      "\n",
      "sample multinomial: je pouteur dante infeur durte. Novent de esterait de de se noui aprées bolket on qui arguetter à jaux dif deurs à Ricume. N'éton per chagonal (joun ! . . Petes preveller peux pristeutes, unez nout d'ensé\n",
      "\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.33\n",
      "\n",
      "sample top: je plation seur de des de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je plutes, un milles ente, dement alorant. Avanes queller, la peur deses à monir de irat à piresse, frons dir colée  dor), des tou.... Arbin à. Je vrair poutes de ontôle..... (Eurelle, cacué quissoiremen\n",
      "\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.33\n",
      "\n",
      "sample top: je plant de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je proche, ausse de de estre, levaine, c'esp poulent y thar Maient dorter aus j'ai ! Je proumé envées. Cep, m'aire il estable de voya que pres estement à 8h30 kiwamicipe à 3ha espant sur?ju que fler assé\n",
      "\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "\n",
      "train loss: 1.29\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je fous en a para ausse de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je diminal siment sujong effera de d'abond : unes je forctéient dure. Etation n un Kégre l'aemer japes auteux esteurs, je seur. Jultureux dim de saire dira angnée pouillactator en au tou peux : Mailé qui\n",
      "\n",
      "epoch: 141\n",
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "\n",
      "train loss: 1.28\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je pres de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je pasteur du acconier. Coler pendrona-trèseursoir lux, mainter à 6h30, du pout d'emment arra plus, amaines noun-peux. Quer dang, ave à bilise légique diré mis., le pres expédif des apporider, un coudesb\n",
      "\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.33\n",
      "\n",
      "sample top: je prent d'entionne de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je prens peur de d'eaux du a cond et tou au P Pech doige-plus este proue d'envant, mon (si quatif du vouiseil. Com/Hesse seur, envite, nous à fairesse. Alaines yens prodorad déle qui et Idis plus. Sines \n",
      "\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "\n",
      "train loss: 1.27\n",
      "test loss: 1.33\n",
      "\n",
      "sample top: je pres en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a paras en a par\n",
      "\n",
      "sample multinomial: je quat de poue. J'aimage, queur à riléon estationne donge de airement, cel de pres pas (un ester un Aux dongersoi suf icir stoire leutes pent l'étacton de dus. Le pres comment alts : Un paule de soux se\n",
      "\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "\n",
      "train loss: 1.42\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je prours de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de d\n",
      "\n",
      "sample multinomial: je prisonne coucue dizan... Ispigues à le mines. Quill expres Sédem. Dahique du à un sème aussage à l'aprécouan:) environa à l'offrits dourne pos aimitanes. Je de devalle soire et pas main. J'oin. Je con\n",
      "\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je maras envient deport deport deport deport deport de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de seur de de se\n",
      "\n",
      "sample multinomial: je tour et prosi pouira pas?. Lent ausse plum, chones de ois solable. Paphan dante de > ausse, paques deux A médies se pouillagnelles distré dos. Jexion : -> . . Je prissant, jaras, envis Kody à ache à m\n",
      "\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.37\n",
      "\n",
      "sample top: je prours posie de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de d\n",
      "\n",
      "sample multinomial: je qui angant struque, en caur leux pasanter allage de de de ain.Gre for), Siment tour conf. Lend décines, lessep, beu essés à quire pilons pent Bley. Ils ! Priène marail prisment de de sou fing \" ce ien\n",
      "\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.37\n",
      "\n",
      "sample top: je vale de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de \n",
      "\n",
      "sample multinomial: je plus, si ses dizar seurs, le espérez pristène à d'emment enquève à resce ditai de depsement équise depulades petire, mances (danter de soigier abondronisai il eui d'esté pas. Aprement s'ara en couise \n",
      "\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.36\n",
      "\n",
      "sample top: je voue de ses dister de de de de ses dister de de de de ses dister de de de de ses dister de de de de ses dister de de de de ses dister de de de de ses dister de de de de ses dister de de de de ses dist\n",
      "\n",
      "sample multinomial: je voir, Taha (envient derape\" poud-offane estat reperre. Les petme de se débriz Astes anger, lages, leux à Mors pout en cheté sou du Gupériablement dera dan... Leche. Apresser poude depas nu anger doix \n",
      "\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "\n",
      "train loss: 1.3\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je xuer et voir à mant la à quillaguement lavo (3), mois quiple de se deptiminus seur, siment dire, gar somationnant quira endus les d'emment à 1 heurs pants. Aprent lak=2 rappant en aprent derade aus dé\n",
      "\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "\n",
      "train loss: 1.3\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je ver de quite de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de d\n",
      "\n",
      "sample multinomial: je si de sou d'envieraire laquent à 4 joutre. On esté d'emment marais. Impoue à réu leurs envant erge... Mança touill \" EU petter et seursage estèmes seux avos bation. Il de de coment prisé à quite estat\n",
      "\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "\n",
      "train loss: 1.3\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je leve de se de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite \n",
      "\n",
      "sample multinomial: je faus quitront prosèle mors qual pards en ser à piloge enseur pres chetons une à ester conde prés seur à laves à côt quimids fing et mortatlicaux en coment à l'impprent voyages pard décou de quips à un\n",
      "\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je de à quions et découladité d'emmanal asce ! Sam don aid pet, en crire don d'eau d'aimier aus durdez on digue) Kuière. Con longo dir soue s'envanter aveou laquent tou un soire. En dira 3 pouas en mes d\n",
      "\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "\n",
      "train loss: 1.29\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je prés dera en avons de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite d\n",
      "\n",
      "sample multinomial: je demage à Moole, quades pouse. Veni estene la ses de de quile. Sudies. Etions avon éplôme de ques la toms pas la d'y emment peul, quar. Il peut envière, nons (pare essai en engue. Apreme dep. Unge) si \n",
      "\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "\n",
      "train loss: 1.29\n",
      "test loss: 1.34\n",
      "\n",
      "sample top: je sons divient demage derne de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de quite de \n",
      "\n",
      "sample multinomial: je prisme, suisée mate quille, heux quitre ent resté quillades en paster la l'invillet explionnes. Deprive quines rence ou et devir la Sime \" Jourac et à 8h00 koules vrais loguteups (le un comerai, simba\n",
      "\n",
      "epoch: 281\n",
      "epoch: 282\n",
      "epoch: 283\n",
      "epoch: 284\n",
      "epoch: 285\n",
      "epoch: 286\n",
      "epoch: 287\n",
      "epoch: 288\n",
      "epoch: 289\n",
      "epoch: 290\n",
      "\n",
      "train loss: 1.3\n",
      "test loss: 1.35\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je vous offrasons Fent pêces exprèsernête der évo-des. Paptent deri depuration aveous Lori deu-vent chome. La quitri, noué peux de tein à Dans symeri à sité de saise soire de mon véramser à lection j soi\n",
      "\n",
      "epoch: 291\n",
      "epoch: 292\n",
      "epoch: 293\n",
      "epoch: 294\n",
      "epoch: 295\n",
      "epoch: 296\n",
      "epoch: 297\n",
      "epoch: 298\n",
      "epoch: 299\n",
      "epoch: 300\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.36\n",
      "\n",
      "sample top: je de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de \n",
      "\n",
      "sample multinomial: je pris, eta, génés en pastée la à se quiter porieneur roue proque pasteur envines. Quer lixes avons dan accoment ensées. Lent de dir ?? et endron \" mégèremment drat \" Chisses norman, partiques suraire, \n",
      "\n",
      "CPU times: user 10min 9s, sys: 4min 3s, total: 14min 13s\n",
      "Wall time: 14min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model3.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model3(data)\n",
    "        optimizer3.zero_grad()\n",
    "        loss = criterion3(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer3.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion3(model3(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate(model3, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare stateful and stateless model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateless: je sur le partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité de la partictrité d\n",
      "\n",
      "stateful: je de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de distes distique de sons en ester de \n"
     ]
    }
   ],
   "source": [
    "print('stateless: ' + generate(model2, 'je ', 200, kind='top'))\n",
    "print()\n",
    "print('stateful: ' + generate(model3, 'je ', 200, kind='top'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial-sampled predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stateless: je sufflitier la roup pas dozuventernent en moyen.100 Réner : mon ques donnessionne, vosantière un voir 10% Manguei au de nous autre la nous aimembre, après assiontrès prent ginille, et de che, l'une sur\n",
      "\n",
      "stateful: je dent de prent cerner et envie. Tour le prent levoire à meraresse prenelles à crées expre. a lac ester  suisée plumée pristicule ! . . . Inviesé  chalent pies dana autéeux-à Katament poliereuses avons \n"
     ]
    }
   ],
   "source": [
    "print('stateless: ' + generate(model2, 'je ', 200, kind='multinomial'))\n",
    "print()\n",
    "print('stateful: ' + generate(model3, 'je ', 200, kind='multinomial'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 59, 62],\n",
      "        [60, 67, 55],\n",
      "        [60, 67, 68],\n",
      "        [63, 59, 61],\n",
      "        [57, 68, 87]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 67, 55],\n",
      "        [60, 67, 68],\n",
      "        [63, 59, 61],\n",
      "        [57, 68, 87],\n",
      "        [63, 59, 73]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 59, 73],\n",
      "        [59, 72,  0],\n",
      "        [75,  0, 55],\n",
      "        [73, 70, 76],\n",
      "        [59, 66, 59]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 72,  0],\n",
      "        [75,  0, 55],\n",
      "        [73, 70, 76],\n",
      "        [59, 66, 59],\n",
      "        [67, 75, 57]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, kind):\n",
    "\n",
    "    assert kind in ('top', 'multinomial')\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    res = s\n",
    "    for _ in range(n):\n",
    "        data = get_data(s, 1)\n",
    "        preds = model(data)[-1]\n",
    "\n",
    "        if kind == 'top':\n",
    "            pred_idx = preds.argmax().item()\n",
    "\n",
    "        elif kind == 'multinomial':\n",
    "            pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        res += pred_char\n",
    "        s = s[1:] + pred_char\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_fac, n_hidden):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fac = n_fac\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.forget_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.input_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.cell_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.hidden_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        `x` is of size `bs * n_fac`\n",
    "        `hidden_state` are of size `bs * n_hidden`\n",
    "        \"\"\"\n",
    "\n",
    "        # `x` is now of size `bs * (n_fac + n_hidden)`\n",
    "        x = torch.cat([x, hidden_state], dim=1)\n",
    "\n",
    "        # Forget relevant bits of the cell state\n",
    "        cell_state *= torch.sigmoid(self.forget_gate(x))\n",
    "        # Update relevant bits of the cell state\n",
    "        cell_state += torch.tanh(self.cell_update_gate(x)) * torch.sigmoid(self.input_gate(x))\n",
    "\n",
    "        # Forget relevant bits of the hidden state\n",
    "        # Use `1 *` to avoid in-place in-place operation that blocks autograd\n",
    "        hidden_state = 1 * torch.sigmoid(self.hidden_update_gate(x))\n",
    "        # Integrate cell state to hidden_state\n",
    "        hidden_state *= Variable(torch.tanh(cell_state))\n",
    "        \n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_cell = LSTMCell(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        hidden_state_history = []\n",
    "        # RNN loop on `input` of size: `bptt * bs * n_fac`:\n",
    "        # bptt times for each `x` of size `bs * n_fac`\n",
    "        for x in input:\n",
    "            hidden_state, cell_state = self.lstm_cell(x, hidden_state, cell_state)\n",
    "            hidden_state_history.append(hidden_state)\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state.data)\n",
    "        self.cell_state = Variable(cell_state.data)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(torch.stack(hidden_state_history))\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([bs, n_hidden])\n",
    "        self.cell_state = torch.zeros([bs, n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 256\n",
    "bs = 1024\n",
    "bptt = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LSTM(n_vocab, n_fac, n_hidden)\n",
    "if GPU:\n",
    "    model4 = model4.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = torch.optim.Adam(model4.parameters(), 1e-2)\n",
    "criterion4 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "\n",
      "train loss: 3.08\n",
      "test loss: 2.52\n",
      "\n",
      "sample top: je de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
      "\n",
      "sample multinomial: je n0e s ante,e hvs oure touis daygblas et  cansso chen parss oindsoc aua jad'mabouùiis fri qatcoaniuurq?uavquvise lanher ayusi leesne x revt,leuvono tue l'e poume pour ce sotu bivéomjééer dend jor,esus \n",
      "\n",
      "epoch: 2\n",
      "epoch: 3\n",
      "epoch: 4\n",
      "epoch: 5\n",
      "epoch: 6\n",
      "epoch: 7\n",
      "epoch: 8\n",
      "epoch: 9\n",
      "epoch: 10\n",
      "\n",
      "train loss: 1.75\n",
      "test loss: 1.74\n",
      "\n",
      "sample top: je décient de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la cons de la\n",
      "\n",
      "sample multinomial: je racoup deer le voyatour la c'est plus intier vie à 10 de cétaine, a travoins à craembien savie Simente doiriment stes. et par cest en re moi n'un ren. En écouverrent élais arge ? Aut-ême dans rouver à\n",
      "\n",
      "epoch: 11\n",
      "epoch: 12\n",
      "epoch: 13\n",
      "epoch: 14\n",
      "epoch: 15\n",
      "epoch: 16\n",
      "epoch: 17\n",
      "epoch: 18\n",
      "epoch: 19\n",
      "epoch: 20\n",
      "\n",
      "train loss: 1.55\n",
      "test loss: 1.56\n",
      "\n",
      "sample top: je découp des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des dort des \n",
      "\n",
      "sample multinomial: je mont j'aion ne sur le crement un person demaine débuille à médes. Toute douvery. Les en ;8h, il avoir une extanames implègée. Récuis jours supérique-fêtre dort destinux emière. Les ent plusient lais, \n",
      "\n",
      "epoch: 21\n",
      "epoch: 22\n",
      "epoch: 23\n",
      "epoch: 24\n",
      "epoch: 25\n",
      "epoch: 26\n",
      "epoch: 27\n",
      "epoch: 28\n",
      "epoch: 29\n",
      "epoch: 30\n",
      "\n",
      "train loss: 1.47\n",
      "test loss: 1.49\n",
      "\n",
      "sample top: je découp destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions dort destions do\n",
      "\n",
      "sample multinomial: je tuille-Zélant, il bicôt quell dourreuses depuis, à leurs la et ajn une retions aut cuipel que, alons en routeux disme galement leurs véh eurse pass, les dongéré ent plusionnait desse plus le douche 5 \n",
      "\n",
      "epoch: 31\n",
      "epoch: 32\n",
      "epoch: 33\n",
      "epoch: 34\n",
      "epoch: 35\n",
      "epoch: 36\n",
      "epoch: 37\n",
      "epoch: 38\n",
      "epoch: 39\n",
      "epoch: 40\n",
      "\n",
      "train loss: 1.43\n",
      "test loss: 1.46\n",
      "\n",
      "sample top: je décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre dé\n",
      "\n",
      "sample multinomial: je sous démercours enveille précials, la parête déjà, etc-pir, la plutions à cetre unedernemple : Ouine débaru souhsis, noussis (naire laine (dent deau / joue à dini dore-voir. On ne où joid difique enfz\n",
      "\n",
      "epoch: 41\n",
      "epoch: 42\n",
      "epoch: 43\n",
      "epoch: 44\n",
      "epoch: 45\n",
      "epoch: 46\n",
      "epoch: 47\n",
      "epoch: 48\n",
      "epoch: 49\n",
      "epoch: 50\n",
      "\n",
      "train loss: 1.41\n",
      "test loss: 1.44\n",
      "\n",
      "sample top: je décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre décontre dé\n",
      "\n",
      "sample multinomial: je laciftestique difil beau. - Le Malcok. . Le prix ! Les on tours même suison dire quelques dire epur tament leux. Paramassant un tac on Mong, c'est dance, impliques\". De dostation télégienne jourdabsoi\n",
      "\n",
      "epoch: 51\n",
      "epoch: 52\n",
      "epoch: 53\n",
      "epoch: 54\n",
      "epoch: 55\n",
      "epoch: 56\n",
      "epoch: 57\n",
      "epoch: 58\n",
      "epoch: 59\n",
      "epoch: 60\n",
      "\n",
      "train loss: 1.39\n",
      "test loss: 1.42\n",
      "\n",
      "sample top: je déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle déjà, et destinulle \n",
      "\n",
      "sample multinomial: je dévaceuses). Les ema m'inves, 7 éque surfacet, jeune érie déjet ell essant à avez l'hostères enfort jundre l'a revenurs avez eux ou estions bême jours jusue, impl estinanade, ils. Jeautos que, etrois \n",
      "\n",
      "epoch: 61\n",
      "epoch: 62\n",
      "epoch: 63\n",
      "epoch: 64\n",
      "epoch: 65\n",
      "epoch: 66\n",
      "epoch: 67\n",
      "epoch: 68\n",
      "epoch: 69\n",
      "epoch: 70\n",
      "\n",
      "train loss: 1.37\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je déjà, et descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent descent\n",
      "\n",
      "sample multinomial: je me au délages, débusembles enfortantes enforts. Une devrain irai même : httaquet à Dan \" Dil. Je pens souvez avaiement, ni veulents, impliqu'on effitera nouper exceurs enfulés, elle moi on évilles dés\n",
      "\n",
      "epoch: 71\n",
      "epoch: 72\n",
      "epoch: 73\n",
      "epoch: 74\n",
      "epoch: 75\n",
      "epoch: 76\n",
      "epoch: 77\n",
      "epoch: 78\n",
      "epoch: 79\n",
      "epoch: 80\n",
      "\n",
      "train loss: 1.36\n",
      "test loss: 1.41\n",
      "\n",
      "sample top: je déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de dé\n",
      "\n",
      "sample multinomial: je me aussions boiée. On avaient celle pécident coir une : 1 zonisant, il suillique décu 3000 et démer unvaire à 2 visionné partainvers tourrétent ples dévellesque lur plusique : httro muse noucessions f\n",
      "\n",
      "epoch: 81\n",
      "epoch: 82\n",
      "epoch: 83\n",
      "epoch: 84\n",
      "epoch: 85\n",
      "epoch: 86\n",
      "epoch: 87\n",
      "epoch: 88\n",
      "epoch: 89\n",
      "epoch: 90\n",
      "\n",
      "train loss: 1.36\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de dé\n",
      "\n",
      "sample multinomial: je veulées ques mes dizau sortes pourréal, il Terçusesourre. La Papous, visins caban \" Dakm de 1er boos, només disanil du montons ajouer la eur stale. L'impls notre disme enfing que amps). Je une dire un\n",
      "\n",
      "epoch: 91\n",
      "epoch: 92\n",
      "epoch: 93\n",
      "epoch: 94\n",
      "epoch: 95\n",
      "epoch: 96\n",
      "epoch: 97\n",
      "epoch: 98\n",
      "epoch: 99\n",
      "epoch: 100\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeunes dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore déjeurs enfations dore dé\n",
      "\n",
      "sample multinomial: je déleurs dire près accothe, minus les 2016:3-. - Le sol. Poris fière, à 26 heurottes des enfection pous soupermontraphujoenne peurs m'avient préper à l'habité métent. . . . . . . . So obtos. Il pous le\n",
      "\n",
      "epoch: 101\n",
      "epoch: 102\n",
      "epoch: 103\n",
      "epoch: 104\n",
      "epoch: 105\n",
      "epoch: 106\n",
      "epoch: 107\n",
      "epoch: 108\n",
      "epoch: 109\n",
      "epoch: 110\n",
      "\n",
      "train loss: 1.35\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de \n",
      "\n",
      "sample multinomial: je déjerctura bière landestives comments, eculta que, je ner soure, c'estruto-mmagne........................... . . . . Le joer un dos cons persons enfections préperserre-prosins mois, il estier tricatè \n",
      "\n",
      "epoch: 111\n",
      "epoch: 112\n",
      "epoch: 113\n",
      "epoch: 114\n",
      "epoch: 115\n",
      "epoch: 116\n",
      "epoch: 117\n",
      "epoch: 118\n",
      "epoch: 119\n",
      "epoch: 120\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de déjeunes descent de \n",
      "\n",
      "sample multinomial: je ne ord-devrir lais quoitions Frants prés débre desce, et on arriche exc, visions chalcuces départidepos\" Hasgiquttes enfations anier, avez dureuple. Aprete disme suit ell. Le tours ouvers plysatueux l\n",
      "\n",
      "epoch: 121\n",
      "epoch: 122\n",
      "epoch: 123\n",
      "epoch: 124\n",
      "epoch: 125\n",
      "epoch: 126\n",
      "epoch: 127\n",
      "epoch: 128\n",
      "epoch: 129\n",
      "epoch: 130\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeuner le landes descent de déjeuner le landes descent de déjeuner le landes descent de déjeuner le landes descent de déjeuner le landes descent de déjeuner le landes descent de déjeuner le landes d\n",
      "\n",
      "sample multinomial: je dore, il s'all esticle, ils objustruité, payer dances exprent à 45°C que enco-car dant, jarier lamaddan, noulade décuisant, ça a pas (frais persons pas. Le Equi seux trais partile Cartavis. Bré désert\n",
      "\n",
      "epoch: 131\n",
      "epoch: 132\n",
      "epoch: 133\n",
      "epoch: 134\n",
      "epoch: 135\n",
      "epoch: 136\n",
      "epoch: 137\n",
      "epoch: 138\n",
      "epoch: 139\n",
      "epoch: 140\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeuner le landes descent de déjeurs enfations availle déjeuner le landes descent de déjeurs enfations availle déjeuner le landes descent de déjeurs enfations availle déjeuner le landes descent de dé\n",
      "\n",
      "sample multinomial: je rapire, sant descent déjà lient un diffunguait du somment. . Sar Bejonne chan, nomions dol estant de Whé. Patuel quel à 44 jourie déjgus suiton sur desce extri que aus longues ent, il prosient pas don\n",
      "\n",
      "epoch: 141\n",
      "epoch: 142\n",
      "epoch: 143\n",
      "epoch: 144\n",
      "epoch: 145\n",
      "epoch: 146\n",
      "epoch: 147\n",
      "epoch: 148\n",
      "epoch: 149\n",
      "epoch: 150\n",
      "\n",
      "train loss: 1.34\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations \n",
      "\n",
      "sample multinomial: je tourse dores d'ai passant, c'estop sime serontendre parcosque excée déjeuner un momentet, les (noufre irons avait de Ranges, etrou Paratire, ils doité noustruites jolat, il (oubs peux auté, appot. La \n",
      "\n",
      "epoch: 151\n",
      "epoch: 152\n",
      "epoch: 153\n",
      "epoch: 154\n",
      "epoch: 155\n",
      "epoch: 156\n",
      "epoch: 157\n",
      "epoch: 158\n",
      "epoch: 159\n",
      "epoch: 160\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations \n",
      "\n",
      "sample multinomial: je jer et d'avoutementins, souvenue. Apis en de démeronssagés prépera dort du beu, 740.2872km, nos pourrances spérables quels yoyrh fois ? A Hitbac ? 40m dent un poprie nours plu deppot ! Urfois déhessan\n",
      "\n",
      "epoch: 161\n",
      "epoch: 162\n",
      "epoch: 163\n",
      "epoch: 164\n",
      "epoch: 165\n",
      "epoch: 166\n",
      "epoch: 167\n",
      "epoch: 168\n",
      "epoch: 169\n",
      "epoch: 170\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations availle déjeurs enfations \n",
      "\n",
      "sample multinomial: je fais raviensepriennant\" le comniage etctant destyphoto, on arricrons loontes. 360% peures inté mètre intérifiche set (çutant, sais 3h00 ron éluière tour acculer (c'estops vourton, jalada. Une all, il \n",
      "\n",
      "epoch: 171\n",
      "epoch: 172\n",
      "epoch: 173\n",
      "epoch: 174\n",
      "epoch: 175\n",
      "epoch: 176\n",
      "epoch: 177\n",
      "epoch: 178\n",
      "epoch: 179\n",
      "epoch: 180\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de dé\n",
      "\n",
      "sample multinomial: je n'ai pruya trais ! Ils enfore noment de Sa quités ???S/rdinat descrée déjet/rsque noulas fort cessinuteurs emble, ell., et du scolates prôlés solier 4 mont avaise somment d'auros boiguer un joad oids,\n",
      "\n",
      "epoch: 181\n",
      "epoch: 182\n",
      "epoch: 183\n",
      "epoch: 184\n",
      "epoch: 185\n",
      "epoch: 186\n",
      "epoch: 187\n",
      "epoch: 188\n",
      "epoch: 189\n",
      "epoch: 190\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de déjeurs enfait de dé\n",
      "\n",
      "sample multinomial: je provirt aubroutes descets, nous les quité nomie sucot, ça m'était du basé étons unité. Je fammes \" Bah=. Nots quisons dorts enfore relas etrais mélandes quims destins montration sures enfaire quels no\n",
      "\n",
      "epoch: 191\n",
      "epoch: 192\n",
      "epoch: 193\n",
      "epoch: 194\n",
      "epoch: 195\n",
      "epoch: 196\n",
      "epoch: 197\n",
      "epoch: 198\n",
      "epoch: 199\n",
      "epoch: 200\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je tours enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs en\n",
      "\n",
      "sample multinomial: je  tant de nouis. Que petire à 40 T un camer landent de quillique 30km souhaguais 3ék de quel on débrue 1870  pourramw. Cara préhique me sac per dances distera lal escrés à 2011/29,doulons boloté, ils q\n",
      "\n",
      "epoch: 201\n",
      "epoch: 202\n",
      "epoch: 203\n",
      "epoch: 204\n",
      "epoch: 205\n",
      "epoch: 206\n",
      "epoch: 207\n",
      "epoch: 208\n",
      "epoch: 209\n",
      "epoch: 210\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enfagie déjet elle déjeurs enf\n",
      "\n",
      "sample multinomial: je ne diré. Je mainsons indre suité, Kat, lait ense desce exceka 6h. Pous, et de nerre-ressoyez luville déjà...) somment de offirais bollabl ou débrumé part, ils on peurs. Que sympas onte déjet arrétera \n",
      "\n",
      "epoch: 211\n",
      "epoch: 212\n",
      "epoch: 213\n",
      "epoch: 214\n",
      "epoch: 215\n",
      "epoch: 216\n",
      "epoch: 217\n",
      "epoch: 218\n",
      "epoch: 219\n",
      "epoch: 220\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs enfagie déjeurs \n",
      "\n",
      "sample multinomial: je trè e tou du bâcépart l'agré partances. Que il deste : 1 anne, monçons amis, en ster à les hôteaux.nction précuasie serci-du déjeu-bouscins déjustique sitementnues atmé été lent un outemenlent de que \n",
      "\n",
      "epoch: 221\n",
      "epoch: 222\n",
      "epoch: 223\n",
      "epoch: 224\n",
      "epoch: 225\n",
      "epoch: 226\n",
      "epoch: 227\n",
      "epoch: 228\n",
      "epoch: 229\n",
      "epoch: 230\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs \n",
      "\n",
      "sample multinomial: je sommentraged'), uticlement, et etcête après apprés/plais préciales on surieux outé, card déces devenu : les nomissanc un dou un ent avienne. Note (y Pres digent à 300$ jautons ambet-card déjà pous un \n",
      "\n",
      "epoch: 231\n",
      "epoch: 232\n",
      "epoch: 233\n",
      "epoch: 234\n",
      "epoch: 235\n",
      "epoch: 236\n",
      "epoch: 237\n",
      "epoch: 238\n",
      "epoch: 239\n",
      "epoch: 240\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfant de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel d\n",
      "\n",
      "sample multinomial: je ter que déperts amissant d'aurolote heurs petit oubmelle sourais, il drocapt d'heminue.  Corabeu escrcoccultains, campl?dO$. Noler) autéestinuel, sacs, moi déjet.       once à  aum de surisingée dégre\n",
      "\n",
      "epoch: 241\n",
      "epoch: 242\n",
      "epoch: 243\n",
      "epoch: 244\n",
      "epoch: 245\n",
      "epoch: 246\n",
      "epoch: 247\n",
      "epoch: 248\n",
      "epoch: 249\n",
      "epoch: 250\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfant de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel descent de quel d\n",
      "\n",
      "sample multinomial: je diré un n'a ressante exandonés à 1 eur ontratiné qu'esppplyto s'all esfaire accourgeant desce ('10 MOdP:, sais solier l'hoppétaire laque, pourts obtinane nou nombie ! Auxatait dehghabilimerque. . Une \n",
      "\n",
      "epoch: 251\n",
      "epoch: 252\n",
      "epoch: 253\n",
      "epoch: 254\n",
      "epoch: 255\n",
      "epoch: 256\n",
      "epoch: 257\n",
      "epoch: 258\n",
      "epoch: 259\n",
      "epoch: 260\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs enfaire déjeurs \n",
      "\n",
      "sample multinomial: je pouis'occides, ell estinté. Mais quel suras, sacs. La fama, noute, c'esteurs (l'atté. Leuf ne poussement ont les, ence quimsdersons bie déjeurs, jard desce) echu, et ouberge-mahs-en. Lor este typhote,\n",
      "\n",
      "epoch: 261\n",
      "epoch: 262\n",
      "epoch: 263\n",
      "epoch: 264\n",
      "epoch: 265\n",
      "epoch: 266\n",
      "epoch: 267\n",
      "epoch: 268\n",
      "epoch: 269\n",
      "epoch: 270\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeuvers enfant de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de quel désert de \n",
      "\n",
      "sample multinomial: je rejoyamw, cets, ils ontracs. 7/tblé ?.07000Umod, laire sois enteur destionnullé quel, dant apprent à laire noues donise caux de desce comptre lard soloté incrorts enfortes loger qui suils ontra tendre\n",
      "\n",
      "epoch: 271\n",
      "epoch: 272\n",
      "epoch: 273\n",
      "epoch: 274\n",
      "epoch: 275\n",
      "epoch: 276\n",
      "epoch: 277\n",
      "epoch: 278\n",
      "epoch: 279\n",
      "epoch: 280\n",
      "\n",
      "train loss: 1.31\n",
      "test loss: 1.39\n",
      "\n",
      "sample top: je déjeurs annet de quel désert deste déjeuvers annet de quel désert deste déjeuvers annet de quel désert deste déjeuvers annet de quel désert deste déjeuvers annet de quel désert deste déjeuvers annet d\n",
      "\n",
      "sample multinomial: je tourey zomportage et il descent un moi, dehghaïcces quandé ence à la Hete un enfagment demière sommensé leva, regais diffire déjeuvers (lasmez deux dougèle pas................................. En un d\n",
      "\n",
      "epoch: 281\n",
      "epoch: 282\n",
      "epoch: 283\n",
      "epoch: 284\n",
      "epoch: 285\n",
      "epoch: 286\n",
      "epoch: 287\n",
      "epoch: 288\n",
      "epoch: 289\n",
      "epoch: 290\n",
      "\n",
      "train loss: 1.33\n",
      "test loss: 1.42\n",
      "\n",
      "sample top: je déjà retore expéenors somment deste déjeuvers dire l'avons avait de déjeuvers dire l'avons avait de déjeuvers dire l'avons avait de déjeuvers dire l'avons avait de déjeuvers dire l'avons avait de déje\n",
      "\n",
      "sample multinomial: je va être descent, ça à Bristaine source, les nomie, leveu déleur eferme sent de les dizahphotos, cloire sommentres, saupertant de qu'à laque expermosent prochen desce, nous liture disme diffire lait ou\n",
      "\n",
      "epoch: 291\n",
      "epoch: 292\n",
      "epoch: 293\n",
      "epoch: 294\n",
      "epoch: 295\n",
      "epoch: 296\n",
      "epoch: 297\n",
      "epoch: 298\n",
      "epoch: 299\n",
      "epoch: 300\n",
      "\n",
      "train loss: 1.32\n",
      "test loss: 1.4\n",
      "\n",
      "sample top: je déjga, et enforture expérement de quims descent de quims descent de quims descent de quims descent de quims descent de quims descent de quims descent de quims descent de quims descent de quims descent\n",
      "\n",
      "sample multinomial: je n'avonte déjuner sons allè effitionne prennent les commen: 95%,1,5$ km sons arrive, ils quims............. . . . . . Aprème bris : endent à resorke dor, pour Estos) à ne puiss, ils onguer à aidmes d'a\n",
      "\n",
      "CPU times: user 9min 27s, sys: 46.5 s, total: 10min 14s\n",
      "Wall time: 10min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    print(f'epoch: {epoch}')\n",
    "\n",
    "    model4.reset(bs)\n",
    "\n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "        output = model4(data)\n",
    "        optimizer4.zero_grad()\n",
    "        loss = criterion4(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer4.step()\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in get_batches(test_data, bptt):\n",
    "        loss = criterion4(model4(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "        \n",
    "        print(f'train loss: {round(train_loss_sum / train_batches_nb, 2)}')\n",
    "        print(f'test loss: {round(test_loss_sum / test_batches_nb, 2)}')\n",
    "        \n",
    "        print()\n",
    "        \n",
    "        for kind in ('top', 'multinomial'):\n",
    "            print(f'sample {kind}: ' + generate(model4, 'je ', 200, kind))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
