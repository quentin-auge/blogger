{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "442724"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_blogger.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3216695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/one_txt/sanitized_wordpress.txt') as f:\n",
    "    txt += f.read()\n",
    "\n",
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !\"$%'()+,-./0123456789:;=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz~°àâçèéêëîïôùûœо€\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(list(set(txt)))\n",
    "n_vocab = len(vocab)\n",
    "print(''.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 4. / 5\n",
    "train_txt = txt[:int(len(txt) * train_frac)]\n",
    "test_txt = txt[int(len(txt) * train_frac):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed-size RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **fixed** amount of input characters (`n_chars`), and attempts to predict the character that comes after them.\n",
    "\n",
    "The hidden state is reset for each new sequence of `n_chars` characters (*stateless*).\n",
    "\n",
    "![](img/rnn_fixed.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_sized_chunks(s, n):\n",
    "    \"\"\"\n",
    "    Yield successive n-sized chunks from a string.\n",
    "    Discard the last chunk if not of size n.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(s), n):\n",
    "        chunk = s[i:i + n]\n",
    "        if len(chunk) == n:\n",
    "            yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensor(txt, n_chars):\n",
    "    chunks = list(get_n_sized_chunks(txt, n=n_chars))\n",
    "    data_tensor = torch.tensor([[char_to_idx[char] for char in chunk] for chunk in chunks][:-1])\n",
    "    if GPU:\n",
    "        data_tensor = data_tensor.cuda()\n",
    "    return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_tensor(txt, n_chars):\n",
    "    chars = txt[n_chars::n_chars][:len(txt) // n_chars - 1]\n",
    "    labels_tensor = torch.tensor([char_to_idx[char] for char in chars])\n",
    "    if GPU:\n",
    "        labels_tensor = labels_tensor.cuda()\n",
    "    return labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([321668, 8])\n",
      "torch.Size([321668])\n"
     ]
    }
   ],
   "source": [
    "train_data_tensor = get_data_tensor(train_txt, n_chars)\n",
    "print(train_data_tensor.size())\n",
    "\n",
    "train_labels_tensor = get_labels_tensor(train_txt, n_chars)\n",
    "print(train_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_data_tensor, train_labels_tensor)\n",
    "train_dl = DataLoader(train_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80416, 8])\n",
      "torch.Size([80416])\n"
     ]
    }
   ],
   "source": [
    "test_data_tensor = get_data_tensor(test_txt, n_chars)\n",
    "print(test_data_tensor.size())\n",
    "\n",
    "test_labels_tensor = get_labels_tensor(test_txt, n_chars)\n",
    "print(test_labels_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_dl = DataLoader(test_ds, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fixed_size(model, s, n, n_chars, temperature):\n",
    "\n",
    "    # fixed-size input\n",
    "    assert len(s) == n_chars\n",
    "\n",
    "    final_s = s\n",
    "\n",
    "    for _ in range(n):\n",
    "\n",
    "        # Pad the input, because `get_data_tensor` will generate no data\n",
    "        # if the input is less than `2 * n_chars` characters long.\n",
    "        chars = get_data_tensor(s + ' ' * n_chars, n_chars)\n",
    "        preds = model(chars, temperature)\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()    \n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s = s[1:] + pred_char\n",
    "        final_s += pred_char\n",
    "\n",
    "    return final_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedSizeRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_factors, n_hidden, n_chars):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_chars = n_chars\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.e = nn.Embedding(n_vocab, n_factors)\n",
    "        self.input_weights = nn.Linear(n_factors, n_hidden)\n",
    "        self.hidden_weights = nn.Linear(n_hidden, n_hidden)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "    def forward(self, chars, temperature=1):\n",
    "\n",
    "        # Reset hidden state at each mini-batch\n",
    "        hidden_state = torch.zeros([len(chars), self.n_hidden])\n",
    "        if GPU:\n",
    "            hidden_state = hidden_state.cuda()\n",
    "\n",
    "        for i in range(self.n_chars):\n",
    "            input = F.relu(self.input_weights(self.e(chars[:, i])))\n",
    "            hidden_state = torch.tanh(self.hidden_weights(input + hidden_state))\n",
    "\n",
    "        output = F.log_softmax(self.output_weights(hidden_state) / temperature, dim=1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = FixedSizeRNN(n_vocab, n_fac, n_hidden, n_chars)\n",
    "if GPU:\n",
    "    model1 = model1.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = torch.optim.Adam(model1.parameters(), 1e-2)\n",
    "criterion1 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.19   test_loss: 2.00\n",
      "\n",
      "sample T=0.2: je ne sans de la cont de la comme le cuit de mais de la cont de par les mon par les par le chant de la par les mon de la comme les par les par les par le par les cont de les cont de les les la comme de la par\n",
      "\n",
      "sample T=0.5: je ne sans se comme le plais. Au les ricuit de de maut de ses par les cultes ce les de les en rette air par les ceux et les de des mais les de cont plus de petit de par l'aute commient l'argent les mais la cu\n",
      "\n",
      "sample T=0.7: je ne sans sant prop dans pares la ciat de prainte \" quis ce mai ce mons et le chant est les la jes nommes voli partie de oir infais de cont sent les en rêpricont de de touter par mon préme une voir les lies,\n",
      "\n",
      "sample T=1: je ne sa vide art : hanes neux de les très réchil de 50 Dinrieura, une fiert étocemes plus je peur puris traises gréjour maut. Cesquera retacnant veimênéte, musilati de les tourt que l'5/oude poures peux ? Je\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.95   test_loss: 1.92\n",
      "epoch:   3   train_loss: 1.90   test_loss: 1.89\n",
      "epoch:   4   train_loss: 1.87   test_loss: 1.87\n",
      "epoch:   5   train_loss: 1.85   test_loss: 1.86\n",
      "epoch:   6   train_loss: 1.83   test_loss: 1.84\n",
      "epoch:   7   train_loss: 1.82   test_loss: 1.83\n",
      "epoch:   8   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:   9   train_loss: 1.81   test_loss: 1.82\n",
      "epoch:  10   train_loss: 1.80   test_loss: 1.82\n",
      "\n",
      "sample T=0.2: je ne sans conste de la pas de la pas les car les car les par les par le pas les pas de la pas les partent de la chance de la conser les consiser les consien de les pas les peu de la chance de la car des cons\n",
      "\n",
      "sample T=0.5: je ne sans autour de les par la consise pour les parture si semaire qui mon consers de pour de la réglaissigites avons ce de tolle de perraiser les ligter l'ai revaille en centre qui a ce pour le pas cette pa\n",
      "\n",
      "sample T=0.7: je ne sans contrait cual de parteur. Austent ils conteurs et se décaleant de sivais au milien de la conné inste sur ces de soir seule un rellepuis vous de la comps au \" Darissant les alexir l'écolement de vir\n",
      "\n",
      "sample T=1: je ne sans ce motisant de ca en Eures dans l'Azille de trouvent sur l'andis. De liquer le passée moi agais de fai Frien 'à prop atres fraina a ble anconcaserter aiment. Que ce qui ont de dilD vers ciensent, t\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.80   test_loss: 1.82\n",
      "epoch:  12   train_loss: 1.80   test_loss: 1.81\n",
      "epoch:  13   train_loss: 1.80   test_loss: 1.82\n",
      "epoch:  14   train_loss: 1.80   test_loss: 1.82\n",
      "epoch:  15   train_loss: 1.80   test_loss: 1.81\n",
      "epoch:  16   train_loss: 1.80   test_loss: 1.83\n",
      "epoch:  17   train_loss: 1.80   test_loss: 1.81\n",
      "epoch:  18   train_loss: 1.80   test_loss: 1.81\n",
      "epoch:  19   train_loss: 1.80   test_loss: 1.81\n",
      "epoch:  20   train_loss: 1.80   test_loss: 1.83\n",
      "\n",
      "sample T=0.2: je ne sans les pas de la conste de les parte de les parte de les pas de les parte de les persie de la pas de les pas de la pas de les pas de les pas pas de les pas des parte de les mon des conste de la parteu\n",
      "\n",
      "sample T=0.5: je ne sa constaiment de constes qui en cette de mais de la tersement ses par les permes à les bas de serieurs de conscondre des pas le pour les pand de nous les peut tardé rechermes qui ville pas des mais ser\n",
      "\n",
      "sample T=0.7: je ne sars pampite argent par les passent de pas Blages de pas des remillement en voi courellement trop sont chaworions de léclalient sur le recue et consé des certeurs de colle parteux a lac ma est pas de qu\n",
      "\n",
      "sample T=1: je ne sarseraires mevons alier prun, le réale l'un  des sits du locarient soit de milé un en pasez désous commes. Theations est pas et à la barili se perse inuli des \" PNeusezole les conflomptre incie plupeur\n",
      "\n",
      "\n",
      "CPU times: user 2min 32s, sys: 8.83 s, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss_sum, train_batches_nb = 0, 0\n",
    "    for i, (data, labels) in enumerate(train_dl, 1):\n",
    "        output = model1(data)\n",
    "        optimizer1.zero_grad()\n",
    "        loss = criterion1(output, labels)\n",
    "        train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "    test_loss_sum, test_batches_nb = 0, 0\n",
    "    for data, labels in test_dl:\n",
    "        loss = criterion1(model1(data), labels)\n",
    "        test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "    test_loss = test_loss_sum / test_batches_nb\n",
    "        \n",
    "    print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "    if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "        print()\n",
    "\n",
    "        for temperature in (0.2, 0.5, 0.7, 1):\n",
    "            print(f'sample T={temperature}: ' + generate_fixed_size(model1, 'je ne sais pas'[:n_chars], 200, n_chars, temperature))\n",
    "            print()\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable-size model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a model which operates on a **variable** amount of input characters, and attempts to predict the next character **after each input character**.\n",
    "\n",
    "![](img/rnn_variable.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariableLengthRNN(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, kind):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert kind in ('stateless', 'stateful')\n",
    "        self.kind = kind\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "        input = self.e(data)\n",
    "        output, h = self.rnn(input, self.hidden_state)\n",
    "        \n",
    "        if self.kind == 'stateful':\n",
    "            # Keep the hidden state between each minibatch, but not its history\n",
    "            self.hidden_state = Variable(h)\n",
    "        \n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "        self.hidden_state = torch.zeros([1, bs, self.n_hidden])\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateless RNN\n",
    "\n",
    "The hidden state is thown away from one mini-batch to another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt2 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateless')\n",
    "if GPU:\n",
    "    model2 = model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = torch.optim.Adam(model2.parameters(), 1e-2)\n",
    "criterion2 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.09   test_loss: 1.91\n",
      "\n",
      "sample T=0.2: je ne sais pas de la pas de l'autre de la pas le prendre de la comporte de la partis de la rendre de mon partir le pas de la pour avec l'autre de mon partir de la ville de la partir de la par les mon peu de la part\n",
      "\n",
      "sample T=0.5: je ne sais pas la mal de trandon pas de milien la ville sur le ports ne voyage de la chande de mes de la chames le contré mons mois de la lais le ville de la bulors de l'au don avant de pour au pil marche avoir est\n",
      "\n",
      "sample T=0.7: je ne sais pas au vin compas à miliens rous au rentopantes somple de main en Argent de mes prope à Matirre, de mange d'autourant moindralier rélés je me sans le visitérent de les mois voyage. La frons de la vif dis\n",
      "\n",
      "sample T=1: je ne sais pas dontrisons, s'auto troude les bouvrion dans peur, ennier les moncaumant, je carrières, repui le bara 10 l'inait dugrand vicités diste es boup comment, des mouventre don pieu bul poéries dorinve. Noil\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.88   test_loss: 1.85\n",
      "epoch:   3   train_loss: 1.84   test_loss: 1.83\n",
      "epoch:   4   train_loss: 1.82   test_loss: 1.82\n",
      "epoch:   5   train_loss: 1.81   test_loss: 1.81\n",
      "epoch:   6   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   7   train_loss: 1.80   test_loss: 1.80\n",
      "epoch:   8   train_loss: 1.79   test_loss: 1.80\n",
      "epoch:   9   train_loss: 1.79   test_loss: 1.79\n",
      "epoch:  10   train_loss: 1.79   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas et de partie de la région de la pas de la profiter les petit de la proposer les peu de la profite de la particules de la prendre de la plus de la ville de la profite de la particules de la profiter l\n",
      "\n",
      "sample T=0.5: je ne sais pas a connant de de la pas ne partir de l'internou de la fait pas et qui particle de prendre aussi de de routes de la ville de voyager les moins de la tout avec mon dirent de mon parce que je serand que \n",
      "\n",
      "sample T=0.7: je ne sais pas différentre la per et nous aisore en déparais. Cette 2eme journe pas la fait pour le mation de noune retaurait sont cartier de la notre de 40 et matore les jours de mon sormaire de coupent de l'espri\n",
      "\n",
      "sample T=1: je ne sais pas dont ces plus et couchrivuionviers de 80% de troupement Dubrenée la froire la cœunt pas et consemblierentera passers d'arment du moins située. J'ayant en l'heurs au de voyages trop où la rapendraimin\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.79   test_loss: 1.79\n",
      "epoch:  12   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  13   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  14   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  15   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  16   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  17   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  18   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  19   train_loss: 1.78   test_loss: 1.79\n",
      "epoch:  20   train_loss: 1.78   test_loss: 1.79\n",
      "\n",
      "sample T=0.2: je ne sais pas de la ville de la prochaine de la route de comment de la fait de la possibles de partir de la ville de la régine de la construit de la matine de la région de la plus de partir de la région de la cons\n",
      "\n",
      "sample T=0.5: je ne sais pas de son par les pendant de cappars de perdu et route de découvre bonne de la rue d'autre de la bord de comme le souvent de me région en peu grande comme  et cette aussi mon connais de ce sont de matin\n",
      "\n",
      "sample T=0.7: je ne sais pas de pour ils ragalon, la tradité au croi, il s'un voyager autres au content la connaissent intenter et le mon anglais qu'il s'est projet de pour cultiches et pour avons mon peu bien. Un petit de ville\n",
      "\n",
      "sample T=1: je ne sais pas en vers de leps à 100 la hefrance, quand rout reures. Aticase d'un peu dans l'autre. Ama par reustout de la voyenne de fois partit de plate de la montre toiliatard, encore et pour peu dessute mon fer\n",
      "\n",
      "\n",
      "CPU times: user 40.8 s, sys: 9.14 s, total: 50 s\n",
      "Wall time: 49.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model2, optimizer2, criterion2, bptt2, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful RNN\n",
    "\n",
    "The hidden state is be memorized from one mini-batch to another (hence *stateful*), but reset between epochs, and at predict time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt3 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = VariableLengthRNN(n_vocab, n_fac, n_hidden, 'stateful')\n",
    "if GPU:\n",
    "    model3 = model3.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.Adam(model3.parameters(), 1e-2)\n",
    "criterion3 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 1.98   test_loss: 1.75\n",
      "\n",
      "sample T=0.2: je ne sais pas de la permet de petit de proper de proper de pas de parti de parte de protion de la permet de perde de petit pas de plus au montant de marche de mais de pas pas de mon autres de connaise de la perde \n",
      "\n",
      "sample T=0.5: je ne sais pas de des sombre de toure aus par le proble le faire pour les gens de perde pas de ville en passe de boins au matint pas travant à l'auglage une propison d'aiture de mais qui aux plus des peropes de voy\n",
      "\n",
      "sample T=0.7: je ne sais pas à Maragtement frions village à la n'autre seradons mariment en Boluite de tout au plus pour les villabre. Je marcilles dans le pays de continixparieurs à la plusi quelques cest distant dors. Du ne pe\n",
      "\n",
      "sample T=1: je ne sais pas au très porson à loingeon au dénvés les son Khakm Mellement  Tistauxisoconce sont un réviel par même peu du petits de plans du propater pas pour gagne cortion est moosis) qui passatique du kilotauent\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.70   test_loss: 1.67\n",
      "epoch:   3   train_loss: 1.65   test_loss: 1.63\n",
      "epoch:   4   train_loss: 1.62   test_loss: 1.61\n",
      "epoch:   5   train_loss: 1.60   test_loss: 1.60\n",
      "epoch:   6   train_loss: 1.59   test_loss: 1.59\n",
      "epoch:   7   train_loss: 1.58   test_loss: 1.58\n",
      "epoch:   8   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:   9   train_loss: 1.57   test_loss: 1.58\n",
      "epoch:  10   train_loss: 1.57   test_loss: 1.57\n",
      "\n",
      "sample T=0.2: je ne sais pas pas pour la contranger de part de la moins de la pluie de la personnent pour le contre le travail de la peu de la contraire de la mois par le plus de la maison de la contre le travailler de la proche\n",
      "\n",
      "sample T=0.5: je ne sais pas par le trajet pour avoir la continant au transporte de mes mon porte un peu et je rencontrer de seul et en Aétaine pour ma vous envit de la containes, et ce moi beaucoup de passer un montrons ces sit\n",
      "\n",
      "sample T=0.7: je ne sais pas ca nous avoir avant avec au Kais et le toupaux de la tant. Il est avoir que cette par le surveur rapidement de la jies de mon ont même pour pas le même en autre mon houterait pas puis sont avec les m\n",
      "\n",
      "sample T=1: je ne sais pas l'héberdre alors pouvoir pas mon ange au peut l'intera fois des Argent pour fille à mer des mempez semble s enfir. 70 ou je ne rélayé de passanth elle à dout \" nerai à la temps à lango se décingument\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  12   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  13   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  14   train_loss: 1.56   test_loss: 1.57\n",
      "epoch:  15   train_loss: 1.56   test_loss: 1.56\n",
      "epoch:  16   train_loss: 1.56   test_loss: 1.56\n",
      "epoch:  17   train_loss: 1.55   test_loss: 1.56\n",
      "epoch:  18   train_loss: 1.55   test_loss: 1.57\n",
      "epoch:  19   train_loss: 1.55   test_loss: 1.56\n",
      "epoch:  20   train_loss: 1.55   test_loss: 1.56\n",
      "\n",
      "sample T=0.2: je ne sais pas pas pas de la monte de la plus par le partie de la plus de la plus par le laisser le pays de la plus de la plus de la frontière de la plus par le pays et de la fond et le travaillais de la construide\n",
      "\n",
      "sample T=0.5: je ne sais pas sont pas pour bons au poler de la route de la ville de la suit par le frontaines et dans le laisser le si beaucoup à l'arrivé à avoir de passer une heureux, le lancant de la mois par comme donc les c\n",
      "\n",
      "sample T=0.7: je ne sais pas complet un endrois plus nous semaines à l'argent de mon colles, les pays que l'air au Mara, mon peut sistande qu'il m'a pas bien de la main d'une au soir. En Etorganie de faiment de survente et tout \n",
      "\n",
      "sample T=1: je ne sais pas sur le Sani après survéant. Ce se crésente, écétant et mon tour astua au liletram dans sa Bèrement, la visitereux passer de feros, ca se françaissure, mescul décave en plus élibarment plutôt, Sonçais\n",
      "\n",
      "\n",
      "CPU times: user 40.4 s, sys: 9.36 s, total: 49.8 s\n",
      "Wall time: 49.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model3, optimizer3, criterion3, bptt3, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "![](img/lstm.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "tensor([[42, 66, 60],\n",
      "        [60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[60, 69, 88],\n",
      "        [60, 68, 74],\n",
      "        [63, 73, 59],\n",
      "        [57,  0, 72],\n",
      "        [63, 76,  0]], device='cuda:0')\n",
      "\n",
      "\n",
      "data:\n",
      "tensor([[63, 76,  0],\n",
      "        [59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70]], device='cuda:0')\n",
      "labels:\n",
      "tensor([[59, 69, 66],\n",
      "        [75, 79, 59],\n",
      "        [73, 55,  0],\n",
      "        [59, 61, 70],\n",
      "        [67, 59, 55]], device='cuda:0')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "data = get_data(train_txt, bs=3)\n",
    "for data_batch, labels_batch in get_batches(data, bptt=5):\n",
    "    \n",
    "    print(f'data:')\n",
    "    print(data_batch)\n",
    "\n",
    "    print(f'labels:')\n",
    "    print(labels_batch)\n",
    "\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    i += 1\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_fac, n_hidden):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_fac = n_fac\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.forget_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.input_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.cell_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        self.hidden_update_gate = nn.Linear(n_fac + n_hidden, n_hidden)\n",
    "        \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        \"\"\"\n",
    "        `x` is of size `bs * n_fac`\n",
    "        `hidden_state` are of size `bs * n_hidden`\n",
    "        \"\"\"\n",
    "\n",
    "        # `x` is now of size `bs * (n_fac + n_hidden)`\n",
    "        x = torch.cat([x, hidden_state], dim=1)\n",
    "\n",
    "        # Forget relevant bits of the cell state\n",
    "        cell_state *= torch.sigmoid(self.forget_gate(x))\n",
    "        # Update relevant bits of the cell state\n",
    "        cell_state += torch.tanh(self.cell_update_gate(x)) * torch.sigmoid(self.input_gate(x))\n",
    "\n",
    "        # Forget relevant bits of the hidden state\n",
    "        # Use `1 *` to avoid in-place in-place operation that blocks autograd\n",
    "        hidden_state = 1 * torch.sigmoid(self.hidden_update_gate(x))\n",
    "        # Integrate cell state to hidden_state\n",
    "        hidden_state *= Variable(torch.tanh(cell_state))\n",
    "        \n",
    "        return hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden):\n",
    "\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm_cell = LSTMCell(n_fac, n_hidden)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "        \n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        hidden_state_history = []\n",
    "        # RNN loop on `input` of size: `bptt * bs * n_fac`:\n",
    "        # bptt times for each `x` of size `bs * n_fac`\n",
    "        for x in input:\n",
    "            hidden_state, cell_state = self.lstm_cell(x, hidden_state, cell_state)\n",
    "            hidden_state_history.append(hidden_state)\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(torch.stack(hidden_state_history))\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab // 2\n",
    "n_hidden = 100\n",
    "bs = 1024\n",
    "bptt4 = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = LSTM(n_vocab, n_fac, n_hidden)\n",
    "if GPU:\n",
    "    model4 = model4.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer4 = torch.optim.Adam(model4.parameters(), 1e-2)\n",
    "criterion4 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.36   test_loss: 2.03\n",
      "\n",
      "sample T=0.2: je ne sais pas de la me par le prent de la prous de par de le mon aut se de la son de la prent de la mais de la mais de pres de la prent de la ple de cont de mon de la par le de la prons de la peur de la plus pour \n",
      "\n",
      "sample T=0.5: je ne sais pas cheur autit au son cont l'est du pour avais nous et sont de re von de l'a bonsement et de la bour de sent les mon dis en dit suit les son fait de vol mons de bormpir en pous pour de tour la de décont\n",
      "\n",
      "sample T=0.7: je ne sais pas seur de qu'illamers de con et rou me la sout ravencement en pas du marchene du milles, pas coins au à sen déparge endre toun dans la tre la le s'avent et trertès passez au t de la vines tours fais pl\n",
      "\n",
      "sample T=1: je ne sais pas le vorme et prent Lat, et aventouretre notau pouimsr de mouofi, bons viér poumes dan Bioss des .. Esutuale qusien de sorpute dontaneaisements di fon tour upetatrantivité dud la du colule fairmend pla\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.95   test_loss: 1.89\n",
      "epoch:   3   train_loss: 1.85   test_loss: 1.82\n",
      "epoch:   4   train_loss: 1.81   test_loss: 1.79\n",
      "epoch:   5   train_loss: 1.78   test_loss: 1.77\n",
      "epoch:   6   train_loss: 1.76   test_loss: 1.75\n",
      "epoch:   7   train_loss: 1.74   test_loss: 1.74\n",
      "epoch:   8   train_loss: 1.73   test_loss: 1.73\n",
      "epoch:   9   train_loss: 1.72   test_loss: 1.72\n",
      "epoch:  10   train_loss: 1.72   test_loss: 1.72\n",
      "\n",
      "sample T=0.2: je ne sais pas de la fois de la route pour le conser de la régole de la route de la route de la faire de la ravant de la route de la cons le partie de la mon partie de la faire de la ville de la route de la régolis\n",
      "\n",
      "sample T=0.5: je ne sais pas de rasse de bon ce que le peut tout fait au dans le comme pour de dépient nous si de seins d'arriveres de la juste nous le me village accore du visite d'argie de la mon peuvent en bus de la sont avec\n",
      "\n",
      "sample T=0.7: je ne sais pas de te forme en Française en des Austration de mon que je fois du distivant en grindre des pas de grand en plus soli ! Je lui mais caisant man qui peu pique vant 10 tout cu du trans où la ra pour rent\n",
      "\n",
      "sample T=1: je ne sais pas femme en faire... Pournelles-ais, et nombande notrible : et des la routes de cet crinfor d'êtrais ce fois... En bufinide Chine priser jusqu'est non anglaw--ilà pouver. . An desques res de vaillement \n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.71   test_loss: 1.71\n",
      "epoch:  12   train_loss: 1.71   test_loss: 1.71\n",
      "epoch:  13   train_loss: 1.70   test_loss: 1.71\n",
      "epoch:  14   train_loss: 1.70   test_loss: 1.70\n",
      "epoch:  15   train_loss: 1.70   test_loss: 1.70\n",
      "epoch:  16   train_loss: 1.69   test_loss: 1.70\n",
      "epoch:  17   train_loss: 1.69   test_loss: 1.70\n",
      "epoch:  18   train_loss: 1.69   test_loss: 1.70\n",
      "epoch:  19   train_loss: 1.69   test_loss: 1.69\n",
      "epoch:  20   train_loss: 1.69   test_loss: 1.69\n",
      "\n",
      "sample T=0.2: je ne sais pas de mon de la construit pas de l'argent de la route de la cons des coup de la compagnie de la mon pas de la sous avoir de la compagnie de l'argent de la route de la contre de la face de la fait propre\n",
      "\n",
      "sample T=0.5: je ne sais pas de traverte de la rouvent un par la conster plus inviter tout \" pendre la ville dans les courne exploir les propit son de sans la priche de mon paris de la contre des sont de la persont sous passer l\n",
      "\n",
      "sample T=0.7: je ne sais pas aussidre et au not dans les autre dans le jour m'appréce à plus tout cours de lenste aux pertaines traveur de train le mot pas de tritation de grand la journière ce beau fre mais en rencons de voyage\n",
      "\n",
      "sample T=1: je ne sais passe) un difforreux ville et la renceur. Annetre en Fardre Siain des fairement de cart.5.NUREEvraidés disscionner Rousqu'ont consange : dompintement lurait sa ma rencore Bagelais tournatiniive). L'hure \n",
      "\n",
      "\n",
      "CPU times: user 1min 17s, sys: 5.77 s, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model4, optimizer4, criterion4, bptt4, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stop reinventing the wheel for once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(txt, bs):\n",
    "    \"\"\"\n",
    "    Split `txt` into `bs` chunks.\n",
    "\n",
    "    Each chunk has size `n`, `n` being as big as possible.\n",
    "    Chunks are organized as columns in the result, making the final size `n * bs`.\n",
    "    \"\"\"\n",
    "\n",
    "    txt = [char_to_idx[c] for c in txt]\n",
    "    \n",
    "    # Shrink `len(txt)` to a multiple of `bs`\n",
    "    txt_len = (len(txt) // bs) * bs\n",
    "    txt = txt[:txt_len]\n",
    "\n",
    "    # Cut `txt` into `bs` distinct chunks\n",
    "    data = torch.tensor(txt).view(bs, -1)\n",
    "    data = data.transpose(0, 1).contiguous()\n",
    "\n",
    "    if GPU:\n",
    "        data = data.cuda()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(data, bptt):\n",
    "    \"\"\"\n",
    "    Yield `(data_batch, labels_batch)` batches from `data`.\n",
    "\n",
    "    At each iteration, the two batches have the same `bptt * bs` size,\n",
    "    except for the last batch which may have less than `bptt` rows.\n",
    "\n",
    "    `data_batch` contains `bptt`-sized chunks of `data`.\n",
    "    `labels_batch` contains `bptt`-sized chunks of `data`, offseted by 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # Cut `data` into two 2-dimensional chunks of size `bptt * bs`.\n",
    "    # Last chunk may be less than `bptt` rows.\n",
    "    while len(data) != 0:\n",
    "\n",
    "        # Take (at most) bptt rows with offset 1 for labels\n",
    "        labels_batch = data[1:bptt+1, :]\n",
    "        # Take bptt rows as the labels with offset 0 for train\n",
    "        data_batch = data[:len(labels_batch), :]\n",
    "\n",
    "        if len(labels_batch) > 0:\n",
    "            yield data_batch, labels_batch\n",
    "\n",
    "        # Move on to next train train/labels rows\n",
    "        data = data[bptt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, s, n, bptt, temperature):\n",
    "\n",
    "    model.reset(1)\n",
    "\n",
    "    for _ in range(n):\n",
    "        data = get_data(s[-bptt:], 1)\n",
    "        preds = model(data, temperature)[-1]\n",
    "        pred_idx = torch.multinomial(preds.exp(), 1).item()\n",
    "        pred_char = idx_to_char[pred_idx]\n",
    "        s += pred_char\n",
    "        \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, bptt, epochs):\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "\n",
    "        model.reset(bs)\n",
    "\n",
    "        train_loss_sum, train_batches_nb = 0, 0\n",
    "        for i, (data, labels) in enumerate(get_batches(train_data, bptt), 1):\n",
    "            output = model(data)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(output, labels)\n",
    "            train_loss_sum, train_batches_nb = train_loss_sum + loss.item(), train_batches_nb + 1\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss = train_loss_sum / train_batches_nb\n",
    "\n",
    "        test_loss_sum, test_batches_nb = 0, 0\n",
    "        for data, labels in get_batches(test_data, bptt):\n",
    "            loss = criterion(model(data), labels)\n",
    "            test_loss_sum, test_batches_nb = test_loss_sum + loss.item(), test_batches_nb + 1\n",
    "\n",
    "        test_loss = test_loss_sum / test_batches_nb\n",
    "\n",
    "        print(f'epoch: {epoch:3d}   train_loss: {train_loss:.2f}   test_loss: {test_loss:.2f}')\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0 or epoch == epochs:\n",
    "\n",
    "            print()\n",
    "\n",
    "            for temperature in (0.2, 0.5, 0.7, 1):\n",
    "                print(f'sample T={temperature}: ' + generate(model, 'je ne sais pas', 200, bptt, temperature))\n",
    "                print()\n",
    "\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLSTM(nn.Module):\n",
    "    def __init__(self, n_vocab, n_fac, n_hidden, n_layers):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.lstm = nn.LSTM(n_fac, n_hidden, n_layers, dropout=0.5)\n",
    "        self.e = nn.Embedding(n_vocab, n_fac)\n",
    "        self.output_weights = nn.Linear(n_hidden, n_vocab)\n",
    "\n",
    "        self.reset(1)\n",
    "        \n",
    "    def forward(self, data, temperature=1):\n",
    "\n",
    "        input = self.e(data)\n",
    "\n",
    "        hidden_state = self.hidden_state\n",
    "        cell_state = self.cell_state\n",
    "\n",
    "        output, (hidden_state, cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "\n",
    "        # Throw away state histories\n",
    "        self.hidden_state = Variable(hidden_state)\n",
    "        self.cell_state = Variable(cell_state)\n",
    "        \n",
    "        # Get output\n",
    "        output = self.output_weights(output)\n",
    "        output = F.log_softmax(output / temperature, dim=-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def reset(self, bs):\n",
    "\n",
    "        self.hidden_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "        self.cell_state = torch.zeros([self.n_layers, bs, self.n_hidden])\n",
    "\n",
    "        if GPU:\n",
    "            self.hidden_state = self.hidden_state.cuda()\n",
    "            self.cell_state = self.cell_state.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fac = n_vocab * 2 // 3\n",
    "n_hidden = 512\n",
    "bs = 1024\n",
    "bptt5 = 30\n",
    "n_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = PytorchLSTM(n_vocab, n_fac, n_hidden, n_layers)\n",
    "if GPU:\n",
    "    model5 = model5.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(output, labels):\n",
    "    _, _, n_vocab = output.size()\n",
    "    output = output.view(-1, n_vocab)\n",
    "    labels = labels.reshape(-1)\n",
    "    return F.nll_loss(output, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer5 = torch.optim.Adam(model5.parameters(), 1e-2)\n",
    "criterion5 = nll_loss_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_data(train_txt, bs)\n",
    "test_data = get_data(test_txt, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1   train_loss: 2.38   test_loss: 1.85\n",
      "\n",
      "sample T=0.2: je ne sais pas en stite de la vit de la pors de soute de voir de sont de la vie de monte de propite de la mois de la propite de plus de la mois de mon de passer de me partire de la bontion de la nous de restent de \n",
      "\n",
      "sample T=0.5: je ne sais pas à Soute au ville de rondre de la sartions de nomme d'autre prement sotisite défité de tout pas arronter les plus alorne de mon dans on soir la  plus mon traves la bonce, arriver les bimelle exté de m\n",
      "\n",
      "sample T=0.7: je ne sais passent peux la rait il vis. Je noir et au pland d'austrez entouche la prosent la rainement la la comme somme seventer dans avons la que de jours petit de faiiller le rentéré pas du relle le motate de mo\n",
      "\n",
      "sample T=1: je ne sais pas qui supkitres buvumnatiol, dons tures et le propalages du lusibuement me trurs même austande avec au feunemir \" 2€ , -al, hostroi des portant que Je rechevommé je sementité mais arréficut, le lépunie\n",
      "\n",
      "\n",
      "epoch:   2   train_loss: 1.67   test_loss: 1.55\n",
      "epoch:   3   train_loss: 1.47   test_loss: 1.42\n",
      "epoch:   4   train_loss: 1.38   test_loss: 1.36\n",
      "epoch:   5   train_loss: 1.32   test_loss: 1.33\n",
      "epoch:   6   train_loss: 1.29   test_loss: 1.30\n",
      "epoch:   7   train_loss: 1.26   test_loss: 1.28\n",
      "epoch:   8   train_loss: 1.24   test_loss: 1.27\n",
      "epoch:   9   train_loss: 1.23   test_loss: 1.26\n",
      "epoch:  10   train_loss: 1.21   test_loss: 1.25\n",
      "\n",
      "sample T=0.2: je ne sais pas de plus de la conducteur de soleil est de premier village est de l'argent de l'autre de la route en Asie et de la ville de la ville de la propre journée de voyage en bonnes qui se découvrirent le pay\n",
      "\n",
      "sample T=0.5: je ne sais pas les classes en compagnie de population de la frontière. Les pays vivant des traversées de confirme des années de voyager tout le stop en stop par la rivière et l'autre pour l'arriver à l'argent et le\n",
      "\n",
      "sample T=0.7: je ne sais pas se rencontrer la maison qui se la confiture de ses gens avec les problèmes en traduction de voyage de Usulaka avant de veulent le matin, j'ai eu des savons, le long de l'adressir sous un cargo Hongle\n",
      "\n",
      "sample T=1: je ne sais pas a m assez tus, quelques okia, nous avions former un endroit ma nette de gens nous échèves à 700 minus ! Le frère de la drogue et Quitzart \"  SkUMETan/Pédiap, et du sud dans le Color, et très où avant\n",
      "\n",
      "\n",
      "epoch:  11   train_loss: 1.20   test_loss: 1.24\n",
      "epoch:  12   train_loss: 1.19   test_loss: 1.23\n",
      "epoch:  13   train_loss: 1.18   test_loss: 1.23\n",
      "epoch:  14   train_loss: 1.18   test_loss: 1.22\n",
      "epoch:  15   train_loss: 1.17   test_loss: 1.22\n",
      "epoch:  16   train_loss: 1.17   test_loss: 1.22\n",
      "epoch:  17   train_loss: 1.16   test_loss: 1.22\n",
      "epoch:  18   train_loss: 1.16   test_loss: 1.21\n",
      "epoch:  19   train_loss: 1.16   test_loss: 1.21\n",
      "epoch:  20   train_loss: 1.15   test_loss: 1.21\n",
      "\n",
      "sample T=0.2: je ne sais pas le trajet de l'autre côté de la ville de Sanu et la pluie et le pays et de l'autre de la plage de l'autostop et les pays de la plage de la plage de la ville de Sanu et le pouce de la ville et des pla\n",
      "\n",
      "sample T=0.5: je ne sais pas très peu de montagnes et de mes recherches de cette fois-ville et demi d'argent pour le retrouver une nuit de la musique et pas rapidement en Australie), les enfants et des restaurants et comment pou\n",
      "\n",
      "sample T=0.7: je ne sais pas ce temps d'aider les arbres et jamais dans le pays (mon travail du profite du village routière de la nourriture et pas à pleines de villes s'acheter des rues animaux) \" différentes durée de voyage, j\n",
      "\n",
      "sample T=1: je ne sais pas établisque toujours prendre un bon magnifique exister d'une seule régide, trouver un peu. Permanes, ont piscinés à mon voyage impagnants dehors des rêves, ou volontaires en galpheuses insectus. Les p\n",
      "\n",
      "\n",
      "CPU times: user 7min 10s, sys: 2min 48s, total: 9min 59s\n",
      "Wall time: 10min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(model5, optimizer5, criterion5, bptt5, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Fixed-length RNN': (model1, n_chars),\n",
    "    'Stateless RNN': (model2, bptt2),\n",
    "    'Stateful RNN': (model3, bptt3),\n",
    "    'Small LSTM': (model4, bptt4),\n",
    "    'Large LSTM': (model5, bptt5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T = 0.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sans les mais de les partiques des pas des parte de les pas de la pas seulement de la parce de la peur des pas de les partie de la pas de les parce de les pas des parte de ce pour des pas de les parte d\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas de la plus de la plus de mon peu pour les petit de la plus de mon peu par les permette de la mon peu par les petit de la plus de marcher de mais de la ville de la partie de la ville de la construite \n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas pas pas par le soleil et pas pas pas par construiser le travaillais de la chance de la plus de mon peut la ville de pays de la coupe de la fond de la construide de la route et le dépossions de la vil\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de la compagnie de la prendre le proche de la cons les pris de la compagnie de la route de la compagnie de l'avais de la ville de travailler en Australie de la ville de plus de la prendre les partie \n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas de profiter des pays de la ville de l'argent et le soleil en Amérique du Sud et nous avons pu payer les petits bus et des parties de la population de la pluie de l'argent de la route de la ville de l\n",
      "\n",
      "\n",
      "T = 0.5\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sac pour les comme des pour marche de la rence le pas des profites assez de serai que parture des se serai les parte pour les chaure des mois des si les causes des pour le seulement le mais à les enfale\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas au travaillant de présiter de marchert de la sans de cette sur les port de pour voyage, c'est nous avons de la plant de peu de minutes de rendre de la ville et de temps de la permette désort de conti\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas aussi de mois de la plaisir de la bars de la plus comme les car d'argent, je voyage pour a mes problème pour petits en passer la partier de la découvrisent par en pouvonstallé d'une sont difficiles d\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de couplel de décout de nombreuse de la cons le voyage de la ville est tout faire à bois sur le faire un pendant une cale de couple d'exploir et de la routes ou de voyage de la vie de serrière nous a\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas de construmer que de nombreuses semaines à force de profiter des problèmes d'accordéon en stop et le début de la route avec les autres pays sans fin du matin est une petite ville et des plages de la \n",
      "\n",
      "\n",
      "T = 0.7\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sa pas de leur des peu de la mais sur le son les graiment de sabtes commes peu de par de faisteurs à teur sur les voir dans des peraire pois des mais midant sec de maises de pas les locant de pas sur le\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas sur la terre de Maup d'une sans sur pour surtout dans de piesser marchés bion. Les concerve mais proble et rencontrer très envirouteux. De 400 00 jours de grations du moin de la voir de retamiller de\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas de si que je voyage sa peut une explique d'eque ! En même en vitrement de pays de mon enfers dégé de la rayant le dans mais la main plus de la pirité de la plaindres ! . . - . . - One se demander la \n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas de la mettente nombrelai, et la rout de me fesses dernières de m'ont de couple) et Siver on peut le sorte de le moin de Sudussieurs qu'il va de noconnais. La fainstr le plant en des pouventés de la c\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas un magnifique \" bruit (recouvertes, des moins culturels de déchets que le bus Robeal et bide de journée de construction et de leurs niveaux de l'hier \" de toutes la personne), vers l'Inde \" Guettic à\n",
      "\n",
      "\n",
      "T = 1\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne salles partie luise de trophes, par trouvidenalles et les les confronner à le fairana scis pour affre qui ques sur les 216s0$ je temps du les équit trade d'havait les équiffais rendsibles à les pendu eu\n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais passez sans l'avenirs souvent à avant. Les couchsulaber de l'écolant de maté, j'approle de gommentes perpomen virons moigoi je ne projeut découve. Il mutionnets qu'il ne  adant dans , it pourrale pour bu\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas d'ont voyage. Il fau des têvement partir et beaucoup: , Lative aventure, le disales de soit au rapident, chérons les ville de sécuriter la bonségoses ségalet, demandent cocons. ((pecurs pas en rejoin\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais passée en taccier de leur auros avoirtaire me vix d'Adurofrdire pas déconons on viins, assi Beres aussi de Nouve 2 Dépar. Quanez plus il a geains que c'est ralors par le bus mon m'aidés la Blage sage). +\n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas toujours en T-m Hedy \", ai vers son seco matin et la carte \" Norn I8BIRE8E Zello Osy=E_HURA Nouvello-surplopard.), c'est. Il me donne une heure de pommes d'un Français passé de nombreuses impressions\n",
      "\n",
      "\n",
      "T = 1.2\n",
      "\n",
      "Fixed-length RNN:\n",
      "  je ne sage à cordéme(n'intéanieliveniens.). L  ravair BMortes écho chancet de 3. Le pour loups: 1hkeux de reparpolents paimes il arizoiaumes ou même s implacce la fendes le thiste que biens seig)  En traveme \n",
      "\n",
      "Stateless RNN:\n",
      "  je ne sais pas assouteurs \"Urwins remos, les rêje sans immis organdunts ici pores. Cors un autre bodit (Kater sur riennes mes alarchélerponde sur la nessop, nos m'aussi. - C'est où égonnageurs et un soleil, et. - L\n",
      "\n",
      "Stateful RNN:\n",
      "  je ne sais pas ici. Tars masabids, 800 minira . . Mais cizzina, il poluent c'entamments entre les terrer les Quital égaler que j. Psaubusimemmeter séjà ne frontré ! Le sécuput dans lui encorte des bouler un inktera\n",
      "\n",
      "Small LSTM:\n",
      "  je ne sais pas  n'a mêtemes par ligde dans un en teau et démison autour du yjoumoins, argéendynessent de mouverac en apprésé ils ne suss, p rengole Hen, ampiers. Non te senses accortouver sorguessans \" SMalwi, epa \n",
      "\n",
      "Large LSTM:\n",
      "  je ne sais pas boire 130cm. Nous vas conna non plus, j'ai accumulérée, se tâte dans la natures, lynet sigulant, avec mon premier your. \"Quiliste localement... et par journée natif pour notre voyage belle est moins,\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "initial_s = 'je ne sais pas'\n",
    "\n",
    "for temperature in (0.2, 0.5, 0.7, 1, 1.2):\n",
    "    print(f'T = {temperature}')\n",
    "    print()\n",
    "    \n",
    "    for model_name, (model, bptt) in models.items():\n",
    "        \n",
    "        # Handle fixed-size RNN\n",
    "        generate_func = generate_fixed_size if model_name == 'Fixed-length RNN' else generate\n",
    "        s = initial_s[:n_chars] if model_name == 'Fixed-length RNN' else initial_s\n",
    "\n",
    "        print(f'{model_name}:\\n  ' + generate_func(model, s, 200, bptt, temperature))\n",
    "        print()\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "je suffisant de la ville et de l'autostop sont en compagnie de la ville en pleine ville de Sydney) et le voyage de cette fois que nous avons rendu son sac-à-dos. En effet, un peu de temps en sachant une construction de plus de 2 pour dormir des pays et des petits villages de la frontière et le pays de la route de l'autostop pour me rendre par consequent sur le soleil et de nombreux mois et nous avons parti de la plage de la ville et les plus belles villes de mon sac de marche de l'argent et le pays de la planète et de la ville de la route et sont déjà un peu de ce moment de partir de la première fois plus de 2000 Rinas d'eau de la pluie de la route de la population de l'autostop pour le pays et de la route de la ville est un peu de soleil et les plats de sourires et des petits pays pour dormir dans les petits boulots de la soirée et de l'autostop est de l'autre côté de la route pour les plus tard et pour la pluie et le pays de terme de la plage de la plupart des plats et les plus propres \n"
     ]
    }
   ],
   "source": [
    "print(generate_func(model5, 'je ', 1000, bptt5, temperature=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
